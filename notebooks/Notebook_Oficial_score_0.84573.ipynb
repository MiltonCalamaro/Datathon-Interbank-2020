{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "after-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm \n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuffed-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "def calculate_mode(x):\n",
    "    try:\n",
    "        moda=stat.mode(x)\n",
    "    except:\n",
    "        moda=np.nan\n",
    "    return moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exciting-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data'\n",
    "y_train = pd.read_csv(f'{path}/y_train.csv', index_col = 'key_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-cheat",
   "metadata": {},
   "source": [
    "## Procesar RCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc_train = pd.read_csv(f'{path}/rcc_train.csv')\n",
    "rcc_test = pd.read_csv(f'{path}/rcc_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc_test['cod_instit_financiera'].fillna(rcc_test['cod_instit_financiera'].value_counts().index[0], inplace=True)\n",
    "rcc_test['PRODUCTO'].fillna(rcc_test['PRODUCTO'].value_counts().index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'codmes': 'int32',\n",
    " 'key_value': 'int32',\n",
    " 'condicion': 'int32',\n",
    " 'tipo_credito': 'int32',\n",
    " 'cod_instit_financiera': 'int32',\n",
    " 'PRODUCTO': 'int32',\n",
    " 'RIESGO_DIRECTO': 'int32',\n",
    " 'COD_CLASIFICACION_DEUDOR': 'int32'}\n",
    "rcc_train = rcc_train.astype(dict_)\n",
    "rcc_test = rcc_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-telescope",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, 0, 10, 20, 30, 60, 90, 180, 360, 720, float(\"inf\")]\n",
    "rcc_train[\"condicion\"] = pd.cut(rcc_train.condicion, bins)\n",
    "rcc_train[\"condicion\"] = rcc_train[\"condicion\"].cat.codes\n",
    "rcc_test[\"condicion\"] = pd.cut(rcc_test.condicion, bins)\n",
    "rcc_test[\"condicion\"] = rcc_test[\"condicion\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "moda=lambda x: calculate_mode(x)\n",
    "moda.__name__='mode'\n",
    "agg_rcc = {'cod_instit_financiera':['nunique','min','max',moda],\n",
    "           'PRODUCTO':['nunique','min','max',moda],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rcc_train_agg = []\n",
    "for n,i in enumerate(sorted(set(rcc_train.codmes),reverse=True)):\n",
    "    for c in ['tipo_credito','RIESGO_DIRECTO','COD_CLASIFICACION_DEUDOR','condicion']:\n",
    "        print(f'haciendo {c} desde {i}')\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.sum().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoSum_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.size().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoUnique_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.min().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoMin_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.max().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoMax_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.std().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoStd_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "        \n",
    "    print(f'haciendo aggregate de {i}')\n",
    "    rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby('key_value').agg(agg_rcc)\n",
    "    rcc_train_agg.columns = [i+'_'+j+f'_ult{n+1}mes' for i,j in rcc_train_agg.columns]\n",
    "    list_rcc_train_agg.append(rcc_train_agg)\n",
    "    gc.collect()\n",
    "        \n",
    "rcc_train_ = pd.concat(list_rcc_train_agg, axis=1)\n",
    "del rcc_train, list_rcc_train_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rcc_test_agg = []\n",
    "for n,i in enumerate(sorted(set(rcc_test.codmes),reverse=True)):\n",
    "    for c in ['tipo_credito','RIESGO_DIRECTO','COD_CLASIFICACION_DEUDOR','condicion']:\n",
    "        print(f'haciendo {c} desde {i}')\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.sum().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoSum_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.size().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoUnique_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.min().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoMin_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.max().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoMax_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.std().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoStd_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "    print(f'haciendo aggregate de {i}')\n",
    "    rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby('key_value').agg(agg_rcc)\n",
    "    rcc_test_agg.columns = [i+'_'+j+f'_ult{n+1}mes' for i,j in rcc_test_agg.columns]\n",
    "    list_rcc_test_agg.append(rcc_test_agg)\n",
    "    gc.collect()\n",
    "\n",
    "rcc_test_ = pd.concat(list_rcc_test_agg, axis=1)\n",
    "del rcc_test, list_rcc_test_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "### asegurar que las columnas esten en ambas bases (train/test)\n",
    "print(rcc_train_.shape, rcc_test_.shape)\n",
    "keep_columns = list(set(rcc_train_.columns).intersection(rcc_test_.columns))\n",
    "rcc_train_ = rcc_train_[keep_columns].copy()\n",
    "rcc_test_ = rcc_test_[keep_columns].copy()\n",
    "print(rcc_train_.shape, rcc_test_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "### unir RCC en la base final\n",
    "train = rcc_train_.copy()\n",
    "test = rcc_test_.copy()\n",
    "del rcc_train_, rcc_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-auction",
   "metadata": {},
   "source": [
    "## Procesar SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_train = pd.read_csv(f'{path}/se_train.csv', index_col = 'key_value')\n",
    "se_test = pd.read_csv(f'{path}/se_test.csv', index_col = 'key_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'sexo':'int32',\n",
    "         'est_cvl':'int32',\n",
    "         'sit_lab':'int32',\n",
    "         'cod_ocu':'int32',\n",
    "         'ctd_hijos':'int32',\n",
    "         'flg_sin_email':'int32',\n",
    "         'ctd_veh':'int32',\n",
    "         'lgr_vot':'int32',\n",
    "         'prv':'int32',\n",
    "         'dto':'int32',\n",
    "         'rgn':'int32',\n",
    "         'tip_lvledu':'int32'}\n",
    "se_train = se_train.astype(dict_)\n",
    "se_test = se_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-pension",
   "metadata": {},
   "outputs": [],
   "source": [
    "### unir SE en la base final\n",
    "train = train.join(se_train) \n",
    "test = test.join(se_test)\n",
    "del se_train, se_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-northwest",
   "metadata": {},
   "source": [
    "## Procesar SUNAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunat_train = pd.read_csv(f'{path}/sunat_train.csv')\n",
    "sunat_test = pd.read_csv(f'{path}/sunat_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### eliminar registros duplicados\n",
    "sunat_train.drop_duplicates(inplace=True)\n",
    "sunat_test.drop_duplicates(inplace=True)\n",
    "sunat_train.shape, sunat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'tipcontribuyente': 'int32',\n",
    "         'tippersona': 'int32',\n",
    "         'ciiu': 'int32',\n",
    "         'ubigeo': 'int32',\n",
    "         'condiciondomicilio': 'int32',\n",
    "         'estadocontribuyente': 'int32',\n",
    "         'codvia': 'int32',\n",
    "         'codzona': 'int32',\n",
    "         'contabilidad': 'int32',\n",
    "         'facturacion': 'int32',\n",
    "         'domiciliado': 'int32',\n",
    "         'comercioexterior': 'int32',\n",
    "         'cargorele': 'int32',\n",
    "         'codentidadtributo': 'int32',\n",
    "         'estadotributo': 'int32'}\n",
    "sunat_train = sunat_train.astype(dict_)\n",
    "sunat_test = sunat_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunat_train['diff_fech'] = sunat_train['fecbaja'] - sunat_train['fecalta']\n",
    "sunat_test['diff_fech'] = sunat_test['fecbaja'] - sunat_test['fecalta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "moda=lambda x: calculate_mode(x)\n",
    "moda.__name__='mode'\n",
    "agg_sunat = {'tipcontribuyente':['nunique',moda],\n",
    "           'tippersona':['nunique',moda],\n",
    "           'ciiu':['nunique', moda],\n",
    "           'ubigeo':['nunique',moda],\n",
    "           'condiciondomicilio':['nunique',moda],\n",
    "           'estadocontribuyente':['nunique',moda],\n",
    "           'codvia':['nunique',moda],\n",
    "           'codzona':['nunique',moda],\n",
    "           'contabilidad':['nunique',moda],\n",
    "           'facturacion':['nunique',moda],\n",
    "           'domiciliado':['nunique',moda],\n",
    "           'comercioexterior':['nunique',moda],\n",
    "           'cargorele':['nunique',moda],\n",
    "           'codentidadtributo':['nunique',moda],\n",
    "           'estadotributo':['nunique',moda],\n",
    "           'fecalta':['mean','max', 'nunique','min','std'],\n",
    "           'fecbaja':['mean','max', 'nunique','min','std'], \n",
    "           'diff_fech':['mean','max', 'nunique','min','std'], \n",
    "            }\n",
    "\n",
    "sunat_train_ = sunat_train.groupby('key_value').agg(agg_sunat)\n",
    "sunat_train_.columns = [i+'_'+j for i,j in sunat_train_.columns]\n",
    "sunat_test_ = sunat_test.groupby('key_value').agg(agg_sunat)\n",
    "sunat_test_.columns = [i+'_'+j for i,j in sunat_test_.columns]\n",
    "del sunat_train, sunat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "### unir SUNAT en la base final\n",
    "train = train.join(sunat_train_)\n",
    "test = test.join(sunat_test_)\n",
    "del sunat_train_, sunat_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-trauma",
   "metadata": {},
   "source": [
    "## Training\n",
    "* despues de realizar varias iteraciones empleando el modelo lightGBM para identificar las variables. Eliminado aquellas que tienen zero_importance y low_importance (cuando su valor de importancia no contribuye con el 0.99 de importancia acumulativo). Parar hasta que no exista variable con zero_importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # eliminar variables con zero_importance\n",
    "# zero_importance = fi[fi==0]\n",
    "# aux = fi[fi>0].sort_values(ascending=False)\n",
    "# keep_columns = []\n",
    "# count = 0\n",
    "# for feature,values in zip(aux.index, aux.values):\n",
    "#     count+=values\n",
    "#     if count<=0.99:\n",
    "#         keep_columns.append(feature)\n",
    "        \n",
    "# len(keep_columns), len(zero_importance), len(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "developing-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_pickle('train_1799features.pkl')\n",
    "# test = pd.read_pickle('test_1799features.pkl')\n",
    "# #### guardar archivo procesado\n",
    "# train=pd.read_pickle('train_907features.pkl')\n",
    "# test = pd.read_pickle('test_907features.pkl')\n",
    "# ## guardar archivo procesado\n",
    "train = pd.read_pickle('train_886features.pkl')\n",
    "test = pd.read_pickle('test_886features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "freelance-browser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 886), (396666, 886))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "increased-berry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.844244\ttraining's binary_logloss: 0.30249\tvalid_1's auc: 0.836231\tvalid_1's binary_logloss: 0.308795\n",
      "[100]\ttraining's auc: 0.858563\ttraining's binary_logloss: 0.291167\tvalid_1's auc: 0.845098\tvalid_1's binary_logloss: 0.301731\n",
      "[150]\ttraining's auc: 0.866926\ttraining's binary_logloss: 0.284583\tvalid_1's auc: 0.848041\tvalid_1's binary_logloss: 0.299274\n",
      "[200]\ttraining's auc: 0.873269\ttraining's binary_logloss: 0.279527\tvalid_1's auc: 0.849593\tvalid_1's binary_logloss: 0.297966\n",
      "[250]\ttraining's auc: 0.878481\ttraining's binary_logloss: 0.275355\tvalid_1's auc: 0.850331\tvalid_1's binary_logloss: 0.297354\n",
      "[300]\ttraining's auc: 0.883379\ttraining's binary_logloss: 0.271322\tvalid_1's auc: 0.85079\tvalid_1's binary_logloss: 0.29691\n",
      "Early stopping, best iteration is:\n",
      "[311]\ttraining's auc: 0.884357\ttraining's binary_logloss: 0.270534\tvalid_1's auc: 0.850842\tvalid_1's binary_logloss: 0.296852\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.844728\ttraining's binary_logloss: 0.302191\tvalid_1's auc: 0.836968\tvalid_1's binary_logloss: 0.307774\n",
      "[100]\ttraining's auc: 0.858886\ttraining's binary_logloss: 0.291032\tvalid_1's auc: 0.845584\tvalid_1's binary_logloss: 0.301048\n",
      "[150]\ttraining's auc: 0.866996\ttraining's binary_logloss: 0.28442\tvalid_1's auc: 0.848648\tvalid_1's binary_logloss: 0.298476\n",
      "[200]\ttraining's auc: 0.873324\ttraining's binary_logloss: 0.279382\tvalid_1's auc: 0.850019\tvalid_1's binary_logloss: 0.297261\n",
      "[250]\ttraining's auc: 0.878852\ttraining's binary_logloss: 0.274984\tvalid_1's auc: 0.850499\tvalid_1's binary_logloss: 0.296798\n",
      "Early stopping, best iteration is:\n",
      "[240]\ttraining's auc: 0.877827\ttraining's binary_logloss: 0.275809\tvalid_1's auc: 0.850513\tvalid_1's binary_logloss: 0.29683\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.844495\ttraining's binary_logloss: 0.302814\tvalid_1's auc: 0.839745\tvalid_1's binary_logloss: 0.306011\n",
      "[100]\ttraining's auc: 0.858953\ttraining's binary_logloss: 0.291436\tvalid_1's auc: 0.847958\tvalid_1's binary_logloss: 0.29904\n",
      "[150]\ttraining's auc: 0.86688\ttraining's binary_logloss: 0.284946\tvalid_1's auc: 0.850444\tvalid_1's binary_logloss: 0.296688\n",
      "[200]\ttraining's auc: 0.873139\ttraining's binary_logloss: 0.279914\tvalid_1's auc: 0.851673\tvalid_1's binary_logloss: 0.295558\n",
      "[250]\ttraining's auc: 0.87867\ttraining's binary_logloss: 0.275541\tvalid_1's auc: 0.852298\tvalid_1's binary_logloss: 0.294963\n",
      "Early stopping, best iteration is:\n",
      "[280]\ttraining's auc: 0.881612\ttraining's binary_logloss: 0.273162\tvalid_1's auc: 0.852778\tvalid_1's binary_logloss: 0.294615\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.84444\ttraining's binary_logloss: 0.302909\tvalid_1's auc: 0.838456\tvalid_1's binary_logloss: 0.306344\n",
      "[100]\ttraining's auc: 0.858612\ttraining's binary_logloss: 0.291695\tvalid_1's auc: 0.847335\tvalid_1's binary_logloss: 0.299194\n",
      "[150]\ttraining's auc: 0.867188\ttraining's binary_logloss: 0.284995\tvalid_1's auc: 0.850651\tvalid_1's binary_logloss: 0.296456\n",
      "[200]\ttraining's auc: 0.873581\ttraining's binary_logloss: 0.279889\tvalid_1's auc: 0.851805\tvalid_1's binary_logloss: 0.295274\n",
      "[250]\ttraining's auc: 0.878952\ttraining's binary_logloss: 0.275532\tvalid_1's auc: 0.85288\tvalid_1's binary_logloss: 0.294411\n",
      "Early stopping, best iteration is:\n",
      "[277]\ttraining's auc: 0.881671\ttraining's binary_logloss: 0.273316\tvalid_1's auc: 0.85324\tvalid_1's binary_logloss: 0.294087\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.846376\ttraining's binary_logloss: 0.302192\tvalid_1's auc: 0.8309\tvalid_1's binary_logloss: 0.308721\n",
      "[100]\ttraining's auc: 0.86093\ttraining's binary_logloss: 0.290472\tvalid_1's auc: 0.839198\tvalid_1's binary_logloss: 0.302142\n",
      "[150]\ttraining's auc: 0.869323\ttraining's binary_logloss: 0.283683\tvalid_1's auc: 0.842004\tvalid_1's binary_logloss: 0.299909\n",
      "[200]\ttraining's auc: 0.875514\ttraining's binary_logloss: 0.278521\tvalid_1's auc: 0.843363\tvalid_1's binary_logloss: 0.298798\n",
      "Early stopping, best iteration is:\n",
      "[206]\ttraining's auc: 0.87623\ttraining's binary_logloss: 0.277964\tvalid_1's auc: 0.843559\tvalid_1's binary_logloss: 0.29867\n",
      "*********************\n",
      "roc auc estimado:  0.8502134825474761\n",
      "roc auc varianza:  0.0008526359723787526\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs = []\n",
    "train_probs = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index].target\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index].target\n",
    "\n",
    "    learner = LGBMClassifier(n_estimators=1000, boosting_type='gbdt',min_child_samples=1500, \n",
    "                   colsample_bytree=0.8,subsample=0.8, max_bin=200, learning_rate=0.1)\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50)\n",
    "    test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "train_probs = pd.concat(train_probs)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))\n",
    "print(\"roc auc varianza: \", np.std([roc_auc_score(y_train.loc[folds[i]], train_probs.iloc[folds[i]]) for i in range(len(folds))]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unexpected-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name='target'\n",
    "test_probs.to_csv('../results/test_LGBM_886Features_0.8502134.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
