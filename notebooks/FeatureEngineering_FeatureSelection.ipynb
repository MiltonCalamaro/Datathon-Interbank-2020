{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boolean-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm \n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "about-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "def calculate_mode(x):\n",
    "    try:\n",
    "        moda=stat.mode(x)\n",
    "    except:\n",
    "        moda=np.nan\n",
    "    return moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "supposed-bargain",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data'\n",
    "y_train = pd.read_csv(f'{path}/y_train.csv', index_col = 'key_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-potential",
   "metadata": {},
   "source": [
    "## Procesar RCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "graduate-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc_train = pd.read_csv(f'{path}/rcc_train.csv')\n",
    "rcc_test = pd.read_csv(f'{path}/rcc_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "pharmaceutical-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc_test['cod_instit_financiera'].fillna(rcc_test['cod_instit_financiera'].value_counts().index[0], inplace=True)\n",
    "rcc_test['PRODUCTO'].fillna(rcc_test['PRODUCTO'].value_counts().index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "brown-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'codmes': 'int32',\n",
    " 'key_value': 'int32',\n",
    " 'condicion': 'int32',\n",
    " 'tipo_credito': 'int32',\n",
    " 'cod_instit_financiera': 'int32',\n",
    " 'PRODUCTO': 'int32',\n",
    " 'RIESGO_DIRECTO': 'int32',\n",
    " 'COD_CLASIFICACION_DEUDOR': 'int32'}\n",
    "rcc_train = rcc_train.astype(dict_)\n",
    "rcc_test = rcc_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "automated-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, 0, 10, 20, 30, 60, 90, 180, 360, 720, float(\"inf\")]\n",
    "rcc_train[\"condicion\"] = pd.cut(rcc_train.condicion, bins)\n",
    "rcc_train[\"condicion\"] = rcc_train[\"condicion\"].cat.codes\n",
    "rcc_test[\"condicion\"] = pd.cut(rcc_test.condicion, bins)\n",
    "rcc_test[\"condicion\"] = rcc_test[\"condicion\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "federal-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "moda=lambda x: calculate_mode(x)\n",
    "moda.__name__='mode'\n",
    "agg_rcc = {'cod_instit_financiera':['nunique','min','max',moda],\n",
    "           'PRODUCTO':['nunique','min','max',moda],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naughty-mobile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo tipo_credito desde 201802\n",
      "haciendo RIESGO_DIRECTO desde 201802\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201802\n",
      "haciendo condicion desde 201802\n",
      "haciendo aggregate de 201802\n",
      "haciendo tipo_credito desde 201801\n",
      "haciendo RIESGO_DIRECTO desde 201801\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201801\n",
      "haciendo condicion desde 201801\n",
      "haciendo aggregate de 201801\n",
      "haciendo tipo_credito desde 201712\n",
      "haciendo RIESGO_DIRECTO desde 201712\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201712\n",
      "haciendo condicion desde 201712\n",
      "haciendo aggregate de 201712\n",
      "haciendo tipo_credito desde 201711\n",
      "haciendo RIESGO_DIRECTO desde 201711\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201711\n",
      "haciendo condicion desde 201711\n",
      "haciendo aggregate de 201711\n",
      "haciendo tipo_credito desde 201710\n",
      "haciendo RIESGO_DIRECTO desde 201710\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201710\n",
      "haciendo condicion desde 201710\n",
      "haciendo aggregate de 201710\n",
      "haciendo tipo_credito desde 201709\n",
      "haciendo RIESGO_DIRECTO desde 201709\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201709\n",
      "haciendo condicion desde 201709\n",
      "haciendo aggregate de 201709\n",
      "haciendo tipo_credito desde 201708\n",
      "haciendo RIESGO_DIRECTO desde 201708\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201708\n",
      "haciendo condicion desde 201708\n",
      "haciendo aggregate de 201708\n",
      "haciendo tipo_credito desde 201707\n",
      "haciendo RIESGO_DIRECTO desde 201707\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201707\n",
      "haciendo condicion desde 201707\n",
      "haciendo aggregate de 201707\n",
      "haciendo tipo_credito desde 201706\n",
      "haciendo RIESGO_DIRECTO desde 201706\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201706\n",
      "haciendo condicion desde 201706\n",
      "haciendo aggregate de 201706\n",
      "haciendo tipo_credito desde 201705\n",
      "haciendo RIESGO_DIRECTO desde 201705\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201705\n",
      "haciendo condicion desde 201705\n",
      "haciendo aggregate de 201705\n",
      "haciendo tipo_credito desde 201704\n",
      "haciendo RIESGO_DIRECTO desde 201704\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201704\n",
      "haciendo condicion desde 201704\n",
      "haciendo aggregate de 201704\n",
      "haciendo tipo_credito desde 201703\n",
      "haciendo RIESGO_DIRECTO desde 201703\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201703\n",
      "haciendo condicion desde 201703\n",
      "haciendo aggregate de 201703\n"
     ]
    }
   ],
   "source": [
    "list_rcc_train_agg = []\n",
    "for n,i in enumerate(sorted(set(rcc_train.codmes),reverse=True)):\n",
    "    for c in ['tipo_credito','RIESGO_DIRECTO','COD_CLASIFICACION_DEUDOR','condicion']:\n",
    "        print(f'haciendo {c} desde {i}')\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.sum().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoSum_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.size().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoUnique_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.min().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoMin_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.max().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoMax_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.std().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoStd_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.mean().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoMean_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "        \n",
    "        rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby(['key_value', c]).saldo.median().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_train_agg.columns = [f'{rcc_train_agg.columns.name}_{c}_saldoMedian_ult{n+1}meses' for c in rcc_train_agg.columns]\n",
    "        list_rcc_train_agg.append(rcc_train_agg)\n",
    "        gc.collect()\n",
    "        \n",
    "    print(f'haciendo aggregate de {i}')\n",
    "    rcc_train_agg = rcc_train[rcc_train.codmes>=i].groupby('key_value').agg(agg_rcc)\n",
    "    rcc_train_agg.columns = [i+'_'+j+f'_ult{n+1}mes' for i,j in rcc_train_agg.columns]\n",
    "    list_rcc_train_agg.append(rcc_train_agg)\n",
    "    gc.collect()\n",
    "        \n",
    "rcc_train_ = pd.concat(list_rcc_train_agg, axis=1)\n",
    "del rcc_train, list_rcc_train_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hazardous-particular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo tipo_credito desde 201902\n",
      "haciendo RIESGO_DIRECTO desde 201902\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201902\n",
      "haciendo condicion desde 201902\n",
      "haciendo aggregate de 201902\n",
      "haciendo tipo_credito desde 201901\n",
      "haciendo RIESGO_DIRECTO desde 201901\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201901\n",
      "haciendo condicion desde 201901\n",
      "haciendo aggregate de 201901\n",
      "haciendo tipo_credito desde 201812\n",
      "haciendo RIESGO_DIRECTO desde 201812\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201812\n",
      "haciendo condicion desde 201812\n",
      "haciendo aggregate de 201812\n",
      "haciendo tipo_credito desde 201811\n",
      "haciendo RIESGO_DIRECTO desde 201811\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201811\n",
      "haciendo condicion desde 201811\n",
      "haciendo aggregate de 201811\n",
      "haciendo tipo_credito desde 201810\n",
      "haciendo RIESGO_DIRECTO desde 201810\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201810\n",
      "haciendo condicion desde 201810\n",
      "haciendo aggregate de 201810\n",
      "haciendo tipo_credito desde 201809\n",
      "haciendo RIESGO_DIRECTO desde 201809\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201809\n",
      "haciendo condicion desde 201809\n",
      "haciendo aggregate de 201809\n",
      "haciendo tipo_credito desde 201808\n",
      "haciendo RIESGO_DIRECTO desde 201808\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201808\n",
      "haciendo condicion desde 201808\n",
      "haciendo aggregate de 201808\n",
      "haciendo tipo_credito desde 201807\n",
      "haciendo RIESGO_DIRECTO desde 201807\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201807\n",
      "haciendo condicion desde 201807\n",
      "haciendo aggregate de 201807\n",
      "haciendo tipo_credito desde 201806\n",
      "haciendo RIESGO_DIRECTO desde 201806\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201806\n",
      "haciendo condicion desde 201806\n",
      "haciendo aggregate de 201806\n",
      "haciendo tipo_credito desde 201805\n",
      "haciendo RIESGO_DIRECTO desde 201805\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201805\n",
      "haciendo condicion desde 201805\n",
      "haciendo aggregate de 201805\n",
      "haciendo tipo_credito desde 201804\n",
      "haciendo RIESGO_DIRECTO desde 201804\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201804\n",
      "haciendo condicion desde 201804\n",
      "haciendo aggregate de 201804\n",
      "haciendo tipo_credito desde 201803\n",
      "haciendo RIESGO_DIRECTO desde 201803\n",
      "haciendo COD_CLASIFICACION_DEUDOR desde 201803\n",
      "haciendo condicion desde 201803\n",
      "haciendo aggregate de 201803\n"
     ]
    }
   ],
   "source": [
    "list_rcc_test_agg = []\n",
    "for n,i in enumerate(sorted(set(rcc_test.codmes),reverse=True)):\n",
    "    for c in ['tipo_credito','RIESGO_DIRECTO','COD_CLASIFICACION_DEUDOR','condicion']:\n",
    "        print(f'haciendo {c} desde {i}')\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.sum().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoSum_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.size().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoUnique_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.min().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoMin_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.max().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoMax_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.std().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoStd_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.mean().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoMean_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby(['key_value', c]).saldo.median().unstack(level = 1, fill_value=np.nan)\n",
    "        rcc_test_agg.columns = [f'{rcc_test_agg.columns.name}_{c}_saldoMedian_ult{n+1}meses' for c in rcc_test_agg.columns]\n",
    "        list_rcc_test_agg.append(rcc_test_agg)\n",
    "        gc.collect()\n",
    "\n",
    "        \n",
    "    print(f'haciendo aggregate de {i}')\n",
    "    rcc_test_agg = rcc_test[rcc_test.codmes>=i].groupby('key_value').agg(agg_rcc)\n",
    "    rcc_test_agg.columns = [i+'_'+j+f'_ult{n+1}mes' for i,j in rcc_test_agg.columns]\n",
    "    list_rcc_test_agg.append(rcc_test_agg)\n",
    "    gc.collect()\n",
    "\n",
    "rcc_test_ = pd.concat(list_rcc_test_agg, axis=1)\n",
    "del rcc_test, list_rcc_test_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "conservative-johns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358487, 2532) (396666, 2616)\n",
      "(358487, 2532) (396666, 2532)\n"
     ]
    }
   ],
   "source": [
    "### asegurar que las columnas esten en ambas bases (train/test)\n",
    "print(rcc_train_.shape, rcc_test_.shape)\n",
    "keep_columns = list(set(rcc_train_.columns).intersection(rcc_test_.columns))\n",
    "rcc_train_ = rcc_train_[keep_columns].copy()\n",
    "rcc_test_ = rcc_test_[keep_columns].copy()\n",
    "print(rcc_train_.shape, rcc_test_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "union-demand",
   "metadata": {},
   "outputs": [],
   "source": [
    "### unir RCC en la base final\n",
    "train = rcc_train_.copy()\n",
    "test = rcc_test_.copy()\n",
    "del rcc_train_, rcc_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fewer-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('train_2532features.pkl')\n",
    "# test.to_pickle('test_2532features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "incident-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validation_lightgbm(train, y_train, test):\n",
    "    folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "    test_probs = []\n",
    "    train_probs = []\n",
    "    fi = []\n",
    "    for i, idx in enumerate(folds):\n",
    "        print(\"*\"*10, i, \"*\"*10)\n",
    "        Xt = train.loc[idx]\n",
    "        yt = y_train.loc[Xt.index].target\n",
    "\n",
    "        Xv = train.drop(Xt.index)\n",
    "        yv = y_train.loc[Xv.index].target\n",
    "\n",
    "        learner = LGBMClassifier(n_estimators=1000, boosting_type='gbdt',min_child_samples=1500, \n",
    "                       colsample_bytree=0.8,subsample=0.8, max_bin=200, learning_rate=0.1)\n",
    "        learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                    eval_set=[(Xt, yt), (Xv, yv)], verbose=50)\n",
    "        test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "        train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "        fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "    test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "    train_probs = pd.concat(train_probs)\n",
    "    fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "    print(\"*\" * 21)\n",
    "    print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))\n",
    "    print(\"roc auc varianza: \", np.std([roc_auc_score(y_train.loc[folds[i]], train_probs.iloc[folds[i]]) for i in range(len(folds))])) \n",
    "    return fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "external-boulder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.83882\ttraining's binary_logloss: 0.304788\tvalid_1's auc: 0.830858\tvalid_1's binary_logloss: 0.310663\n",
      "[100]\ttraining's auc: 0.851767\ttraining's binary_logloss: 0.295217\tvalid_1's auc: 0.837418\tvalid_1's binary_logloss: 0.305689\n",
      "[150]\ttraining's auc: 0.85948\ttraining's binary_logloss: 0.289459\tvalid_1's auc: 0.83942\tvalid_1's binary_logloss: 0.304103\n",
      "[200]\ttraining's auc: 0.865892\ttraining's binary_logloss: 0.284753\tvalid_1's auc: 0.84049\tvalid_1's binary_logloss: 0.303278\n",
      "Early stopping, best iteration is:\n",
      "[216]\ttraining's auc: 0.867463\ttraining's binary_logloss: 0.283446\tvalid_1's auc: 0.840636\tvalid_1's binary_logloss: 0.303178\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838444\ttraining's binary_logloss: 0.30488\tvalid_1's auc: 0.830145\tvalid_1's binary_logloss: 0.310827\n",
      "[100]\ttraining's auc: 0.85122\ttraining's binary_logloss: 0.29542\tvalid_1's auc: 0.836749\tvalid_1's binary_logloss: 0.305834\n",
      "[150]\ttraining's auc: 0.859062\ttraining's binary_logloss: 0.289643\tvalid_1's auc: 0.838591\tvalid_1's binary_logloss: 0.304347\n",
      "[200]\ttraining's auc: 0.865554\ttraining's binary_logloss: 0.284922\tvalid_1's auc: 0.839665\tvalid_1's binary_logloss: 0.303491\n",
      "[250]\ttraining's auc: 0.871195\ttraining's binary_logloss: 0.280766\tvalid_1's auc: 0.840091\tvalid_1's binary_logloss: 0.303169\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttraining's auc: 0.874457\ttraining's binary_logloss: 0.278308\tvalid_1's auc: 0.840211\tvalid_1's binary_logloss: 0.303078\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838463\ttraining's binary_logloss: 0.305248\tvalid_1's auc: 0.833414\tvalid_1's binary_logloss: 0.308716\n",
      "[100]\ttraining's auc: 0.851232\ttraining's binary_logloss: 0.295825\tvalid_1's auc: 0.83925\tvalid_1's binary_logloss: 0.303949\n",
      "[150]\ttraining's auc: 0.858878\ttraining's binary_logloss: 0.290095\tvalid_1's auc: 0.840571\tvalid_1's binary_logloss: 0.302484\n",
      "[200]\ttraining's auc: 0.865346\ttraining's binary_logloss: 0.285422\tvalid_1's auc: 0.841441\tvalid_1's binary_logloss: 0.30169\n",
      "Early stopping, best iteration is:\n",
      "[198]\ttraining's auc: 0.865151\ttraining's binary_logloss: 0.285568\tvalid_1's auc: 0.841445\tvalid_1's binary_logloss: 0.301689\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838316\ttraining's binary_logloss: 0.305419\tvalid_1's auc: 0.831598\tvalid_1's binary_logloss: 0.30951\n",
      "[100]\ttraining's auc: 0.851036\ttraining's binary_logloss: 0.29609\tvalid_1's auc: 0.837741\tvalid_1's binary_logloss: 0.304582\n",
      "[150]\ttraining's auc: 0.85905\ttraining's binary_logloss: 0.290226\tvalid_1's auc: 0.839767\tvalid_1's binary_logloss: 0.302807\n",
      "Early stopping, best iteration is:\n",
      "[172]\ttraining's auc: 0.862094\ttraining's binary_logloss: 0.288012\tvalid_1's auc: 0.840413\tvalid_1's binary_logloss: 0.302343\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.839927\ttraining's binary_logloss: 0.304814\tvalid_1's auc: 0.825039\tvalid_1's binary_logloss: 0.311106\n",
      "[100]\ttraining's auc: 0.852889\ttraining's binary_logloss: 0.295158\tvalid_1's auc: 0.831124\tvalid_1's binary_logloss: 0.30666\n",
      "[150]\ttraining's auc: 0.860924\ttraining's binary_logloss: 0.289182\tvalid_1's auc: 0.832853\tvalid_1's binary_logloss: 0.305284\n",
      "[200]\ttraining's auc: 0.867254\ttraining's binary_logloss: 0.284427\tvalid_1's auc: 0.833544\tvalid_1's binary_logloss: 0.3047\n",
      "Early stopping, best iteration is:\n",
      "[235]\ttraining's auc: 0.871496\ttraining's binary_logloss: 0.281386\tvalid_1's auc: 0.833945\tvalid_1's binary_logloss: 0.304375\n",
      "*********************\n",
      "roc auc estimado:  0.8393558323247359\n",
      "roc auc varianza:  0.0006603057129060726\n",
      "total de variables : 1482\n",
      "variables con importancia acumulada al 99% : 1309\n",
      "variables con zero importancia : 14\n",
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838397\ttraining's binary_logloss: 0.305021\tvalid_1's auc: 0.830182\tvalid_1's binary_logloss: 0.311013\n",
      "[100]\ttraining's auc: 0.851183\ttraining's binary_logloss: 0.2955\tvalid_1's auc: 0.836721\tvalid_1's binary_logloss: 0.306216\n",
      "[150]\ttraining's auc: 0.859185\ttraining's binary_logloss: 0.289618\tvalid_1's auc: 0.839101\tvalid_1's binary_logloss: 0.304441\n",
      "[200]\ttraining's auc: 0.865456\ttraining's binary_logloss: 0.285013\tvalid_1's auc: 0.84002\tvalid_1's binary_logloss: 0.303811\n",
      "[250]\ttraining's auc: 0.871106\ttraining's binary_logloss: 0.28081\tvalid_1's auc: 0.840489\tvalid_1's binary_logloss: 0.303489\n",
      "Early stopping, best iteration is:\n",
      "[249]\ttraining's auc: 0.870973\ttraining's binary_logloss: 0.280889\tvalid_1's auc: 0.84053\tvalid_1's binary_logloss: 0.303465\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838431\ttraining's binary_logloss: 0.305053\tvalid_1's auc: 0.830173\tvalid_1's binary_logloss: 0.310998\n",
      "[100]\ttraining's auc: 0.851165\ttraining's binary_logloss: 0.295511\tvalid_1's auc: 0.837098\tvalid_1's binary_logloss: 0.305807\n",
      "[150]\ttraining's auc: 0.858933\ttraining's binary_logloss: 0.289694\tvalid_1's auc: 0.839297\tvalid_1's binary_logloss: 0.304165\n",
      "[200]\ttraining's auc: 0.865343\ttraining's binary_logloss: 0.285034\tvalid_1's auc: 0.839966\tvalid_1's binary_logloss: 0.303576\n",
      "Early stopping, best iteration is:\n",
      "[234]\ttraining's auc: 0.86914\ttraining's binary_logloss: 0.282197\tvalid_1's auc: 0.840175\tvalid_1's binary_logloss: 0.303335\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838516\ttraining's binary_logloss: 0.305262\tvalid_1's auc: 0.833468\tvalid_1's binary_logloss: 0.308852\n",
      "[100]\ttraining's auc: 0.850858\ttraining's binary_logloss: 0.296006\tvalid_1's auc: 0.839286\tvalid_1's binary_logloss: 0.304044\n",
      "[150]\ttraining's auc: 0.85868\ttraining's binary_logloss: 0.290289\tvalid_1's auc: 0.840932\tvalid_1's binary_logloss: 0.302433\n",
      "[200]\ttraining's auc: 0.865116\ttraining's binary_logloss: 0.285554\tvalid_1's auc: 0.841606\tvalid_1's binary_logloss: 0.30177\n",
      "Early stopping, best iteration is:\n",
      "[229]\ttraining's auc: 0.868474\ttraining's binary_logloss: 0.28304\tvalid_1's auc: 0.841928\tvalid_1's binary_logloss: 0.301447\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838664\ttraining's binary_logloss: 0.305282\tvalid_1's auc: 0.831816\tvalid_1's binary_logloss: 0.309386\n",
      "[100]\ttraining's auc: 0.851473\ttraining's binary_logloss: 0.295815\tvalid_1's auc: 0.838115\tvalid_1's binary_logloss: 0.304313\n",
      "[150]\ttraining's auc: 0.85926\ttraining's binary_logloss: 0.289993\tvalid_1's auc: 0.83998\tvalid_1's binary_logloss: 0.302653\n",
      "[200]\ttraining's auc: 0.86552\ttraining's binary_logloss: 0.285382\tvalid_1's auc: 0.840551\tvalid_1's binary_logloss: 0.302012\n",
      "Early stopping, best iteration is:\n",
      "[213]\ttraining's auc: 0.86726\ttraining's binary_logloss: 0.284228\tvalid_1's auc: 0.84066\tvalid_1's binary_logloss: 0.301901\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.840194\ttraining's binary_logloss: 0.304618\tvalid_1's auc: 0.825751\tvalid_1's binary_logloss: 0.310749\n",
      "[100]\ttraining's auc: 0.852953\ttraining's binary_logloss: 0.295017\tvalid_1's auc: 0.831616\tvalid_1's binary_logloss: 0.306291\n",
      "[150]\ttraining's auc: 0.860824\ttraining's binary_logloss: 0.289092\tvalid_1's auc: 0.83343\tvalid_1's binary_logloss: 0.304919\n",
      "[200]\ttraining's auc: 0.867392\ttraining's binary_logloss: 0.284337\tvalid_1's auc: 0.83427\tvalid_1's binary_logloss: 0.30429\n",
      "Early stopping, best iteration is:\n",
      "[215]\ttraining's auc: 0.869198\ttraining's binary_logloss: 0.283011\tvalid_1's auc: 0.834446\tvalid_1's binary_logloss: 0.304195\n",
      "*********************\n",
      "roc auc estimado:  0.8395719550847607\n",
      "roc auc varianza:  0.0006413703119856295\n",
      "total de variables : 1309\n",
      "variables con importancia acumulada al 99% : 1208\n",
      "variables con zero importancia : 3\n",
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838507\ttraining's binary_logloss: 0.304955\tvalid_1's auc: 0.830439\tvalid_1's binary_logloss: 0.310926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's auc: 0.851265\ttraining's binary_logloss: 0.295491\tvalid_1's auc: 0.836937\tvalid_1's binary_logloss: 0.306101\n",
      "[150]\ttraining's auc: 0.859193\ttraining's binary_logloss: 0.289689\tvalid_1's auc: 0.839099\tvalid_1's binary_logloss: 0.304503\n",
      "[200]\ttraining's auc: 0.86571\ttraining's binary_logloss: 0.284861\tvalid_1's auc: 0.84018\tvalid_1's binary_logloss: 0.303777\n",
      "[250]\ttraining's auc: 0.871272\ttraining's binary_logloss: 0.280664\tvalid_1's auc: 0.840795\tvalid_1's binary_logloss: 0.303378\n",
      "Early stopping, best iteration is:\n",
      "[269]\ttraining's auc: 0.873068\ttraining's binary_logloss: 0.279276\tvalid_1's auc: 0.840894\tvalid_1's binary_logloss: 0.303313\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838426\ttraining's binary_logloss: 0.304897\tvalid_1's auc: 0.829758\tvalid_1's binary_logloss: 0.311015\n",
      "[100]\ttraining's auc: 0.851152\ttraining's binary_logloss: 0.295415\tvalid_1's auc: 0.836629\tvalid_1's binary_logloss: 0.305945\n",
      "[150]\ttraining's auc: 0.859061\ttraining's binary_logloss: 0.289604\tvalid_1's auc: 0.838833\tvalid_1's binary_logloss: 0.304313\n",
      "[200]\ttraining's auc: 0.865457\ttraining's binary_logloss: 0.284863\tvalid_1's auc: 0.839733\tvalid_1's binary_logloss: 0.303564\n",
      "Early stopping, best iteration is:\n",
      "[210]\ttraining's auc: 0.866583\ttraining's binary_logloss: 0.284017\tvalid_1's auc: 0.83987\tvalid_1's binary_logloss: 0.303455\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838254\ttraining's binary_logloss: 0.305347\tvalid_1's auc: 0.833602\tvalid_1's binary_logloss: 0.308722\n",
      "[100]\ttraining's auc: 0.850934\ttraining's binary_logloss: 0.295872\tvalid_1's auc: 0.839083\tvalid_1's binary_logloss: 0.303995\n",
      "[150]\ttraining's auc: 0.858786\ttraining's binary_logloss: 0.29013\tvalid_1's auc: 0.840836\tvalid_1's binary_logloss: 0.302414\n",
      "[200]\ttraining's auc: 0.865459\ttraining's binary_logloss: 0.28538\tvalid_1's auc: 0.841511\tvalid_1's binary_logloss: 0.301739\n",
      "[250]\ttraining's auc: 0.870723\ttraining's binary_logloss: 0.281299\tvalid_1's auc: 0.841927\tvalid_1's binary_logloss: 0.301258\n",
      "Early stopping, best iteration is:\n",
      "[242]\ttraining's auc: 0.869955\ttraining's binary_logloss: 0.281921\tvalid_1's auc: 0.841993\tvalid_1's binary_logloss: 0.301267\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838316\ttraining's binary_logloss: 0.305486\tvalid_1's auc: 0.831465\tvalid_1's binary_logloss: 0.309545\n",
      "[100]\ttraining's auc: 0.851158\ttraining's binary_logloss: 0.296056\tvalid_1's auc: 0.838031\tvalid_1's binary_logloss: 0.304386\n",
      "[150]\ttraining's auc: 0.859172\ttraining's binary_logloss: 0.290185\tvalid_1's auc: 0.840035\tvalid_1's binary_logloss: 0.30273\n",
      "Early stopping, best iteration is:\n",
      "[169]\ttraining's auc: 0.86164\ttraining's binary_logloss: 0.288307\tvalid_1's auc: 0.840361\tvalid_1's binary_logloss: 0.30244\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.840035\ttraining's binary_logloss: 0.304783\tvalid_1's auc: 0.825748\tvalid_1's binary_logloss: 0.310846\n",
      "[100]\ttraining's auc: 0.852728\ttraining's binary_logloss: 0.295185\tvalid_1's auc: 0.831614\tvalid_1's binary_logloss: 0.30652\n",
      "[150]\ttraining's auc: 0.860854\ttraining's binary_logloss: 0.28917\tvalid_1's auc: 0.83354\tvalid_1's binary_logloss: 0.305071\n",
      "Early stopping, best iteration is:\n",
      "[183]\ttraining's auc: 0.865079\ttraining's binary_logloss: 0.285956\tvalid_1's auc: 0.833971\tvalid_1's binary_logloss: 0.304633\n",
      "*********************\n",
      "roc auc estimado:  0.8394286423328631\n",
      "roc auc varianza:  0.0006897519165602672\n",
      "total de variables : 1208\n",
      "variables con importancia acumulada al 99% : 1123\n",
      "variables con zero importancia : 2\n",
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838161\ttraining's binary_logloss: 0.305214\tvalid_1's auc: 0.830311\tvalid_1's binary_logloss: 0.311238\n",
      "[100]\ttraining's auc: 0.851144\ttraining's binary_logloss: 0.295574\tvalid_1's auc: 0.837295\tvalid_1's binary_logloss: 0.306009\n",
      "[150]\ttraining's auc: 0.859309\ttraining's binary_logloss: 0.289614\tvalid_1's auc: 0.839607\tvalid_1's binary_logloss: 0.304343\n",
      "[200]\ttraining's auc: 0.865776\ttraining's binary_logloss: 0.284839\tvalid_1's auc: 0.84041\tvalid_1's binary_logloss: 0.303701\n",
      "[250]\ttraining's auc: 0.871198\ttraining's binary_logloss: 0.280696\tvalid_1's auc: 0.841134\tvalid_1's binary_logloss: 0.303209\n",
      "Early stopping, best iteration is:\n",
      "[273]\ttraining's auc: 0.873393\ttraining's binary_logloss: 0.278985\tvalid_1's auc: 0.841433\tvalid_1's binary_logloss: 0.303026\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.83814\ttraining's binary_logloss: 0.305038\tvalid_1's auc: 0.829659\tvalid_1's binary_logloss: 0.311135\n",
      "[100]\ttraining's auc: 0.851169\ttraining's binary_logloss: 0.295523\tvalid_1's auc: 0.836618\tvalid_1's binary_logloss: 0.30597\n",
      "[150]\ttraining's auc: 0.859151\ttraining's binary_logloss: 0.289649\tvalid_1's auc: 0.838589\tvalid_1's binary_logloss: 0.304364\n",
      "[200]\ttraining's auc: 0.865494\ttraining's binary_logloss: 0.284992\tvalid_1's auc: 0.839756\tvalid_1's binary_logloss: 0.303489\n",
      "[250]\ttraining's auc: 0.871572\ttraining's binary_logloss: 0.280653\tvalid_1's auc: 0.840242\tvalid_1's binary_logloss: 0.303153\n",
      "Early stopping, best iteration is:\n",
      "[284]\ttraining's auc: 0.874779\ttraining's binary_logloss: 0.278128\tvalid_1's auc: 0.840481\tvalid_1's binary_logloss: 0.302965\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.83839\ttraining's binary_logloss: 0.305437\tvalid_1's auc: 0.833177\tvalid_1's binary_logloss: 0.309013\n",
      "[100]\ttraining's auc: 0.851161\ttraining's binary_logloss: 0.295921\tvalid_1's auc: 0.839337\tvalid_1's binary_logloss: 0.303804\n",
      "[150]\ttraining's auc: 0.858942\ttraining's binary_logloss: 0.290134\tvalid_1's auc: 0.84127\tvalid_1's binary_logloss: 0.302059\n",
      "[200]\ttraining's auc: 0.865418\ttraining's binary_logloss: 0.285437\tvalid_1's auc: 0.841792\tvalid_1's binary_logloss: 0.30147\n",
      "Early stopping, best iteration is:\n",
      "[227]\ttraining's auc: 0.868481\ttraining's binary_logloss: 0.283173\tvalid_1's auc: 0.842145\tvalid_1's binary_logloss: 0.301167\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838622\ttraining's binary_logloss: 0.305341\tvalid_1's auc: 0.831746\tvalid_1's binary_logloss: 0.309362\n",
      "[100]\ttraining's auc: 0.851344\ttraining's binary_logloss: 0.295964\tvalid_1's auc: 0.838191\tvalid_1's binary_logloss: 0.304318\n",
      "[150]\ttraining's auc: 0.859419\ttraining's binary_logloss: 0.290015\tvalid_1's auc: 0.840254\tvalid_1's binary_logloss: 0.302565\n",
      "[200]\ttraining's auc: 0.865739\ttraining's binary_logloss: 0.285317\tvalid_1's auc: 0.840942\tvalid_1's binary_logloss: 0.301903\n",
      "Early stopping, best iteration is:\n",
      "[195]\ttraining's auc: 0.865248\ttraining's binary_logloss: 0.285745\tvalid_1's auc: 0.84098\tvalid_1's binary_logloss: 0.301917\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.839986\ttraining's binary_logloss: 0.304776\tvalid_1's auc: 0.825946\tvalid_1's binary_logloss: 0.310744\n",
      "[100]\ttraining's auc: 0.852738\ttraining's binary_logloss: 0.295205\tvalid_1's auc: 0.832137\tvalid_1's binary_logloss: 0.306202\n",
      "[150]\ttraining's auc: 0.860869\ttraining's binary_logloss: 0.289244\tvalid_1's auc: 0.834142\tvalid_1's binary_logloss: 0.304696\n",
      "[200]\ttraining's auc: 0.867044\ttraining's binary_logloss: 0.284592\tvalid_1's auc: 0.834797\tvalid_1's binary_logloss: 0.304092\n",
      "Early stopping, best iteration is:\n",
      "[194]\ttraining's auc: 0.86641\ttraining's binary_logloss: 0.285112\tvalid_1's auc: 0.834883\tvalid_1's binary_logloss: 0.304061\n",
      "*********************\n",
      "roc auc estimado:  0.8400005802752911\n",
      "roc auc varianza:  0.0006391885186804281\n",
      "total de variables : 1123\n",
      "variables con importancia acumulada al 99% : 1064\n",
      "variables con zero importancia : 0\n"
     ]
    }
   ],
   "source": [
    "# eliminar variables con zero_importance\n",
    "while True:\n",
    "    fi = cross_validation_lightgbm(train, y_train, test)\n",
    "    zero_importance = fi[fi==0]\n",
    "    aux = fi[fi>0].sort_values(ascending=False)\n",
    "    keep_columns = []\n",
    "    count = 0\n",
    "    for feature,values in zip(aux.index, aux.values):\n",
    "        count+=values\n",
    "        if count<=0.99:\n",
    "            keep_columns.append(feature)\n",
    "            \n",
    "    print(f'total de variables : {len(train.columns)}')\n",
    "    print(f'variables con importancia acumulada al 99% : {len(keep_columns)}')\n",
    "    print(f'variables con zero importancia : {len(zero_importance)}')\n",
    "    train = train[keep_columns]\n",
    "    test = test[keep_columns]\n",
    "    if len(zero_importance)==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "corresponding-disclaimer",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1064), (396666, 1064))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape,  test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "optimum-beast",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838761\ttraining's binary_logloss: 0.304822\tvalid_1's auc: 0.830556\tvalid_1's binary_logloss: 0.310838\n",
      "[100]\ttraining's auc: 0.851409\ttraining's binary_logloss: 0.295462\tvalid_1's auc: 0.836985\tvalid_1's binary_logloss: 0.306073\n",
      "[150]\ttraining's auc: 0.85943\ttraining's binary_logloss: 0.289585\tvalid_1's auc: 0.839204\tvalid_1's binary_logloss: 0.304374\n",
      "[200]\ttraining's auc: 0.865853\ttraining's binary_logloss: 0.284853\tvalid_1's auc: 0.840292\tvalid_1's binary_logloss: 0.303612\n",
      "[250]\ttraining's auc: 0.871309\ttraining's binary_logloss: 0.280703\tvalid_1's auc: 0.840861\tvalid_1's binary_logloss: 0.303241\n",
      "Early stopping, best iteration is:\n",
      "[247]\ttraining's auc: 0.871011\ttraining's binary_logloss: 0.280922\tvalid_1's auc: 0.84091\tvalid_1's binary_logloss: 0.303218\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838125\ttraining's binary_logloss: 0.305126\tvalid_1's auc: 0.829515\tvalid_1's binary_logloss: 0.311417\n",
      "[100]\ttraining's auc: 0.851013\ttraining's binary_logloss: 0.295551\tvalid_1's auc: 0.836347\tvalid_1's binary_logloss: 0.306336\n",
      "[150]\ttraining's auc: 0.858858\ttraining's binary_logloss: 0.289696\tvalid_1's auc: 0.838588\tvalid_1's binary_logloss: 0.304622\n",
      "[200]\ttraining's auc: 0.865287\ttraining's binary_logloss: 0.285058\tvalid_1's auc: 0.83951\tvalid_1's binary_logloss: 0.303909\n",
      "[250]\ttraining's auc: 0.870829\ttraining's binary_logloss: 0.2809\tvalid_1's auc: 0.840097\tvalid_1's binary_logloss: 0.303481\n",
      "Early stopping, best iteration is:\n",
      "[258]\ttraining's auc: 0.87161\ttraining's binary_logloss: 0.280268\tvalid_1's auc: 0.840136\tvalid_1's binary_logloss: 0.303437\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838216\ttraining's binary_logloss: 0.30526\tvalid_1's auc: 0.832955\tvalid_1's binary_logloss: 0.309046\n",
      "[100]\ttraining's auc: 0.850853\ttraining's binary_logloss: 0.295874\tvalid_1's auc: 0.839079\tvalid_1's binary_logloss: 0.304009\n",
      "[150]\ttraining's auc: 0.858825\ttraining's binary_logloss: 0.290049\tvalid_1's auc: 0.840833\tvalid_1's binary_logloss: 0.302328\n",
      "[200]\ttraining's auc: 0.865166\ttraining's binary_logloss: 0.285415\tvalid_1's auc: 0.841419\tvalid_1's binary_logloss: 0.301788\n",
      "Early stopping, best iteration is:\n",
      "[211]\ttraining's auc: 0.866508\ttraining's binary_logloss: 0.284446\tvalid_1's auc: 0.841486\tvalid_1's binary_logloss: 0.301702\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838487\ttraining's binary_logloss: 0.305415\tvalid_1's auc: 0.831645\tvalid_1's binary_logloss: 0.309409\n",
      "[100]\ttraining's auc: 0.85136\ttraining's binary_logloss: 0.295851\tvalid_1's auc: 0.838288\tvalid_1's binary_logloss: 0.30421\n",
      "[150]\ttraining's auc: 0.859338\ttraining's binary_logloss: 0.290055\tvalid_1's auc: 0.839928\tvalid_1's binary_logloss: 0.302679\n",
      "[200]\ttraining's auc: 0.865508\ttraining's binary_logloss: 0.285509\tvalid_1's auc: 0.840428\tvalid_1's binary_logloss: 0.302084\n",
      "[250]\ttraining's auc: 0.871077\ttraining's binary_logloss: 0.28137\tvalid_1's auc: 0.84106\tvalid_1's binary_logloss: 0.301577\n",
      "Early stopping, best iteration is:\n",
      "[257]\ttraining's auc: 0.871781\ttraining's binary_logloss: 0.280816\tvalid_1's auc: 0.841191\tvalid_1's binary_logloss: 0.301499\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.839665\ttraining's binary_logloss: 0.3049\tvalid_1's auc: 0.825168\tvalid_1's binary_logloss: 0.311126\n",
      "[100]\ttraining's auc: 0.852685\ttraining's binary_logloss: 0.295205\tvalid_1's auc: 0.8311\tvalid_1's binary_logloss: 0.306689\n",
      "[150]\ttraining's auc: 0.860695\ttraining's binary_logloss: 0.289235\tvalid_1's auc: 0.832919\tvalid_1's binary_logloss: 0.305312\n",
      "[200]\ttraining's auc: 0.867097\ttraining's binary_logloss: 0.284444\tvalid_1's auc: 0.833847\tvalid_1's binary_logloss: 0.304568\n",
      "Early stopping, best iteration is:\n",
      "[216]\ttraining's auc: 0.869155\ttraining's binary_logloss: 0.282997\tvalid_1's auc: 0.833929\tvalid_1's binary_logloss: 0.304448\n",
      "*********************\n",
      "roc auc estimado:  0.8395557227794049\n",
      "roc auc varianza:  0.0006901652288592364\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs = []\n",
    "train_probs = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index].target\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index].target\n",
    "\n",
    "    learner = LGBMClassifier(n_estimators=1000, boosting_type='gbdt',min_child_samples=1500, \n",
    "                   colsample_bytree=0.8,subsample=0.8, max_bin=200, learning_rate=0.1)\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50)\n",
    "    test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "train_probs = pd.concat(train_probs)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))\n",
    "print(\"roc auc varianza: \", np.std([roc_auc_score(y_train.loc[folds[i]], train_probs.iloc[folds[i]]) for i in range(len(folds))])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "productive-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name = 'target'\n",
    "test_probs.to_csv('../results/lightgbm_with_1064features_0.83955.csv') ### score de 0.84242 en la tabla publica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "under-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('train_1064features.pkl')\n",
    "# test.to_pickle('test_1064features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-disco",
   "metadata": {},
   "source": [
    "## Procesar SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "appropriate-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_train = pd.read_csv(f'{path}/se_train.csv', index_col = 'key_value')\n",
    "se_test = pd.read_csv(f'{path}/se_test.csv', index_col = 'key_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "prerequisite-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'sexo':'int32',\n",
    "         'est_cvl':'int32',\n",
    "         'sit_lab':'int32',\n",
    "         'cod_ocu':'int32',\n",
    "         'ctd_hijos':'int32',\n",
    "         'flg_sin_email':'int32',\n",
    "         'ctd_veh':'int32',\n",
    "         'lgr_vot':'int32',\n",
    "         'prv':'int32',\n",
    "         'dto':'int32',\n",
    "         'rgn':'int32',\n",
    "         'tip_lvledu':'int32'}\n",
    "se_train = se_train.astype(dict_)\n",
    "se_test = se_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "previous-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "### unir SE en la base final\n",
    "train = train.join(se_train) \n",
    "test = test.join(se_test)\n",
    "del se_train, se_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "earned-fortune",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1078), (396666, 1078))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "romance-entity",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.845062\ttraining's binary_logloss: 0.301597\tvalid_1's auc: 0.83638\tvalid_1's binary_logloss: 0.308295\n",
      "[100]\ttraining's auc: 0.859504\ttraining's binary_logloss: 0.290151\tvalid_1's auc: 0.845125\tvalid_1's binary_logloss: 0.301465\n",
      "[150]\ttraining's auc: 0.868168\ttraining's binary_logloss: 0.283312\tvalid_1's auc: 0.848301\tvalid_1's binary_logloss: 0.298846\n",
      "[200]\ttraining's auc: 0.874831\ttraining's binary_logloss: 0.278033\tvalid_1's auc: 0.849607\tvalid_1's binary_logloss: 0.29769\n",
      "[250]\ttraining's auc: 0.880484\ttraining's binary_logloss: 0.273542\tvalid_1's auc: 0.850544\tvalid_1's binary_logloss: 0.296978\n",
      "[300]\ttraining's auc: 0.88537\ttraining's binary_logloss: 0.269528\tvalid_1's auc: 0.851007\tvalid_1's binary_logloss: 0.296578\n",
      "Early stopping, best iteration is:\n",
      "[298]\ttraining's auc: 0.885205\ttraining's binary_logloss: 0.269669\tvalid_1's auc: 0.851064\tvalid_1's binary_logloss: 0.296548\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.84507\ttraining's binary_logloss: 0.301671\tvalid_1's auc: 0.836349\tvalid_1's binary_logloss: 0.307818\n",
      "[100]\ttraining's auc: 0.859121\ttraining's binary_logloss: 0.290414\tvalid_1's auc: 0.845037\tvalid_1's binary_logloss: 0.30098\n",
      "[150]\ttraining's auc: 0.867827\ttraining's binary_logloss: 0.283553\tvalid_1's auc: 0.847976\tvalid_1's binary_logloss: 0.298623\n",
      "[200]\ttraining's auc: 0.874442\ttraining's binary_logloss: 0.278195\tvalid_1's auc: 0.849476\tvalid_1's binary_logloss: 0.297341\n",
      "[250]\ttraining's auc: 0.880023\ttraining's binary_logloss: 0.273758\tvalid_1's auc: 0.850107\tvalid_1's binary_logloss: 0.296851\n",
      "Early stopping, best iteration is:\n",
      "[267]\ttraining's auc: 0.881795\ttraining's binary_logloss: 0.272306\tvalid_1's auc: 0.850192\tvalid_1's binary_logloss: 0.296742\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.845149\ttraining's binary_logloss: 0.302018\tvalid_1's auc: 0.841277\tvalid_1's binary_logloss: 0.304816\n",
      "[100]\ttraining's auc: 0.859251\ttraining's binary_logloss: 0.290817\tvalid_1's auc: 0.848492\tvalid_1's binary_logloss: 0.298349\n",
      "[150]\ttraining's auc: 0.867776\ttraining's binary_logloss: 0.284034\tvalid_1's auc: 0.850917\tvalid_1's binary_logloss: 0.29598\n",
      "[200]\ttraining's auc: 0.874526\ttraining's binary_logloss: 0.278816\tvalid_1's auc: 0.851981\tvalid_1's binary_logloss: 0.29497\n",
      "[250]\ttraining's auc: 0.880262\ttraining's binary_logloss: 0.274279\tvalid_1's auc: 0.8523\tvalid_1's binary_logloss: 0.294581\n",
      "Early stopping, best iteration is:\n",
      "[266]\ttraining's auc: 0.88185\ttraining's binary_logloss: 0.272947\tvalid_1's auc: 0.852517\tvalid_1's binary_logloss: 0.294376\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.845377\ttraining's binary_logloss: 0.302024\tvalid_1's auc: 0.839251\tvalid_1's binary_logloss: 0.305542\n",
      "[100]\ttraining's auc: 0.85962\ttraining's binary_logloss: 0.2907\tvalid_1's auc: 0.847634\tvalid_1's binary_logloss: 0.298516\n",
      "[150]\ttraining's auc: 0.868296\ttraining's binary_logloss: 0.283837\tvalid_1's auc: 0.850486\tvalid_1's binary_logloss: 0.295989\n",
      "[200]\ttraining's auc: 0.874732\ttraining's binary_logloss: 0.278754\tvalid_1's auc: 0.851591\tvalid_1's binary_logloss: 0.294962\n",
      "[250]\ttraining's auc: 0.880371\ttraining's binary_logloss: 0.274275\tvalid_1's auc: 0.852197\tvalid_1's binary_logloss: 0.294337\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttraining's auc: 0.882561\ttraining's binary_logloss: 0.272517\tvalid_1's auc: 0.852349\tvalid_1's binary_logloss: 0.294206\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.846937\ttraining's binary_logloss: 0.301411\tvalid_1's auc: 0.831422\tvalid_1's binary_logloss: 0.308279\n",
      "[100]\ttraining's auc: 0.861361\ttraining's binary_logloss: 0.289623\tvalid_1's auc: 0.838625\tvalid_1's binary_logloss: 0.30214\n",
      "[150]\ttraining's auc: 0.870131\ttraining's binary_logloss: 0.28274\tvalid_1's auc: 0.841422\tvalid_1's binary_logloss: 0.300097\n",
      "[200]\ttraining's auc: 0.876616\ttraining's binary_logloss: 0.277543\tvalid_1's auc: 0.84245\tvalid_1's binary_logloss: 0.299265\n",
      "[250]\ttraining's auc: 0.882376\ttraining's binary_logloss: 0.272889\tvalid_1's auc: 0.84338\tvalid_1's binary_logloss: 0.298583\n",
      "Early stopping, best iteration is:\n",
      "[252]\ttraining's auc: 0.882559\ttraining's binary_logloss: 0.272728\tvalid_1's auc: 0.843383\tvalid_1's binary_logloss: 0.298584\n",
      "*********************\n",
      "roc auc estimado:  0.8499235989370193\n",
      "roc auc varianza:  0.0008241264815268242\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs = []\n",
    "train_probs = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index].target\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index].target\n",
    "\n",
    "    learner = LGBMClassifier(n_estimators=1000, boosting_type='gbdt',min_child_samples=1500, \n",
    "                   colsample_bytree=0.8,subsample=0.8, max_bin=200, learning_rate=0.1)\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50)\n",
    "    test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "train_probs = pd.concat(train_probs)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))\n",
    "print(\"roc auc varianza: \", np.std([roc_auc_score(y_train.loc[folds[i]], train_probs.iloc[folds[i]]) for i in range(len(folds))])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "express-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name = 'target'\n",
    "test_probs.to_csv('../results/lightgbm_with_1078features_0.84992.csv') ### score de 0.84599 en la tabla publica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('train_1078features.pkl')\n",
    "# test.to_pickle('test_1078features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-ottawa",
   "metadata": {},
   "source": [
    "## Procesar SUNAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "complex-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunat_train = pd.read_csv(f'{path}/sunat_train.csv')\n",
    "sunat_test = pd.read_csv(f'{path}/sunat_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "chicken-thesis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((292479, 18), (318821, 18))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### eliminar registros duplicados\n",
    "sunat_train.drop_duplicates(inplace=True)\n",
    "sunat_test.drop_duplicates(inplace=True)\n",
    "sunat_train.shape, sunat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "multiple-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'tipcontribuyente': 'int32',\n",
    "         'tippersona': 'int32',\n",
    "         'ciiu': 'int32',\n",
    "         'ubigeo': 'int32',\n",
    "         'condiciondomicilio': 'int32',\n",
    "         'estadocontribuyente': 'int32',\n",
    "         'codvia': 'int32',\n",
    "         'codzona': 'int32',\n",
    "         'contabilidad': 'int32',\n",
    "         'facturacion': 'int32',\n",
    "         'domiciliado': 'int32',\n",
    "         'comercioexterior': 'int32',\n",
    "         'cargorele': 'int32',\n",
    "         'codentidadtributo': 'int32',\n",
    "         'estadotributo': 'int32'}\n",
    "sunat_train = sunat_train.astype(dict_)\n",
    "sunat_test = sunat_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "recognized-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunat_train['diff_fech'] = sunat_train['fecbaja'] - sunat_train['fecalta']\n",
    "sunat_test['diff_fech'] = sunat_test['fecbaja'] - sunat_test['fecalta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "collectible-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "moda=lambda x: calculate_mode(x)\n",
    "moda.__name__='mode'\n",
    "agg_sunat = {'tipcontribuyente':['nunique',moda],\n",
    "           'tippersona':['nunique',moda],\n",
    "           'ciiu':['nunique', moda],\n",
    "           'ubigeo':['nunique',moda],\n",
    "           'condiciondomicilio':['nunique',moda],\n",
    "           'estadocontribuyente':['nunique',moda],\n",
    "           'codvia':['nunique',moda],\n",
    "           'codzona':['nunique',moda],\n",
    "           'contabilidad':['nunique',moda],\n",
    "           'facturacion':['nunique',moda],\n",
    "           'domiciliado':['nunique',moda],\n",
    "           'comercioexterior':['nunique',moda],\n",
    "           'cargorele':['nunique',moda],\n",
    "           'codentidadtributo':['nunique',moda],\n",
    "           'estadotributo':['nunique',moda],\n",
    "           'fecalta':['mean','max', 'nunique','min','std'],\n",
    "           'fecbaja':['mean','max', 'nunique','min','std'], \n",
    "           'diff_fech':['mean','max', 'nunique','min','std'], \n",
    "            }\n",
    "\n",
    "sunat_train_ = sunat_train.groupby('key_value').agg(agg_sunat)\n",
    "sunat_train_.columns = [i+'_'+j for i,j in sunat_train_.columns]\n",
    "sunat_test_ = sunat_test.groupby('key_value').agg(agg_sunat)\n",
    "sunat_test_.columns = [i+'_'+j for i,j in sunat_test_.columns]\n",
    "del sunat_train, sunat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "thick-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "### unir SUNAT en la base final\n",
    "train = train.join(sunat_train_)\n",
    "test = test.join(sunat_test_)\n",
    "del sunat_train_, sunat_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abroad-feelings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1123), (396666, 1123))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "imperial-numbers",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.845801\ttraining's binary_logloss: 0.301333\tvalid_1's auc: 0.837149\tvalid_1's binary_logloss: 0.307805\n",
      "[100]\ttraining's auc: 0.860437\ttraining's binary_logloss: 0.289712\tvalid_1's auc: 0.846237\tvalid_1's binary_logloss: 0.300648\n",
      "[150]\ttraining's auc: 0.869258\ttraining's binary_logloss: 0.282753\tvalid_1's auc: 0.849549\tvalid_1's binary_logloss: 0.297963\n",
      "[200]\ttraining's auc: 0.875982\ttraining's binary_logloss: 0.277379\tvalid_1's auc: 0.851073\tvalid_1's binary_logloss: 0.296692\n",
      "[250]\ttraining's auc: 0.881774\ttraining's binary_logloss: 0.272785\tvalid_1's auc: 0.851484\tvalid_1's binary_logloss: 0.296332\n",
      "[300]\ttraining's auc: 0.88701\ttraining's binary_logloss: 0.268602\tvalid_1's auc: 0.852077\tvalid_1's binary_logloss: 0.295892\n",
      "Early stopping, best iteration is:\n",
      "[316]\ttraining's auc: 0.888411\ttraining's binary_logloss: 0.2674\tvalid_1's auc: 0.852183\tvalid_1's binary_logloss: 0.295783\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.845621\ttraining's binary_logloss: 0.301518\tvalid_1's auc: 0.837118\tvalid_1's binary_logloss: 0.307729\n",
      "[100]\ttraining's auc: 0.860119\ttraining's binary_logloss: 0.290054\tvalid_1's auc: 0.845933\tvalid_1's binary_logloss: 0.300787\n",
      "[150]\ttraining's auc: 0.868924\ttraining's binary_logloss: 0.282946\tvalid_1's auc: 0.849534\tvalid_1's binary_logloss: 0.297928\n",
      "[200]\ttraining's auc: 0.875718\ttraining's binary_logloss: 0.277459\tvalid_1's auc: 0.850936\tvalid_1's binary_logloss: 0.29665\n",
      "Early stopping, best iteration is:\n",
      "[215]\ttraining's auc: 0.877399\ttraining's binary_logloss: 0.276063\tvalid_1's auc: 0.851062\tvalid_1's binary_logloss: 0.296484\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.845703\ttraining's binary_logloss: 0.301855\tvalid_1's auc: 0.841569\tvalid_1's binary_logloss: 0.304758\n",
      "[100]\ttraining's auc: 0.860076\ttraining's binary_logloss: 0.290407\tvalid_1's auc: 0.849576\tvalid_1's binary_logloss: 0.29795\n",
      "[150]\ttraining's auc: 0.868974\ttraining's binary_logloss: 0.283402\tvalid_1's auc: 0.852034\tvalid_1's binary_logloss: 0.295505\n",
      "[200]\ttraining's auc: 0.8758\ttraining's binary_logloss: 0.277974\tvalid_1's auc: 0.853335\tvalid_1's binary_logloss: 0.294251\n",
      "[250]\ttraining's auc: 0.881511\ttraining's binary_logloss: 0.273337\tvalid_1's auc: 0.853669\tvalid_1's binary_logloss: 0.29378\n",
      "[300]\ttraining's auc: 0.886658\ttraining's binary_logloss: 0.269145\tvalid_1's auc: 0.854203\tvalid_1's binary_logloss: 0.293344\n",
      "Early stopping, best iteration is:\n",
      "[290]\ttraining's auc: 0.885639\ttraining's binary_logloss: 0.26996\tvalid_1's auc: 0.854228\tvalid_1's binary_logloss: 0.293327\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.845657\ttraining's binary_logloss: 0.301822\tvalid_1's auc: 0.839165\tvalid_1's binary_logloss: 0.305848\n",
      "[100]\ttraining's auc: 0.860239\ttraining's binary_logloss: 0.290344\tvalid_1's auc: 0.847997\tvalid_1's binary_logloss: 0.298599\n",
      "[150]\ttraining's auc: 0.868921\ttraining's binary_logloss: 0.283426\tvalid_1's auc: 0.851038\tvalid_1's binary_logloss: 0.29601\n",
      "[200]\ttraining's auc: 0.875646\ttraining's binary_logloss: 0.27813\tvalid_1's auc: 0.852385\tvalid_1's binary_logloss: 0.294778\n",
      "[250]\ttraining's auc: 0.88128\ttraining's binary_logloss: 0.273599\tvalid_1's auc: 0.852921\tvalid_1's binary_logloss: 0.294192\n",
      "Early stopping, best iteration is:\n",
      "[273]\ttraining's auc: 0.883726\ttraining's binary_logloss: 0.27162\tvalid_1's auc: 0.853143\tvalid_1's binary_logloss: 0.293963\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.847546\ttraining's binary_logloss: 0.301261\tvalid_1's auc: 0.8321\tvalid_1's binary_logloss: 0.307979\n",
      "[100]\ttraining's auc: 0.861901\ttraining's binary_logloss: 0.289487\tvalid_1's auc: 0.839655\tvalid_1's binary_logloss: 0.301651\n",
      "[150]\ttraining's auc: 0.870875\ttraining's binary_logloss: 0.282284\tvalid_1's auc: 0.842489\tvalid_1's binary_logloss: 0.29928\n",
      "[200]\ttraining's auc: 0.877552\ttraining's binary_logloss: 0.276819\tvalid_1's auc: 0.843786\tvalid_1's binary_logloss: 0.298238\n",
      "[250]\ttraining's auc: 0.883264\ttraining's binary_logloss: 0.2722\tvalid_1's auc: 0.844535\tvalid_1's binary_logloss: 0.297659\n",
      "Early stopping, best iteration is:\n",
      "[266]\ttraining's auc: 0.884932\ttraining's binary_logloss: 0.27086\tvalid_1's auc: 0.844677\tvalid_1's binary_logloss: 0.297543\n",
      "*********************\n",
      "roc auc estimado:  0.8510847686114105\n",
      "roc auc varianza:  0.0008213315003251726\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs = []\n",
    "train_probs = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index].target\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index].target\n",
    "\n",
    "    learner = LGBMClassifier(n_estimators=1000, boosting_type='gbdt',min_child_samples=1500, \n",
    "                   colsample_bytree=0.8,subsample=0.8, max_bin=200, learning_rate=0.1)\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50)\n",
    "    test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "train_probs = pd.concat(train_probs)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))\n",
    "print(\"roc auc varianza: \", np.std([roc_auc_score(y_train.loc[folds[i]], train_probs.iloc[folds[i]]) for i in range(len(folds))])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "competitive-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name = 'target'\n",
    "test_probs.to_csv('../results/lightgbm_with_1123features_0.85108.csv') ### score de 0.84723 en la tabla publica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "forward-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('train_1123features.pkl')\n",
    "# test.to_pickle('test_1123features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "important-cycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de variables : 1123\n",
      "variables con importancia acumulada al 99% : 1048\n",
      "variables con zero importancia : 4\n"
     ]
    }
   ],
   "source": [
    "zero_importance = fi[fi==0]\n",
    "aux = fi[fi>0].sort_values(ascending=False)\n",
    "keep_columns = []\n",
    "count = 0\n",
    "for feature,values in zip(aux.index, aux.values):\n",
    "    count+=values\n",
    "    if count<=0.99:\n",
    "        keep_columns.append(feature)\n",
    "\n",
    "print(f'total de variables : {len(train.columns)}')\n",
    "print(f'variables con importancia acumulada al 99% : {len(keep_columns)}')\n",
    "print(f'variables con zero importancia : {len(zero_importance)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "composed-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[keep_columns]\n",
    "test = test[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ancient-petroleum",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1048), (396666, 1048))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "young-ferry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.845481\ttraining's binary_logloss: 0.301328\tvalid_1's auc: 0.836866\tvalid_1's binary_logloss: 0.3081\n",
      "[100]\ttraining's auc: 0.860185\ttraining's binary_logloss: 0.289838\tvalid_1's auc: 0.846036\tvalid_1's binary_logloss: 0.301014\n",
      "[150]\ttraining's auc: 0.869007\ttraining's binary_logloss: 0.282863\tvalid_1's auc: 0.849365\tvalid_1's binary_logloss: 0.298266\n",
      "[200]\ttraining's auc: 0.875714\ttraining's binary_logloss: 0.27756\tvalid_1's auc: 0.851014\tvalid_1's binary_logloss: 0.296969\n",
      "[250]\ttraining's auc: 0.881521\ttraining's binary_logloss: 0.272957\tvalid_1's auc: 0.851811\tvalid_1's binary_logloss: 0.29636\n",
      "[300]\ttraining's auc: 0.886627\ttraining's binary_logloss: 0.268763\tvalid_1's auc: 0.85235\tvalid_1's binary_logloss: 0.295868\n",
      "Early stopping, best iteration is:\n",
      "[297]\ttraining's auc: 0.886302\ttraining's binary_logloss: 0.269021\tvalid_1's auc: 0.852364\tvalid_1's binary_logloss: 0.295882\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.845525\ttraining's binary_logloss: 0.301685\tvalid_1's auc: 0.837152\tvalid_1's binary_logloss: 0.307602\n",
      "[100]\ttraining's auc: 0.860364\ttraining's binary_logloss: 0.28993\tvalid_1's auc: 0.846349\tvalid_1's binary_logloss: 0.300457\n",
      "[150]\ttraining's auc: 0.868962\ttraining's binary_logloss: 0.282968\tvalid_1's auc: 0.849565\tvalid_1's binary_logloss: 0.297761\n",
      "[200]\ttraining's auc: 0.875638\ttraining's binary_logloss: 0.27756\tvalid_1's auc: 0.85091\tvalid_1's binary_logloss: 0.296604\n",
      "[250]\ttraining's auc: 0.881558\ttraining's binary_logloss: 0.272886\tvalid_1's auc: 0.851583\tvalid_1's binary_logloss: 0.296026\n",
      "Early stopping, best iteration is:\n",
      "[288]\ttraining's auc: 0.885412\ttraining's binary_logloss: 0.26981\tvalid_1's auc: 0.85213\tvalid_1's binary_logloss: 0.295611\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.845388\ttraining's binary_logloss: 0.302107\tvalid_1's auc: 0.841018\tvalid_1's binary_logloss: 0.305042\n",
      "[100]\ttraining's auc: 0.860154\ttraining's binary_logloss: 0.290331\tvalid_1's auc: 0.849385\tvalid_1's binary_logloss: 0.297953\n",
      "[150]\ttraining's auc: 0.869095\ttraining's binary_logloss: 0.283316\tvalid_1's auc: 0.852045\tvalid_1's binary_logloss: 0.295458\n",
      "[200]\ttraining's auc: 0.875926\ttraining's binary_logloss: 0.277947\tvalid_1's auc: 0.852974\tvalid_1's binary_logloss: 0.294488\n",
      "Early stopping, best iteration is:\n",
      "[227]\ttraining's auc: 0.879151\ttraining's binary_logloss: 0.275389\tvalid_1's auc: 0.853275\tvalid_1's binary_logloss: 0.294145\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.84527\ttraining's binary_logloss: 0.30224\tvalid_1's auc: 0.838924\tvalid_1's binary_logloss: 0.305974\n",
      "[100]\ttraining's auc: 0.860044\ttraining's binary_logloss: 0.290549\tvalid_1's auc: 0.847333\tvalid_1's binary_logloss: 0.299001\n",
      "[150]\ttraining's auc: 0.868883\ttraining's binary_logloss: 0.283577\tvalid_1's auc: 0.850517\tvalid_1's binary_logloss: 0.296296\n",
      "[200]\ttraining's auc: 0.875692\ttraining's binary_logloss: 0.278158\tvalid_1's auc: 0.851869\tvalid_1's binary_logloss: 0.294944\n",
      "[250]\ttraining's auc: 0.881377\ttraining's binary_logloss: 0.273532\tvalid_1's auc: 0.852464\tvalid_1's binary_logloss: 0.294322\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttraining's auc: 0.884654\ttraining's binary_logloss: 0.270849\tvalid_1's auc: 0.852787\tvalid_1's binary_logloss: 0.29402\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.847323\ttraining's binary_logloss: 0.301319\tvalid_1's auc: 0.831716\tvalid_1's binary_logloss: 0.308093\n",
      "[100]\ttraining's auc: 0.861981\ttraining's binary_logloss: 0.289463\tvalid_1's auc: 0.839819\tvalid_1's binary_logloss: 0.301489\n",
      "[150]\ttraining's auc: 0.870852\ttraining's binary_logloss: 0.282395\tvalid_1's auc: 0.842503\tvalid_1's binary_logloss: 0.299295\n",
      "[200]\ttraining's auc: 0.877789\ttraining's binary_logloss: 0.27688\tvalid_1's auc: 0.843976\tvalid_1's binary_logloss: 0.298207\n",
      "Early stopping, best iteration is:\n",
      "[222]\ttraining's auc: 0.880351\ttraining's binary_logloss: 0.274839\tvalid_1's auc: 0.844345\tvalid_1's binary_logloss: 0.29792\n",
      "*********************\n",
      "roc auc estimado:  0.8510019669729569\n",
      "roc auc varianza:  0.0008164862192310212\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs = []\n",
    "train_probs = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index].target\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index].target\n",
    "\n",
    "    learner = LGBMClassifier(n_estimators=1000, boosting_type='gbdt',min_child_samples=1500, \n",
    "                   colsample_bytree=0.8,subsample=0.8, max_bin=200, learning_rate=0.1)\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50)\n",
    "    test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "train_probs = pd.concat(train_probs)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))\n",
    "print(\"roc auc varianza: \", np.std([roc_auc_score(y_train.loc[folds[i]], train_probs.iloc[folds[i]]) for i in range(len(folds))])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "painted-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name = 'target'\n",
    "test_probs.to_csv('../results/lightgbm_with_1048features_0.85100.csv') ### score de 0.84695 en la tabla publica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "colonial-eclipse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de variables : 1048\n",
      "variables con importancia acumulada al 99% : 997\n",
      "variables con zero importancia : 0\n"
     ]
    }
   ],
   "source": [
    "zero_importance = fi[fi==0]\n",
    "aux = fi[fi>0].sort_values(ascending=False)\n",
    "keep_columns = []\n",
    "count = 0\n",
    "for feature,values in zip(aux.index, aux.values):\n",
    "    count+=values\n",
    "    if count<=0.99:\n",
    "        keep_columns.append(feature)\n",
    "\n",
    "print(f'total de variables : {len(train.columns)}')\n",
    "print(f'variables con importancia acumulada al 99% : {len(keep_columns)}')\n",
    "print(f'variables con zero importancia : {len(zero_importance)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
