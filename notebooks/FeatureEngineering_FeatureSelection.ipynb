{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rotary-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm \n",
    "import gc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "different-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stat\n",
    "def calculate_mode(x):\n",
    "    try:\n",
    "        moda=stat.mode(x)\n",
    "    except:\n",
    "        moda=np.nan\n",
    "    return moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "former-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data'\n",
    "y_train = pd.read_csv(f'{path}/y_train.csv', index_col = 'key_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tracked-favorite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1064), (396666, 1064))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_pickle('train_1064features.pkl')\n",
    "test = pd.read_pickle('test_1064features.pkl')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-joint",
   "metadata": {},
   "source": [
    "## Procesar SUNAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "suffering-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunat_train = pd.read_csv(f'{path}/sunat_train.csv')\n",
    "sunat_test = pd.read_csv(f'{path}/sunat_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "similar-faith",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((292479, 18), (318821, 18))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### eliminar registros duplicados\n",
    "sunat_train.drop_duplicates(inplace=True)\n",
    "sunat_test.drop_duplicates(inplace=True)\n",
    "sunat_train.shape, sunat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abroad-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'tipcontribuyente': 'int32',\n",
    "         'tippersona': 'int32',\n",
    "         'ciiu': 'int32',\n",
    "         'ubigeo': 'int32',\n",
    "         'condiciondomicilio': 'int32',\n",
    "         'estadocontribuyente': 'int32',\n",
    "         'codvia': 'int32',\n",
    "         'codzona': 'int32',\n",
    "         'contabilidad': 'int32',\n",
    "         'facturacion': 'int32',\n",
    "         'domiciliado': 'int32',\n",
    "         'comercioexterior': 'int32',\n",
    "         'cargorele': 'int32',\n",
    "         'codentidadtributo': 'int32',\n",
    "         'estadotributo': 'int32'}\n",
    "sunat_train = sunat_train.astype(dict_)\n",
    "sunat_test = sunat_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "virgin-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunat_train['diff_fech'] = sunat_train['fecbaja'] - sunat_train['fecalta']\n",
    "sunat_test['diff_fech'] = sunat_test['fecbaja'] - sunat_test['fecalta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "moda=lambda x: calculate_mode(x)\n",
    "moda.__name__='mode'\n",
    "agg_sunat = {'tipcontribuyente':['nunique',moda],\n",
    "           'tippersona':['nunique',moda],\n",
    "           'ciiu':['nunique', moda],\n",
    "           'ubigeo':['nunique',moda],\n",
    "           'condiciondomicilio':['nunique',moda],\n",
    "           'estadocontribuyente':['nunique',moda],\n",
    "           'codvia':['nunique',moda],\n",
    "           'codzona':['nunique',moda],\n",
    "           'contabilidad':['nunique',moda],\n",
    "           'facturacion':['nunique',moda],\n",
    "           'domiciliado':['nunique',moda],\n",
    "           'comercioexterior':['nunique',moda],\n",
    "           'cargorele':['nunique',moda],\n",
    "           'codentidadtributo':['nunique',moda],\n",
    "           'estadotributo':['nunique',moda],\n",
    "           'fecalta':['mean','max', 'nunique','min','std'],\n",
    "           'fecbaja':['mean','max', 'nunique','min','std'], \n",
    "           'diff_fech':['mean','max', 'nunique','min','std'], \n",
    "            }\n",
    "\n",
    "sunat_train_ = sunat_train.groupby('key_value').agg(agg_sunat)\n",
    "sunat_train_.columns = [i+'_'+j for i,j in sunat_train_.columns]\n",
    "sunat_test_ = sunat_test.groupby('key_value').agg(agg_sunat)\n",
    "sunat_test_.columns = [i+'_'+j for i,j in sunat_test_.columns]\n",
    "del sunat_train, sunat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "little-relation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1109), (396666, 1109))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### unir SUNAT en la base final\n",
    "train = train.join(sunat_train_)\n",
    "test = test.join(sunat_test_)\n",
    "del sunat_train_, sunat_test_\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exciting-interface",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838898\ttraining's binary_logloss: 0.304707\tvalid_1's auc: 0.831139\tvalid_1's binary_logloss: 0.310739\n",
      "[100]\ttraining's auc: 0.852605\ttraining's binary_logloss: 0.294738\tvalid_1's auc: 0.838601\tvalid_1's binary_logloss: 0.305226\n",
      "[150]\ttraining's auc: 0.861071\ttraining's binary_logloss: 0.288522\tvalid_1's auc: 0.841252\tvalid_1's binary_logloss: 0.303124\n",
      "[200]\ttraining's auc: 0.867923\ttraining's binary_logloss: 0.28343\tvalid_1's auc: 0.842414\tvalid_1's binary_logloss: 0.30222\n",
      "[250]\ttraining's auc: 0.874006\ttraining's binary_logloss: 0.278962\tvalid_1's auc: 0.84297\tvalid_1's binary_logloss: 0.301801\n",
      "[300]\ttraining's auc: 0.879019\ttraining's binary_logloss: 0.275043\tvalid_1's auc: 0.84343\tvalid_1's binary_logloss: 0.301518\n",
      "Early stopping, best iteration is:\n",
      "[319]\ttraining's auc: 0.880877\ttraining's binary_logloss: 0.273608\tvalid_1's auc: 0.84363\tvalid_1's binary_logloss: 0.301381\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.839022\ttraining's binary_logloss: 0.304934\tvalid_1's auc: 0.830102\tvalid_1's binary_logloss: 0.311014\n",
      "[100]\ttraining's auc: 0.852928\ttraining's binary_logloss: 0.294683\tvalid_1's auc: 0.838019\tvalid_1's binary_logloss: 0.305062\n",
      "[150]\ttraining's auc: 0.861274\ttraining's binary_logloss: 0.288483\tvalid_1's auc: 0.840272\tvalid_1's binary_logloss: 0.303203\n",
      "[200]\ttraining's auc: 0.867809\ttraining's binary_logloss: 0.283522\tvalid_1's auc: 0.841291\tvalid_1's binary_logloss: 0.30228\n",
      "[250]\ttraining's auc: 0.873723\ttraining's binary_logloss: 0.279135\tvalid_1's auc: 0.841798\tvalid_1's binary_logloss: 0.301818\n",
      "[300]\ttraining's auc: 0.879238\ttraining's binary_logloss: 0.275053\tvalid_1's auc: 0.842288\tvalid_1's binary_logloss: 0.301469\n",
      "Early stopping, best iteration is:\n",
      "[298]\ttraining's auc: 0.878986\ttraining's binary_logloss: 0.275221\tvalid_1's auc: 0.842327\tvalid_1's binary_logloss: 0.301466\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.839198\ttraining's binary_logloss: 0.304938\tvalid_1's auc: 0.833987\tvalid_1's binary_logloss: 0.308591\n",
      "[100]\ttraining's auc: 0.852658\ttraining's binary_logloss: 0.29499\tvalid_1's auc: 0.840703\tvalid_1's binary_logloss: 0.30319\n",
      "[150]\ttraining's auc: 0.861007\ttraining's binary_logloss: 0.288892\tvalid_1's auc: 0.842561\tvalid_1's binary_logloss: 0.301516\n",
      "[200]\ttraining's auc: 0.867689\ttraining's binary_logloss: 0.283924\tvalid_1's auc: 0.843597\tvalid_1's binary_logloss: 0.300636\n",
      "[250]\ttraining's auc: 0.873509\ttraining's binary_logloss: 0.27966\tvalid_1's auc: 0.84456\tvalid_1's binary_logloss: 0.300002\n",
      "[300]\ttraining's auc: 0.878682\ttraining's binary_logloss: 0.275545\tvalid_1's auc: 0.844899\tvalid_1's binary_logloss: 0.299605\n",
      "Early stopping, best iteration is:\n",
      "[303]\ttraining's auc: 0.879007\ttraining's binary_logloss: 0.275298\tvalid_1's auc: 0.844922\tvalid_1's binary_logloss: 0.29958\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.839265\ttraining's binary_logloss: 0.305068\tvalid_1's auc: 0.8319\tvalid_1's binary_logloss: 0.309435\n",
      "[100]\ttraining's auc: 0.853047\ttraining's binary_logloss: 0.294928\tvalid_1's auc: 0.83911\tvalid_1's binary_logloss: 0.303886\n",
      "[150]\ttraining's auc: 0.861425\ttraining's binary_logloss: 0.288649\tvalid_1's auc: 0.841802\tvalid_1's binary_logloss: 0.301712\n",
      "[200]\ttraining's auc: 0.868146\ttraining's binary_logloss: 0.283629\tvalid_1's auc: 0.84276\tvalid_1's binary_logloss: 0.300767\n",
      "Early stopping, best iteration is:\n",
      "[196]\ttraining's auc: 0.867684\ttraining's binary_logloss: 0.283982\tvalid_1's auc: 0.842837\tvalid_1's binary_logloss: 0.300767\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.840732\ttraining's binary_logloss: 0.304716\tvalid_1's auc: 0.826184\tvalid_1's binary_logloss: 0.310699\n",
      "[100]\ttraining's auc: 0.854402\ttraining's binary_logloss: 0.294393\tvalid_1's auc: 0.832644\tvalid_1's binary_logloss: 0.305759\n",
      "[150]\ttraining's auc: 0.863142\ttraining's binary_logloss: 0.287955\tvalid_1's auc: 0.835315\tvalid_1's binary_logloss: 0.303901\n",
      "[200]\ttraining's auc: 0.869956\ttraining's binary_logloss: 0.282891\tvalid_1's auc: 0.836063\tvalid_1's binary_logloss: 0.303277\n",
      "Early stopping, best iteration is:\n",
      "[207]\ttraining's auc: 0.870754\ttraining's binary_logloss: 0.282271\tvalid_1's auc: 0.836261\tvalid_1's binary_logloss: 0.303148\n",
      "*********************\n",
      "roc auc estimado:  0.8419945464021086\n",
      "roc auc varianza:  0.0007401485136384678\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "test_probs = []\n",
    "train_probs = []\n",
    "fi = []\n",
    "for i, idx in enumerate(folds):\n",
    "    print(\"*\"*10, i, \"*\"*10)\n",
    "    Xt = train.loc[idx]\n",
    "    yt = y_train.loc[Xt.index].target\n",
    "\n",
    "    Xv = train.drop(Xt.index)\n",
    "    yv = y_train.loc[Xv.index].target\n",
    "\n",
    "    learner = LGBMClassifier(n_estimators=1000, boosting_type='gbdt',min_child_samples=1500, \n",
    "                   colsample_bytree=0.8,subsample=0.8, max_bin=200, learning_rate=0.1)\n",
    "    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                eval_set=[(Xt, yt), (Xv, yv)], verbose=50)\n",
    "    test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "    train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "    fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "train_probs = pd.concat(train_probs)\n",
    "fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "print(\"*\" * 21)\n",
    "print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))\n",
    "print(\"roc auc varianza: \", np.std([roc_auc_score(y_train.loc[folds[i]], train_probs.iloc[folds[i]]) for i in range(len(folds))])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "narrative-tracker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_probs.name = 'target'\n",
    "# test_probs.to_csv('../results/lightgbm_with_1109features_sinSE_0.84199.csv') \n",
    "# score de 0.8424 ---------a ---  0.84479  en la tabla publica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-windows",
   "metadata": {},
   "source": [
    "## Procesar SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fatal-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_train = pd.read_csv(f'{path}/se_train.csv', index_col = 'key_value')\n",
    "se_test = pd.read_csv(f'{path}/se_test.csv', index_col = 'key_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "expressed-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'sexo':'int32',\n",
    "         'est_cvl':'int32',\n",
    "         'sit_lab':'int32',\n",
    "         'cod_ocu':'int32',\n",
    "         'ctd_hijos':'int32',\n",
    "         'flg_sin_email':'int32',\n",
    "         'ctd_veh':'int32',\n",
    "         'lgr_vot':'int32',\n",
    "         'prv':'int32',\n",
    "         'dto':'int32',\n",
    "         'rgn':'int32',\n",
    "         'tip_lvledu':'int32'}\n",
    "se_train = se_train.astype(dict_)\n",
    "se_test = se_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "obvious-dietary",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validation_lightgbm(train, test, y_train):\n",
    "    folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "    test_probs = []\n",
    "    train_probs = []\n",
    "    fi = []\n",
    "    for i, idx in enumerate(folds):\n",
    "        print(\"*\"*10, i, \"*\"*10)\n",
    "        Xt = train.loc[idx]\n",
    "        yt = y_train.loc[Xt.index].target\n",
    "\n",
    "        Xv = train.drop(Xt.index)\n",
    "        yv = y_train.loc[Xv.index].target\n",
    "\n",
    "        learner = LGBMClassifier(n_estimators=1000, boosting_type='gbdt',min_child_samples=1500, \n",
    "                       colsample_bytree=0.8,subsample=0.8, max_bin=200, learning_rate=0.1)\n",
    "        learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                    eval_set=[(Xt, yt), (Xv, yv)], verbose=50)\n",
    "        test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "        train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "        fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "        gc.collect()\n",
    "    test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "    train_probs = pd.concat(train_probs)\n",
    "    fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "    print(\"*\" * 21)\n",
    "    print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))\n",
    "    print(\"roc auc varianza: \", np.std([roc_auc_score(y_train.loc[folds[i]], train_probs.iloc[folds[i]]) for i in range(len(folds))]))\n",
    "    return test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "instrumental-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(df, feature):\n",
    "    one_hot = pd.get_dummies(df[feature])\n",
    "    one_hot.columns = [feature+'_'+str(i) for i in one_hot.columns]\n",
    "    return one_hot\n",
    "one_hot_sexo_train = get_one_hot(se_train, 'sexo')\n",
    "one_hot_sexo_test = get_one_hot(se_test, 'sexo')\n",
    "one_hot_est_cvl_train = get_one_hot(se_train, 'est_cvl')\n",
    "one_hot_est_cvl_test = get_one_hot(se_test, 'est_cvl')\n",
    "one_hot_rgn_train = get_one_hot(se_train, 'rgn')\n",
    "one_hot_rgn_test = get_one_hot(se_test, 'rgn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "competitive-french",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(one_hot_sexo_train).join(one_hot_est_cvl_train).join(one_hot_rgn_train).join(se_train[['edad','ctd_veh']])\n",
    "test = test.join(one_hot_sexo_test).join(one_hot_est_cvl_test).join(one_hot_rgn_test).join(se_test[['edad','ctd_veh']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "applied-local",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.842519\ttraining's binary_logloss: 0.303194\tvalid_1's auc: 0.834209\tvalid_1's binary_logloss: 0.309338\n",
      "[100]\ttraining's auc: 0.856883\ttraining's binary_logloss: 0.292244\tvalid_1's auc: 0.842555\tvalid_1's binary_logloss: 0.302936\n",
      "[150]\ttraining's auc: 0.865238\ttraining's binary_logloss: 0.285795\tvalid_1's auc: 0.84521\tvalid_1's binary_logloss: 0.300813\n",
      "[200]\ttraining's auc: 0.872005\ttraining's binary_logloss: 0.280588\tvalid_1's auc: 0.846986\tvalid_1's binary_logloss: 0.299555\n",
      "[250]\ttraining's auc: 0.877827\ttraining's binary_logloss: 0.276056\tvalid_1's auc: 0.847567\tvalid_1's binary_logloss: 0.299087\n",
      "Early stopping, best iteration is:\n",
      "[268]\ttraining's auc: 0.879506\ttraining's binary_logloss: 0.27466\tvalid_1's auc: 0.847734\tvalid_1's binary_logloss: 0.298954\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.842644\ttraining's binary_logloss: 0.30331\tvalid_1's auc: 0.833688\tvalid_1's binary_logloss: 0.309416\n",
      "[100]\ttraining's auc: 0.856736\ttraining's binary_logloss: 0.292322\tvalid_1's auc: 0.841994\tvalid_1's binary_logloss: 0.30304\n",
      "[150]\ttraining's auc: 0.865043\ttraining's binary_logloss: 0.285869\tvalid_1's auc: 0.844832\tvalid_1's binary_logloss: 0.300791\n",
      "[200]\ttraining's auc: 0.871553\ttraining's binary_logloss: 0.280819\tvalid_1's auc: 0.845851\tvalid_1's binary_logloss: 0.299964\n",
      "[250]\ttraining's auc: 0.877357\ttraining's binary_logloss: 0.27631\tvalid_1's auc: 0.84652\tvalid_1's binary_logloss: 0.299416\n",
      "Early stopping, best iteration is:\n",
      "[273]\ttraining's auc: 0.879592\ttraining's binary_logloss: 0.274503\tvalid_1's auc: 0.846741\tvalid_1's binary_logloss: 0.29928\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.843119\ttraining's binary_logloss: 0.303412\tvalid_1's auc: 0.837487\tvalid_1's binary_logloss: 0.30717\n",
      "[100]\ttraining's auc: 0.857001\ttraining's binary_logloss: 0.292488\tvalid_1's auc: 0.844816\tvalid_1's binary_logloss: 0.300943\n",
      "[150]\ttraining's auc: 0.865625\ttraining's binary_logloss: 0.285987\tvalid_1's auc: 0.847213\tvalid_1's binary_logloss: 0.29878\n",
      "[200]\ttraining's auc: 0.872283\ttraining's binary_logloss: 0.280944\tvalid_1's auc: 0.848006\tvalid_1's binary_logloss: 0.297991\n",
      "Early stopping, best iteration is:\n",
      "[233]\ttraining's auc: 0.876021\ttraining's binary_logloss: 0.27795\tvalid_1's auc: 0.848348\tvalid_1's binary_logloss: 0.297589\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.84279\ttraining's binary_logloss: 0.303496\tvalid_1's auc: 0.835709\tvalid_1's binary_logloss: 0.307779\n",
      "[100]\ttraining's auc: 0.856821\ttraining's binary_logloss: 0.292679\tvalid_1's auc: 0.843699\tvalid_1's binary_logloss: 0.30148\n",
      "[150]\ttraining's auc: 0.865462\ttraining's binary_logloss: 0.286094\tvalid_1's auc: 0.846544\tvalid_1's binary_logloss: 0.299087\n",
      "[200]\ttraining's auc: 0.872048\ttraining's binary_logloss: 0.281\tvalid_1's auc: 0.847598\tvalid_1's binary_logloss: 0.298012\n",
      "[250]\ttraining's auc: 0.877951\ttraining's binary_logloss: 0.276408\tvalid_1's auc: 0.84817\tvalid_1's binary_logloss: 0.297442\n",
      "Early stopping, best iteration is:\n",
      "[269]\ttraining's auc: 0.88015\ttraining's binary_logloss: 0.27474\tvalid_1's auc: 0.848334\tvalid_1's binary_logloss: 0.297343\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.844132\ttraining's binary_logloss: 0.303173\tvalid_1's auc: 0.829027\tvalid_1's binary_logloss: 0.309765\n",
      "[100]\ttraining's auc: 0.858604\ttraining's binary_logloss: 0.291878\tvalid_1's auc: 0.836813\tvalid_1's binary_logloss: 0.303729\n",
      "[150]\ttraining's auc: 0.867233\ttraining's binary_logloss: 0.285158\tvalid_1's auc: 0.839279\tvalid_1's binary_logloss: 0.301795\n",
      "[200]\ttraining's auc: 0.874047\ttraining's binary_logloss: 0.279939\tvalid_1's auc: 0.840453\tvalid_1's binary_logloss: 0.300925\n",
      "[250]\ttraining's auc: 0.879583\ttraining's binary_logloss: 0.275499\tvalid_1's auc: 0.841004\tvalid_1's binary_logloss: 0.30045\n",
      "Early stopping, best iteration is:\n",
      "[248]\ttraining's auc: 0.879363\ttraining's binary_logloss: 0.275667\tvalid_1's auc: 0.841049\tvalid_1's binary_logloss: 0.300441\n",
      "*********************\n",
      "roc auc estimado:  0.8464651856646881\n",
      "roc auc varianza:  0.0006744691517409108\n"
     ]
    }
   ],
   "source": [
    "test_probs = cross_validation_lightgbm(train, test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "honest-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name = 'target'\n",
    "test_probs.to_csv('../results/lightgbm_with_oneHotSexo_oneHotEstCvl_Edad_OneHotRgn_CtdVeh_0.84646.csv') \n",
    "\n",
    "#                           rcc de 0.8420                  . auc estimado 0.83999\n",
    "\n",
    "# score de 0.8424 ---------a ---  0.84479  en la tabla publica. Una mejore de 0.0023 respecto al score anterior y una mejora\n",
    "#                                 de 0.00279 respecto al test de validacion . auc estimado 0.84199 ***** rcc y sunat\n",
    "# score de 0.84479 ---------a ---  0.84696  en la tabla publica. Una mejore de 0.0021 respecto al score anterior y una mejora\n",
    "#                                 de 0.00264 respecto al test de validacion . auc estimado 0.84432 *******\n",
    "\n",
    "# score de 0.84696  --------a---- 0.84583 . Es decir ha reducido . auc estimado 0.84640 --- one_hot_tip_lvledu_train\n",
    "# score de 0.84696  --------a---- 0.84594 . Es decir ha reducido . auc estimado 0.84664 --- tip_lvledu_train\n",
    "\n",
    "# score de 0.84696 ---------a ---  0.84760  en la tabla publica. Una mejore de 0.00063 respecto al score anterior y una mejora\n",
    "#                                 de 0.00240 respecto al test de validacion . auc estimado 0.84519 ****\n",
    "\n",
    "# score de 0.84760  --------a---- 0.84614 . Es decir ha reducido . auc estimado 0.84717 --- flg_sin_email\n",
    "\n",
    "# score de 0.84760  --------a---- 0.84786 en la tabla publica. Una mejora de 0.00026 respecto al score anterior y una mejora de\n",
    "#                                   0.00218 respecto al test de validacion. auc estimado de 0.84568 *******\n",
    "\n",
    "# score de 0.84786  --------a---- 0.84757 . Es decir ha reducido . auc estimado 0.84627 --- OneHotSitLab\n",
    "# score de 0.84786  --------a---- 0.84779 . Es decir ha reducido . auc estimado 0.84541 --- Dto\n",
    "\n",
    "# score de 0.84786  --------a---- 0.84816 en la tabla publica. Una mejora de 0.00030 respecto al score anterior y una mejora de\n",
    "#                                 0.00204 respecto al test de validacion. auc estimado de  0.84612 *******\n",
    "\n",
    "# score de 0.84816  --------a---- 0.84829 en la tabla publica. Una mejora de 0.00013 respecto al score anterior y una mejora de\n",
    "#                                 0.00183 respecto al test de validacion. auc estimado de  0.84646 *******\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "### sexo tiene 4 clases  ### freq 0\n",
    "### est_cvl tiene 6 clases  ### freq 4\n",
    "### sit_lab tiene 4 clases  ### freq 1\n",
    "### cod_ocu tiene 39 clases  ### freq 22\n",
    "### ctd_hijos tiene 12 clases  ### freq 0\n",
    "### flg_sin_email tiene 2 clases  ### freq 1\n",
    "### lgr_vot tiene 2070 clases  ### freq 18\n",
    "### prv tiene 197 clases  ### freq 111\n",
    "### dto tiene 1662 clases  ### freq 949\n",
    "### rgn tiene 7 clases  ### freq 3\n",
    "### tip_lvledu tiene 8 clases  ### freq 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "considerable-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import category_encoders as ce\n",
    "# X_binenc_train = train[['dto']]\n",
    "# encoder = ce.BinaryEncoder(cols=['dto'])\n",
    "# X_binenc_train = encoder.fit_transform(X_binenc_train)\n",
    "# X_binenc_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se_columns = ['edad', 'sexo', 'est_cvl', 'sit_lab', 'cod_ocu', 'ctd_hijos', 'flg_sin_email',\n",
    "#                 'ctd_veh', 'cod_ubi', 'lgr_vot', 'prv', 'dto', 'rgn', 'tip_lvledu', 'edad']\n",
    "# for feature in se_columns[9:]:\n",
    "#     keep_columns = list(set(train.columns).difference([feature]))\n",
    "#     print(\"*\"*40, f'eliminando {feature}', \"*\"*40)\n",
    "#     cross_validation_lightgbm(train[keep_columns], test[keep_columns], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### eliminar\n",
    "#lgr_vot , cod_ocu, cod_ubi, prv\n",
    "\n",
    "### probados\n",
    "# sexo, tip_lvledu, edad, flg_sin_email, est_cvl, sit_lab, dto, rgn, ctd_hijos, ctd_veh\n",
    "####### mantener oficialmente\n",
    "# one hot sexo , edad , one hot est_cvl, one hot rgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "silent-planet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1128), (396666, 1128))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "separated-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle('train_1128features.pkl')\n",
    "test.to_pickle('test_1128features.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
