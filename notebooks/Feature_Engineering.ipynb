{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "entertaining-agreement",
   "metadata": {},
   "source": [
    "## Importar Librerías Estándares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collectible-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-croatia",
   "metadata": {},
   "source": [
    "## Declarar Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accompanied-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcc():\n",
    "    rcc_train = pd.read_csv(f'{path}/rcc_train.csv')\n",
    "    rcc_test = pd.read_csv(f'{path}/rcc_test.csv')\n",
    "    ##### inputar datos faltantes en la base de rcc_test\n",
    "    rcc_test['cod_instit_financiera'].fillna(rcc_test['cod_instit_financiera'].value_counts().index[0], inplace=True)\n",
    "    rcc_test['PRODUCTO'].fillna(rcc_test['PRODUCTO'].value_counts().index[0], inplace=True)\n",
    "\n",
    "    dict_ = {'codmes': 'int32',\n",
    "     'key_value': 'int32',\n",
    "     'condicion': 'int32',\n",
    "     'tipo_credito': 'int32',\n",
    "     'cod_instit_financiera': 'int32',\n",
    "     'PRODUCTO': 'int32',\n",
    "     'RIESGO_DIRECTO': 'int32',\n",
    "     'COD_CLASIFICACION_DEUDOR': 'int32'}\n",
    "    rcc_train = rcc_train.astype(dict_)\n",
    "    rcc_test = rcc_test.astype(dict_)\n",
    "    return rcc_train, rcc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hybrid-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcc_list(df, desde, n):\n",
    "    rcc_list=[]\n",
    "    for feature in ['cod_instit_financiera','PRODUCTO']:\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Sum'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Unique'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Min'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Max'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Std'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Mean'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Median'))\n",
    "    return rcc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "japanese-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mode(x):\n",
    "    try:\n",
    "        moda=stat.mode(x)\n",
    "    except:\n",
    "        moda=np.nan\n",
    "    return moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "geological-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unstack(df, feature, desde, n, func):\n",
    "    print(f'haciendo unstack de {feature} desde {desde} con la funcion de agregacion {func}')\n",
    "    if func =='Sum':\n",
    "        df_result = df[df.codmes>=desde].groupby(['key_value', feature]).saldo.sum().unstack(level = 1, fill_value=np.nan)\n",
    "    if func == 'Unique':\n",
    "        df_result = df[df.codmes>=desde].groupby(['key_value', feature]).saldo.size().unstack(level = 1, fill_value=np.nan)\n",
    "    if func == 'Min':\n",
    "        df_result = df[df.codmes>=desde].groupby(['key_value', feature]).saldo.min().unstack(level = 1, fill_value=np.nan)\n",
    "    if func == 'Max':\n",
    "        df_result = df[df.codmes>=desde].groupby(['key_value', feature]).saldo.max().unstack(level = 1, fill_value=np.nan)\n",
    "    if func == 'Std':\n",
    "        df_result = df[df.codmes>=desde].groupby(['key_value', feature]).saldo.std().unstack(level = 1, fill_value=np.nan)\n",
    "    if func == 'Mean':\n",
    "        df_result = df[df.codmes>=desde].groupby(['key_value', feature]).saldo.mean().unstack(level = 1, fill_value=np.nan)\n",
    "    if func == 'Median':\n",
    "        df_result = df[df.codmes>=desde].groupby(['key_value', feature]).saldo.median().unstack(level = 1, fill_value=np.nan)\n",
    "    df_result.columns = [f'{df_result.columns.name}_{value}_saldo{func}_ult{n+1}meses' for value in df_result.columns]\n",
    "    gc.collect()      \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tight-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keep_columns(train, test):\n",
    "    print(train.shape, test.shape)\n",
    "    keep_columns = list(set(train.columns).intersection(test.columns))\n",
    "    train = train[keep_columns]\n",
    "    test = test[keep_columns]\n",
    "    print(train.shape, test.shape)\n",
    "    return  train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "drawn-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validation_lightgbm(train, y_train, test):\n",
    "    folds = [train.index[t] for t, v in KFold(5).split(train)]\n",
    "    test_probs = []\n",
    "    train_probs = []\n",
    "    fi = []\n",
    "    for i, idx in enumerate(folds):\n",
    "        print(\"*\"*10, i, \"*\"*10)\n",
    "        Xt = train.loc[idx]\n",
    "        yt = y_train.loc[Xt.index].target\n",
    "\n",
    "        Xv = train.drop(Xt.index)\n",
    "        yv = y_train.loc[Xv.index].target\n",
    "\n",
    "        learner = LGBMClassifier(n_estimators=1000, boosting_type='gbdt',min_child_samples=1500, \n",
    "                       colsample_bytree=0.8,subsample=0.8, max_bin=200, learning_rate=0.1, random_state=42)\n",
    "        learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n",
    "                    eval_set=[(Xt, yt), (Xv, yv)], verbose=50)\n",
    "        test_probs.append(pd.Series(learner.predict_proba(test)[:, -1], index=test.index, name=\"fold_\" + str(i)))\n",
    "        train_probs.append(pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index, name=\"probs\"))\n",
    "        fi.append(pd.Series(learner.feature_importances_ / learner.feature_importances_.sum(), index=Xt.columns))\n",
    "\n",
    "    test_probs = pd.concat(test_probs, axis=1).mean(axis=1)\n",
    "    train_probs = pd.concat(train_probs)\n",
    "    fi = pd.concat(fi, axis=1).mean(axis=1)\n",
    "    print(\"*\" * 21)\n",
    "    print(\"roc auc estimado: \", roc_auc_score(y_train, train_probs.loc[y_train.index]))\n",
    "    print(\"roc auc varianza: \", np.std([roc_auc_score(y_train.loc[folds[i]], train_probs.iloc[folds[i]]) for i in range(len(folds))])) \n",
    "    return test_probs, fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affiliated-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_selection(fi):\n",
    "    zero_importance = fi[fi==0]\n",
    "    aux = fi[fi>0].sort_values(ascending=False)\n",
    "    keep_columns = []\n",
    "    count = 0\n",
    "    for feature,values in zip(aux.index, aux.values):\n",
    "        count+=values\n",
    "        if count<=0.99:\n",
    "            keep_columns.append(feature)\n",
    "            \n",
    "    print(f'total de variables : {len(fi)}')\n",
    "    print(f'variables con importancia acumulada al 99% : {len(keep_columns)}')\n",
    "    print(f'variables con zero importancia : {len(zero_importance)}')\n",
    "    return keep_columns, zero_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "absent-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(df, feature):\n",
    "    one_hot = pd.get_dummies(df[feature])\n",
    "    one_hot.columns = [feature+'_'+str(i) for i in one_hot.columns]\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "identical-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crosstab(df, index, feature):\n",
    "    df = pd.crosstab(df[index], df[feature])\n",
    "    df.columns = [f'{df.columns.name}_{i}' for i in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-cologne",
   "metadata": {},
   "source": [
    "## Cargar la variable Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "convertible-blues",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data'\n",
    "y_train = pd.read_csv(f'{path}/y_train.csv', index_col = 'key_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "internal-planet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train.value_counts().plot(kind='pie', autopct='%1.0f%%');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-april",
   "metadata": {},
   "source": [
    "## Procesar RCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "emerging-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc_train = pd.read_csv(f'{path}/rcc_train.csv')\n",
    "rcc_test = pd.read_csv(f'{path}/rcc_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tough-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### inputar datos faltantes en la base de rcc_test\n",
    "rcc_test['cod_instit_financiera'].fillna(rcc_test['cod_instit_financiera'].value_counts().index[0], inplace=True)\n",
    "rcc_test['PRODUCTO'].fillna(rcc_test['PRODUCTO'].value_counts().index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "controlled-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'codmes': 'int32',\n",
    " 'key_value': 'int32',\n",
    " 'condicion': 'int32',\n",
    " 'tipo_credito': 'int32',\n",
    " 'cod_instit_financiera': 'int32',\n",
    " 'PRODUCTO': 'int32',\n",
    " 'RIESGO_DIRECTO': 'int32',\n",
    " 'COD_CLASIFICACION_DEUDOR': 'int32'}\n",
    "rcc_train = rcc_train.astype(dict_)\n",
    "rcc_test = rcc_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "general-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### categorizar la variable condicion\n",
    "bins = [-1, 0, 10, 20, 30, 60, 90, 180, 360, 720, float(\"inf\")]\n",
    "rcc_train[\"condicion\"] = pd.cut(rcc_train.condicion, bins)\n",
    "rcc_train[\"condicion\"] = rcc_train[\"condicion\"].cat.codes\n",
    "rcc_test[\"condicion\"] = pd.cut(rcc_test.condicion, bins)\n",
    "rcc_test[\"condicion\"] = rcc_test[\"condicion\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "micro-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "### definir las variables que seran agregadas\n",
    "moda=lambda x: calculate_mode(x)\n",
    "moda.__name__='mode'\n",
    "agg_rcc = {'cod_instit_financiera':['nunique','min','max',moda],\n",
    "           'PRODUCTO':['nunique','min','max',moda],}\n",
    "def get_aggregate(df, desde,n):\n",
    "    print(f'haciendo aggregate de cod_instit_financiera y Producto desde {desde}')\n",
    "    df_result = df[df.codmes>=desde].groupby('key_value').agg(agg_rcc)\n",
    "    df_result.columns = [feature+'_'+agg+f'_ult{n+1}mes' for feature, agg in df_result.columns]    \n",
    "    gc.collect()  \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aging-mother",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de tipo_credito desde 201802 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201802 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201802 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201802 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201802 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201802 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201802 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201802 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201802 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201802 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201802 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201802 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201802 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201802 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201802 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201802 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201802 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201802 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201802 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201802 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201802 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201802 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201802 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201802 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201802 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201802 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201802 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201802 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201802\n",
      "haciendo unstack de tipo_credito desde 201801 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201801 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201801 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201801 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201801 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201801 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201801 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201801 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201801 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201801 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201801 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201801 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201801 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201801 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201801 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201801 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201801 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201801 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201801 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201801 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201801 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201801 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201801 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201801 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201801 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201801 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201801 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201801 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201801\n",
      "haciendo unstack de tipo_credito desde 201712 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201712 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201712 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201712 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201712 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201712 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201712 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201712 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201712 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201712 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201712 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201712 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201712 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201712 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201712 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201712 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201712 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201712 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201712 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201712 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201712 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201712 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201712 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201712 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201712 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201712 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201712 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201712 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201712\n",
      "haciendo unstack de tipo_credito desde 201711 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201711 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201711 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201711 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201711 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201711 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201711 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201711 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201711 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201711 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201711 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201711 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201711 con la funcion de agregacion Mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de RIESGO_DIRECTO desde 201711 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201711 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201711 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201711 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201711 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201711 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201711 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201711 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201711 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201711 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201711 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201711 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201711 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201711 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201711 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201711\n",
      "haciendo unstack de tipo_credito desde 201710 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201710 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201710 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201710 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201710 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201710 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201710 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201710 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201710 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201710 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201710 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201710 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201710 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201710 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201710 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201710 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201710 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201710 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201710 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201710 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201710 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201710 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201710 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201710 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201710 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201710 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201710 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201710 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201710\n",
      "haciendo unstack de tipo_credito desde 201709 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201709 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201709 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201709 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201709 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201709 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201709 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201709 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201709 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201709 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201709 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201709 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201709 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201709 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201709 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201709 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201709 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201709 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201709 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201709 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201709 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201709 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201709 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201709 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201709 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201709 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201709 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201709 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201709\n",
      "haciendo unstack de tipo_credito desde 201708 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201708 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201708 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201708 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201708 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201708 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201708 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201708 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201708 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201708 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201708 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201708 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201708 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201708 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201708 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201708 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201708 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201708 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201708 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201708 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201708 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201708 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201708 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201708 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201708 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201708 con la funcion de agregacion Std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de condicion desde 201708 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201708 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201708\n",
      "haciendo unstack de tipo_credito desde 201707 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201707 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201707 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201707 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201707 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201707 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201707 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201707 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201707 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201707 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201707 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201707 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201707 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201707 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201707 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201707 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201707 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201707 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201707 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201707 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201707 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201707 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201707 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201707 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201707 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201707 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201707 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201707 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201707\n",
      "haciendo unstack de tipo_credito desde 201706 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201706 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201706 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201706 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201706 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201706 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201706 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201706 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201706 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201706 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201706 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201706 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201706 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201706 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201706 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201706 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201706 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201706 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201706 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201706 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201706 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201706 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201706 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201706 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201706 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201706 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201706 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201706 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201706\n",
      "haciendo unstack de tipo_credito desde 201705 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201705 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201705 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201705 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201705 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201705 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201705 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201705 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201705 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201705 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201705 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201705 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201705 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201705 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201705 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201705 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201705 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201705 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201705 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201705 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201705 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201705 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201705 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201705 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201705 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201705 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201705 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201705 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201705\n",
      "haciendo unstack de tipo_credito desde 201704 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201704 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201704 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201704 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201704 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201704 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201704 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201704 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201704 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201704 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201704 con la funcion de agregacion Max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de RIESGO_DIRECTO desde 201704 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201704 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201704 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201704 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201704 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201704 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201704 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201704 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201704 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201704 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201704 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201704 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201704 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201704 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201704 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201704 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201704 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201704\n",
      "haciendo unstack de tipo_credito desde 201703 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201703 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201703 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201703 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201703 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201703 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201703 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201703 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201703 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201703 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201703 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201703 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201703 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201703 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201703 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201703 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201703 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201703 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201703 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201703 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201703 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201703 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201703 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201703 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201703 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201703 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201703 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201703 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201703\n"
     ]
    }
   ],
   "source": [
    "rcc_train_list=[]\n",
    "for n,desde in enumerate(sorted(set(rcc_train.codmes),reverse=True)):\n",
    "    for feature in ['tipo_credito','RIESGO_DIRECTO','COD_CLASIFICACION_DEUDOR','condicion']:\n",
    "        rcc_train_list.append(get_unstack(rcc_train, feature, desde, n, 'Sum'))\n",
    "        rcc_train_list.append(get_unstack(rcc_train, feature, desde, n, 'Unique'))\n",
    "        rcc_train_list.append(get_unstack(rcc_train, feature, desde, n, 'Min'))\n",
    "        rcc_train_list.append(get_unstack(rcc_train, feature, desde, n, 'Max'))\n",
    "        rcc_train_list.append(get_unstack(rcc_train, feature, desde, n, 'Std'))\n",
    "        rcc_train_list.append(get_unstack(rcc_train, feature, desde, n, 'Mean'))\n",
    "        rcc_train_list.append(get_unstack(rcc_train, feature, desde, n, 'Median'))\n",
    "    rcc_train_list.append(get_aggregate(rcc_train, desde,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fuzzy-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc_train = pd.concat(rcc_train_list, axis=1)\n",
    "del rcc_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "assured-timeline",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de tipo_credito desde 201902 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201902 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201902 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201902 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201902 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201902 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201902 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201902 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201902 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201902 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201902 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201902 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201902 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201902 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201902 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201902 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201902 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201902 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201902 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201902 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201902 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201902 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201902 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201902 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201902 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201902 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201902 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201902 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201902\n",
      "haciendo unstack de tipo_credito desde 201901 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201901 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201901 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201901 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201901 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201901 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201901 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201901 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201901 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201901 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201901 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201901 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201901 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201901 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201901 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201901 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201901 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201901 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201901 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201901 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201901 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201901 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201901 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201901 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201901 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201901 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201901 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201901 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201901\n",
      "haciendo unstack de tipo_credito desde 201812 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201812 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201812 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201812 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201812 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201812 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201812 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201812 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201812 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201812 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201812 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201812 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201812 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201812 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201812 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201812 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201812 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201812 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201812 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201812 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201812 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201812 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201812 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201812 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201812 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201812 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201812 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201812 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201812\n",
      "haciendo unstack de tipo_credito desde 201811 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201811 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201811 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201811 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201811 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201811 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201811 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201811 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201811 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201811 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201811 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201811 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201811 con la funcion de agregacion Mean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de RIESGO_DIRECTO desde 201811 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201811 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201811 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201811 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201811 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201811 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201811 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201811 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201811 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201811 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201811 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201811 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201811 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201811 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201811 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201811\n",
      "haciendo unstack de tipo_credito desde 201810 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201810 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201810 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201810 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201810 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201810 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201810 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201810 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201810 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201810 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201810 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201810 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201810 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201810 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201810 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201810 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201810 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201810 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201810 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201810 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201810 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201810 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201810 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201810 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201810 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201810 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201810 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201810 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201810\n",
      "haciendo unstack de tipo_credito desde 201809 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201809 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201809 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201809 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201809 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201809 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201809 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201809 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201809 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201809 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201809 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201809 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201809 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201809 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201809 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201809 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201809 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201809 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201809 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201809 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201809 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201809 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201809 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201809 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201809 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201809 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201809 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201809 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201809\n",
      "haciendo unstack de tipo_credito desde 201808 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201808 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201808 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201808 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201808 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201808 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201808 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201808 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201808 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201808 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201808 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201808 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201808 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201808 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201808 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201808 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201808 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201808 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201808 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201808 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201808 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201808 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201808 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201808 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201808 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201808 con la funcion de agregacion Std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de condicion desde 201808 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201808 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201808\n",
      "haciendo unstack de tipo_credito desde 201807 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201807 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201807 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201807 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201807 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201807 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201807 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201807 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201807 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201807 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201807 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201807 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201807 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201807 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201807 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201807 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201807 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201807 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201807 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201807 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201807 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201807 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201807 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201807 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201807 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201807 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201807 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201807 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201807\n",
      "haciendo unstack de tipo_credito desde 201806 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201806 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201806 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201806 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201806 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201806 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201806 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201806 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201806 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201806 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201806 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201806 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201806 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201806 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201806 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201806 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201806 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201806 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201806 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201806 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201806 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201806 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201806 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201806 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201806 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201806 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201806 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201806 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201806\n",
      "haciendo unstack de tipo_credito desde 201805 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201805 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201805 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201805 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201805 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201805 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201805 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201805 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201805 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201805 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201805 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201805 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201805 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201805 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201805 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201805 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201805 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201805 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201805 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201805 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201805 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201805 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201805 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201805 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201805 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201805 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201805 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201805 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201805\n",
      "haciendo unstack de tipo_credito desde 201804 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201804 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201804 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201804 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201804 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201804 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201804 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201804 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201804 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201804 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201804 con la funcion de agregacion Max\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de RIESGO_DIRECTO desde 201804 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201804 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201804 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201804 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201804 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201804 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201804 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201804 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201804 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201804 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201804 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201804 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201804 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201804 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201804 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201804 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201804 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201804\n",
      "haciendo unstack de tipo_credito desde 201803 con la funcion de agregacion Sum\n",
      "haciendo unstack de tipo_credito desde 201803 con la funcion de agregacion Unique\n",
      "haciendo unstack de tipo_credito desde 201803 con la funcion de agregacion Min\n",
      "haciendo unstack de tipo_credito desde 201803 con la funcion de agregacion Max\n",
      "haciendo unstack de tipo_credito desde 201803 con la funcion de agregacion Std\n",
      "haciendo unstack de tipo_credito desde 201803 con la funcion de agregacion Mean\n",
      "haciendo unstack de tipo_credito desde 201803 con la funcion de agregacion Median\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201803 con la funcion de agregacion Sum\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201803 con la funcion de agregacion Unique\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201803 con la funcion de agregacion Min\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201803 con la funcion de agregacion Max\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201803 con la funcion de agregacion Std\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201803 con la funcion de agregacion Mean\n",
      "haciendo unstack de RIESGO_DIRECTO desde 201803 con la funcion de agregacion Median\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201803 con la funcion de agregacion Sum\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201803 con la funcion de agregacion Unique\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201803 con la funcion de agregacion Min\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201803 con la funcion de agregacion Max\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201803 con la funcion de agregacion Std\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201803 con la funcion de agregacion Mean\n",
      "haciendo unstack de COD_CLASIFICACION_DEUDOR desde 201803 con la funcion de agregacion Median\n",
      "haciendo unstack de condicion desde 201803 con la funcion de agregacion Sum\n",
      "haciendo unstack de condicion desde 201803 con la funcion de agregacion Unique\n",
      "haciendo unstack de condicion desde 201803 con la funcion de agregacion Min\n",
      "haciendo unstack de condicion desde 201803 con la funcion de agregacion Max\n",
      "haciendo unstack de condicion desde 201803 con la funcion de agregacion Std\n",
      "haciendo unstack de condicion desde 201803 con la funcion de agregacion Mean\n",
      "haciendo unstack de condicion desde 201803 con la funcion de agregacion Median\n",
      "haciendo aggregate de cod_instit_financiera y Producto desde 201803\n"
     ]
    }
   ],
   "source": [
    "rcc_test_list=[]\n",
    "for n,desde in enumerate(sorted(set(rcc_test.codmes),reverse=True)):\n",
    "    for feature in ['tipo_credito','RIESGO_DIRECTO','COD_CLASIFICACION_DEUDOR','condicion']:\n",
    "        rcc_test_list.append(get_unstack(rcc_test, feature, desde, n, 'Sum'))\n",
    "        rcc_test_list.append(get_unstack(rcc_test, feature, desde, n, 'Unique'))\n",
    "        rcc_test_list.append(get_unstack(rcc_test, feature, desde, n, 'Min'))\n",
    "        rcc_test_list.append(get_unstack(rcc_test, feature, desde, n, 'Max'))\n",
    "        rcc_test_list.append(get_unstack(rcc_test, feature, desde, n, 'Std'))\n",
    "        rcc_test_list.append(get_unstack(rcc_test, feature, desde, n, 'Mean'))\n",
    "        rcc_test_list.append(get_unstack(rcc_test, feature, desde, n, 'Median'))\n",
    "    rcc_test_list.append(get_aggregate(rcc_test, desde,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "arabic-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc_test = pd.concat(rcc_test_list, axis=1)\n",
    "del rcc_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "peripheral-shopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358487, 2532) (396666, 2616)\n",
      "(358487, 2532) (396666, 2532)\n"
     ]
    }
   ],
   "source": [
    "rcc_train, rcc_test = get_keep_columns(rcc_train, rcc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aerial-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "### unir RCC en la base final\n",
    "train = rcc_train\n",
    "test = rcc_test\n",
    "del rcc_train, rcc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "tutorial-acrylic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838761\ttraining's binary_logloss: 0.30485\tvalid_1's auc: 0.830664\tvalid_1's binary_logloss: 0.310894\n",
      "[100]\ttraining's auc: 0.851395\ttraining's binary_logloss: 0.295446\tvalid_1's auc: 0.837102\tvalid_1's binary_logloss: 0.30602\n",
      "[150]\ttraining's auc: 0.859498\ttraining's binary_logloss: 0.289473\tvalid_1's auc: 0.83939\tvalid_1's binary_logloss: 0.304297\n",
      "[200]\ttraining's auc: 0.8658\ttraining's binary_logloss: 0.284764\tvalid_1's auc: 0.840575\tvalid_1's binary_logloss: 0.303488\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttraining's auc: 0.868473\ttraining's binary_logloss: 0.282752\tvalid_1's auc: 0.84075\tvalid_1's binary_logloss: 0.303343\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.8384\ttraining's binary_logloss: 0.30498\tvalid_1's auc: 0.829638\tvalid_1's binary_logloss: 0.31107\n",
      "[100]\ttraining's auc: 0.851342\ttraining's binary_logloss: 0.295405\tvalid_1's auc: 0.836875\tvalid_1's binary_logloss: 0.305864\n",
      "[150]\ttraining's auc: 0.859372\ttraining's binary_logloss: 0.289495\tvalid_1's auc: 0.83894\tvalid_1's binary_logloss: 0.304285\n",
      "[200]\ttraining's auc: 0.865639\ttraining's binary_logloss: 0.284775\tvalid_1's auc: 0.839589\tvalid_1's binary_logloss: 0.30367\n",
      "[250]\ttraining's auc: 0.871017\ttraining's binary_logloss: 0.280714\tvalid_1's auc: 0.840133\tvalid_1's binary_logloss: 0.303314\n",
      "Early stopping, best iteration is:\n",
      "[273]\ttraining's auc: 0.873368\ttraining's binary_logloss: 0.278945\tvalid_1's auc: 0.840295\tvalid_1's binary_logloss: 0.303209\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.83813\ttraining's binary_logloss: 0.305452\tvalid_1's auc: 0.833161\tvalid_1's binary_logloss: 0.308845\n",
      "[100]\ttraining's auc: 0.850721\ttraining's binary_logloss: 0.296013\tvalid_1's auc: 0.839065\tvalid_1's binary_logloss: 0.303887\n",
      "[150]\ttraining's auc: 0.85876\ttraining's binary_logloss: 0.290227\tvalid_1's auc: 0.840874\tvalid_1's binary_logloss: 0.302232\n",
      "[200]\ttraining's auc: 0.864757\ttraining's binary_logloss: 0.285665\tvalid_1's auc: 0.841766\tvalid_1's binary_logloss: 0.301452\n",
      "Early stopping, best iteration is:\n",
      "[234]\ttraining's auc: 0.868881\ttraining's binary_logloss: 0.282691\tvalid_1's auc: 0.842254\tvalid_1's binary_logloss: 0.301091\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838498\ttraining's binary_logloss: 0.305334\tvalid_1's auc: 0.83173\tvalid_1's binary_logloss: 0.30936\n",
      "[100]\ttraining's auc: 0.851328\ttraining's binary_logloss: 0.295888\tvalid_1's auc: 0.83767\tvalid_1's binary_logloss: 0.304534\n",
      "[150]\ttraining's auc: 0.859379\ttraining's binary_logloss: 0.290026\tvalid_1's auc: 0.839486\tvalid_1's binary_logloss: 0.3029\n",
      "[200]\ttraining's auc: 0.865553\ttraining's binary_logloss: 0.285357\tvalid_1's auc: 0.840375\tvalid_1's binary_logloss: 0.302103\n",
      "Early stopping, best iteration is:\n",
      "[231]\ttraining's auc: 0.869109\ttraining's binary_logloss: 0.282712\tvalid_1's auc: 0.840553\tvalid_1's binary_logloss: 0.301836\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.839982\ttraining's binary_logloss: 0.304752\tvalid_1's auc: 0.82548\tvalid_1's binary_logloss: 0.310985\n",
      "[100]\ttraining's auc: 0.853102\ttraining's binary_logloss: 0.295004\tvalid_1's auc: 0.83161\tvalid_1's binary_logloss: 0.306571\n",
      "[150]\ttraining's auc: 0.861086\ttraining's binary_logloss: 0.289084\tvalid_1's auc: 0.833404\tvalid_1's binary_logloss: 0.305241\n",
      "[200]\ttraining's auc: 0.867321\ttraining's binary_logloss: 0.284348\tvalid_1's auc: 0.834017\tvalid_1's binary_logloss: 0.304678\n",
      "Early stopping, best iteration is:\n",
      "[210]\ttraining's auc: 0.868547\ttraining's binary_logloss: 0.283455\tvalid_1's auc: 0.834142\tvalid_1's binary_logloss: 0.304592\n",
      "*********************\n",
      "roc auc estimado:  0.8396256259184243\n",
      "roc auc varianza:  0.0006892940731700594\n",
      "total de variables : 2532\n",
      "variables con importancia acumulada al 99% : 1459\n",
      "variables con zero importancia : 798\n",
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838473\ttraining's binary_logloss: 0.304884\tvalid_1's auc: 0.829481\tvalid_1's binary_logloss: 0.311346\n",
      "[100]\ttraining's auc: 0.85112\ttraining's binary_logloss: 0.295551\tvalid_1's auc: 0.83608\tvalid_1's binary_logloss: 0.306475\n",
      "[150]\ttraining's auc: 0.858924\ttraining's binary_logloss: 0.289755\tvalid_1's auc: 0.838258\tvalid_1's binary_logloss: 0.304911\n",
      "[200]\ttraining's auc: 0.865453\ttraining's binary_logloss: 0.284973\tvalid_1's auc: 0.839366\tvalid_1's binary_logloss: 0.304151\n",
      "[250]\ttraining's auc: 0.870864\ttraining's binary_logloss: 0.280845\tvalid_1's auc: 0.840002\tvalid_1's binary_logloss: 0.303728\n",
      "[300]\ttraining's auc: 0.875804\ttraining's binary_logloss: 0.277048\tvalid_1's auc: 0.840527\tvalid_1's binary_logloss: 0.303375\n",
      "Early stopping, best iteration is:\n",
      "[327]\ttraining's auc: 0.87831\ttraining's binary_logloss: 0.275125\tvalid_1's auc: 0.840732\tvalid_1's binary_logloss: 0.303295\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838433\ttraining's binary_logloss: 0.304935\tvalid_1's auc: 0.829629\tvalid_1's binary_logloss: 0.31115\n",
      "[100]\ttraining's auc: 0.851229\ttraining's binary_logloss: 0.295448\tvalid_1's auc: 0.836472\tvalid_1's binary_logloss: 0.30603\n",
      "[150]\ttraining's auc: 0.859488\ttraining's binary_logloss: 0.289581\tvalid_1's auc: 0.838557\tvalid_1's binary_logloss: 0.304519\n",
      "[200]\ttraining's auc: 0.865873\ttraining's binary_logloss: 0.284833\tvalid_1's auc: 0.839443\tvalid_1's binary_logloss: 0.303845\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttraining's auc: 0.869303\ttraining's binary_logloss: 0.282222\tvalid_1's auc: 0.839941\tvalid_1's binary_logloss: 0.30349\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838349\ttraining's binary_logloss: 0.305247\tvalid_1's auc: 0.833107\tvalid_1's binary_logloss: 0.308922\n",
      "[100]\ttraining's auc: 0.850848\ttraining's binary_logloss: 0.295867\tvalid_1's auc: 0.839135\tvalid_1's binary_logloss: 0.303943\n",
      "[150]\ttraining's auc: 0.858915\ttraining's binary_logloss: 0.290019\tvalid_1's auc: 0.840988\tvalid_1's binary_logloss: 0.302264\n",
      "Early stopping, best iteration is:\n",
      "[167]\ttraining's auc: 0.861185\ttraining's binary_logloss: 0.28837\tvalid_1's auc: 0.841372\tvalid_1's binary_logloss: 0.301955\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.83869\ttraining's binary_logloss: 0.305371\tvalid_1's auc: 0.831651\tvalid_1's binary_logloss: 0.309518\n",
      "[100]\ttraining's auc: 0.851438\ttraining's binary_logloss: 0.295889\tvalid_1's auc: 0.837662\tvalid_1's binary_logloss: 0.30461\n",
      "[150]\ttraining's auc: 0.859369\ttraining's binary_logloss: 0.290046\tvalid_1's auc: 0.839629\tvalid_1's binary_logloss: 0.30287\n",
      "[200]\ttraining's auc: 0.865974\ttraining's binary_logloss: 0.28513\tvalid_1's auc: 0.84034\tvalid_1's binary_logloss: 0.302169\n",
      "[250]\ttraining's auc: 0.871349\ttraining's binary_logloss: 0.281036\tvalid_1's auc: 0.840952\tvalid_1's binary_logloss: 0.301602\n",
      "Early stopping, best iteration is:\n",
      "[258]\ttraining's auc: 0.872059\ttraining's binary_logloss: 0.280489\tvalid_1's auc: 0.840977\tvalid_1's binary_logloss: 0.301594\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.840243\ttraining's binary_logloss: 0.30468\tvalid_1's auc: 0.825138\tvalid_1's binary_logloss: 0.311104\n",
      "[100]\ttraining's auc: 0.852963\ttraining's binary_logloss: 0.294941\tvalid_1's auc: 0.831179\tvalid_1's binary_logloss: 0.306479\n",
      "[150]\ttraining's auc: 0.860829\ttraining's binary_logloss: 0.289102\tvalid_1's auc: 0.832874\tvalid_1's binary_logloss: 0.305195\n",
      "[200]\ttraining's auc: 0.867455\ttraining's binary_logloss: 0.284275\tvalid_1's auc: 0.833568\tvalid_1's binary_logloss: 0.30459\n",
      "Early stopping, best iteration is:\n",
      "[217]\ttraining's auc: 0.86948\ttraining's binary_logloss: 0.282811\tvalid_1's auc: 0.833836\tvalid_1's binary_logloss: 0.304369\n",
      "*********************\n",
      "roc auc estimado:  0.8393800598275646\n",
      "roc auc varianza:  0.0006829157564354514\n",
      "total de variables : 1459\n",
      "variables con importancia acumulada al 99% : 1314\n",
      "variables con zero importancia : 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.83848\ttraining's binary_logloss: 0.30496\tvalid_1's auc: 0.830072\tvalid_1's binary_logloss: 0.311206\n",
      "[100]\ttraining's auc: 0.851438\ttraining's binary_logloss: 0.295314\tvalid_1's auc: 0.837142\tvalid_1's binary_logloss: 0.30594\n",
      "[150]\ttraining's auc: 0.859383\ttraining's binary_logloss: 0.289503\tvalid_1's auc: 0.839533\tvalid_1's binary_logloss: 0.304208\n",
      "[200]\ttraining's auc: 0.865539\ttraining's binary_logloss: 0.284822\tvalid_1's auc: 0.8405\tvalid_1's binary_logloss: 0.303507\n",
      "[250]\ttraining's auc: 0.871157\ttraining's binary_logloss: 0.280609\tvalid_1's auc: 0.841084\tvalid_1's binary_logloss: 0.303146\n",
      "Early stopping, best iteration is:\n",
      "[245]\ttraining's auc: 0.870715\ttraining's binary_logloss: 0.280983\tvalid_1's auc: 0.841082\tvalid_1's binary_logloss: 0.303127\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838422\ttraining's binary_logloss: 0.304985\tvalid_1's auc: 0.829796\tvalid_1's binary_logloss: 0.311189\n",
      "[100]\ttraining's auc: 0.851236\ttraining's binary_logloss: 0.295426\tvalid_1's auc: 0.836229\tvalid_1's binary_logloss: 0.30619\n",
      "[150]\ttraining's auc: 0.859467\ttraining's binary_logloss: 0.289405\tvalid_1's auc: 0.838585\tvalid_1's binary_logloss: 0.304515\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's auc: 0.863876\ttraining's binary_logloss: 0.286074\tvalid_1's auc: 0.839113\tvalid_1's binary_logloss: 0.303991\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838355\ttraining's binary_logloss: 0.305265\tvalid_1's auc: 0.833631\tvalid_1's binary_logloss: 0.308604\n",
      "[100]\ttraining's auc: 0.851075\ttraining's binary_logloss: 0.295819\tvalid_1's auc: 0.839453\tvalid_1's binary_logloss: 0.303679\n",
      "[150]\ttraining's auc: 0.858971\ttraining's binary_logloss: 0.290032\tvalid_1's auc: 0.841087\tvalid_1's binary_logloss: 0.302121\n",
      "[200]\ttraining's auc: 0.865593\ttraining's binary_logloss: 0.285287\tvalid_1's auc: 0.841778\tvalid_1's binary_logloss: 0.301381\n",
      "Early stopping, best iteration is:\n",
      "[238]\ttraining's auc: 0.869774\ttraining's binary_logloss: 0.282124\tvalid_1's auc: 0.841994\tvalid_1's binary_logloss: 0.301107\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838565\ttraining's binary_logloss: 0.305327\tvalid_1's auc: 0.831537\tvalid_1's binary_logloss: 0.309447\n",
      "[100]\ttraining's auc: 0.851241\ttraining's binary_logloss: 0.295969\tvalid_1's auc: 0.837635\tvalid_1's binary_logloss: 0.304603\n",
      "[150]\ttraining's auc: 0.859113\ttraining's binary_logloss: 0.290141\tvalid_1's auc: 0.839465\tvalid_1's binary_logloss: 0.302972\n",
      "[200]\ttraining's auc: 0.865382\ttraining's binary_logloss: 0.285441\tvalid_1's auc: 0.840178\tvalid_1's binary_logloss: 0.302231\n",
      "Early stopping, best iteration is:\n",
      "[199]\ttraining's auc: 0.865256\ttraining's binary_logloss: 0.285531\tvalid_1's auc: 0.840217\tvalid_1's binary_logloss: 0.302224\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.839994\ttraining's binary_logloss: 0.304811\tvalid_1's auc: 0.824977\tvalid_1's binary_logloss: 0.311206\n",
      "[100]\ttraining's auc: 0.853019\ttraining's binary_logloss: 0.294993\tvalid_1's auc: 0.831285\tvalid_1's binary_logloss: 0.306613\n",
      "[150]\ttraining's auc: 0.861\ttraining's binary_logloss: 0.28904\tvalid_1's auc: 0.833242\tvalid_1's binary_logloss: 0.305133\n",
      "[200]\ttraining's auc: 0.867214\ttraining's binary_logloss: 0.284363\tvalid_1's auc: 0.833913\tvalid_1's binary_logloss: 0.304533\n",
      "Early stopping, best iteration is:\n",
      "[228]\ttraining's auc: 0.870407\ttraining's binary_logloss: 0.281944\tvalid_1's auc: 0.834268\tvalid_1's binary_logloss: 0.304271\n",
      "*********************\n",
      "roc auc estimado:  0.8393581188747835\n",
      "roc auc varianza:  0.0006662303852742272\n",
      "total de variables : 1314\n",
      "variables con importancia acumulada al 99% : 1203\n",
      "variables con zero importancia : 3\n",
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838332\ttraining's binary_logloss: 0.304991\tvalid_1's auc: 0.829921\tvalid_1's binary_logloss: 0.311151\n",
      "[100]\ttraining's auc: 0.851397\ttraining's binary_logloss: 0.295384\tvalid_1's auc: 0.836447\tvalid_1's binary_logloss: 0.306183\n",
      "[150]\ttraining's auc: 0.859444\ttraining's binary_logloss: 0.289522\tvalid_1's auc: 0.838576\tvalid_1's binary_logloss: 0.304603\n",
      "[200]\ttraining's auc: 0.865827\ttraining's binary_logloss: 0.284838\tvalid_1's auc: 0.839727\tvalid_1's binary_logloss: 0.303886\n",
      "[250]\ttraining's auc: 0.871371\ttraining's binary_logloss: 0.280631\tvalid_1's auc: 0.840238\tvalid_1's binary_logloss: 0.303569\n",
      "Early stopping, best iteration is:\n",
      "[274]\ttraining's auc: 0.873673\ttraining's binary_logloss: 0.278788\tvalid_1's auc: 0.840544\tvalid_1's binary_logloss: 0.30335\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838406\ttraining's binary_logloss: 0.304973\tvalid_1's auc: 0.829843\tvalid_1's binary_logloss: 0.311105\n",
      "[100]\ttraining's auc: 0.851335\ttraining's binary_logloss: 0.295447\tvalid_1's auc: 0.836434\tvalid_1's binary_logloss: 0.306171\n",
      "[150]\ttraining's auc: 0.859134\ttraining's binary_logloss: 0.289621\tvalid_1's auc: 0.838267\tvalid_1's binary_logloss: 0.304646\n",
      "[200]\ttraining's auc: 0.865467\ttraining's binary_logloss: 0.284933\tvalid_1's auc: 0.839546\tvalid_1's binary_logloss: 0.303715\n",
      "[250]\ttraining's auc: 0.870968\ttraining's binary_logloss: 0.280825\tvalid_1's auc: 0.839931\tvalid_1's binary_logloss: 0.303384\n",
      "Early stopping, best iteration is:\n",
      "[241]\ttraining's auc: 0.870048\ttraining's binary_logloss: 0.281519\tvalid_1's auc: 0.839978\tvalid_1's binary_logloss: 0.303368\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838308\ttraining's binary_logloss: 0.305275\tvalid_1's auc: 0.833138\tvalid_1's binary_logloss: 0.308876\n",
      "[100]\ttraining's auc: 0.851014\ttraining's binary_logloss: 0.29589\tvalid_1's auc: 0.839221\tvalid_1's binary_logloss: 0.303773\n",
      "[150]\ttraining's auc: 0.859118\ttraining's binary_logloss: 0.290064\tvalid_1's auc: 0.840664\tvalid_1's binary_logloss: 0.302346\n",
      "[200]\ttraining's auc: 0.865293\ttraining's binary_logloss: 0.285454\tvalid_1's auc: 0.841266\tvalid_1's binary_logloss: 0.301682\n",
      "Early stopping, best iteration is:\n",
      "[228]\ttraining's auc: 0.868481\ttraining's binary_logloss: 0.283101\tvalid_1's auc: 0.841615\tvalid_1's binary_logloss: 0.301387\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838288\ttraining's binary_logloss: 0.305401\tvalid_1's auc: 0.831161\tvalid_1's binary_logloss: 0.309536\n",
      "[100]\ttraining's auc: 0.851327\ttraining's binary_logloss: 0.295896\tvalid_1's auc: 0.837729\tvalid_1's binary_logloss: 0.304485\n",
      "[150]\ttraining's auc: 0.859138\ttraining's binary_logloss: 0.290055\tvalid_1's auc: 0.839268\tvalid_1's binary_logloss: 0.302901\n",
      "[200]\ttraining's auc: 0.865811\ttraining's binary_logloss: 0.285321\tvalid_1's auc: 0.840007\tvalid_1's binary_logloss: 0.302268\n",
      "[250]\ttraining's auc: 0.871243\ttraining's binary_logloss: 0.281179\tvalid_1's auc: 0.840573\tvalid_1's binary_logloss: 0.301788\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's auc: 0.871811\ttraining's binary_logloss: 0.280751\tvalid_1's auc: 0.840681\tvalid_1's binary_logloss: 0.301721\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.83974\ttraining's binary_logloss: 0.304871\tvalid_1's auc: 0.824908\tvalid_1's binary_logloss: 0.311118\n",
      "[100]\ttraining's auc: 0.852793\ttraining's binary_logloss: 0.295102\tvalid_1's auc: 0.831035\tvalid_1's binary_logloss: 0.306572\n",
      "[150]\ttraining's auc: 0.860761\ttraining's binary_logloss: 0.289248\tvalid_1's auc: 0.83282\tvalid_1's binary_logloss: 0.305206\n",
      "[200]\ttraining's auc: 0.867277\ttraining's binary_logloss: 0.284432\tvalid_1's auc: 0.833711\tvalid_1's binary_logloss: 0.304547\n",
      "Early stopping, best iteration is:\n",
      "[201]\ttraining's auc: 0.867383\ttraining's binary_logloss: 0.284353\tvalid_1's auc: 0.833724\tvalid_1's binary_logloss: 0.304538\n",
      "*********************\n",
      "roc auc estimado:  0.839333312978189\n",
      "roc auc varianza:  0.0006937013744489198\n",
      "total de variables : 1203\n",
      "variables con importancia acumulada al 99% : 1131\n",
      "variables con zero importancia : 0\n"
     ]
    }
   ],
   "source": [
    "# eliminar variables con zero_importance\n",
    "while True:\n",
    "    test_probs, fi = cross_validation_lightgbm(train, y_train, test)\n",
    "    keep_columns, zero_importance = get_feature_selection(fi)\n",
    "    train = train[keep_columns]\n",
    "    test = test[keep_columns]\n",
    "    if len(zero_importance)==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "modern-tracy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838651\ttraining's binary_logloss: 0.304825\tvalid_1's auc: 0.83027\tvalid_1's binary_logloss: 0.310967\n",
      "[100]\ttraining's auc: 0.851202\ttraining's binary_logloss: 0.295509\tvalid_1's auc: 0.836648\tvalid_1's binary_logloss: 0.306163\n",
      "[150]\ttraining's auc: 0.859045\ttraining's binary_logloss: 0.289746\tvalid_1's auc: 0.839037\tvalid_1's binary_logloss: 0.304394\n",
      "[200]\ttraining's auc: 0.865655\ttraining's binary_logloss: 0.284863\tvalid_1's auc: 0.84009\tvalid_1's binary_logloss: 0.303597\n",
      "[250]\ttraining's auc: 0.871143\ttraining's binary_logloss: 0.280685\tvalid_1's auc: 0.840614\tvalid_1's binary_logloss: 0.303281\n",
      "Early stopping, best iteration is:\n",
      "[244]\ttraining's auc: 0.870581\ttraining's binary_logloss: 0.281146\tvalid_1's auc: 0.840623\tvalid_1's binary_logloss: 0.303267\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838819\ttraining's binary_logloss: 0.304788\tvalid_1's auc: 0.830154\tvalid_1's binary_logloss: 0.311124\n",
      "[100]\ttraining's auc: 0.851514\ttraining's binary_logloss: 0.295304\tvalid_1's auc: 0.836602\tvalid_1's binary_logloss: 0.306194\n",
      "[150]\ttraining's auc: 0.859295\ttraining's binary_logloss: 0.289421\tvalid_1's auc: 0.838604\tvalid_1's binary_logloss: 0.304567\n",
      "[200]\ttraining's auc: 0.865629\ttraining's binary_logloss: 0.28476\tvalid_1's auc: 0.839483\tvalid_1's binary_logloss: 0.303748\n",
      "[250]\ttraining's auc: 0.871238\ttraining's binary_logloss: 0.280531\tvalid_1's auc: 0.840149\tvalid_1's binary_logloss: 0.303345\n",
      "Early stopping, best iteration is:\n",
      "[263]\ttraining's auc: 0.872432\ttraining's binary_logloss: 0.279609\tvalid_1's auc: 0.840255\tvalid_1's binary_logloss: 0.303259\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838599\ttraining's binary_logloss: 0.305128\tvalid_1's auc: 0.833192\tvalid_1's binary_logloss: 0.30886\n",
      "[100]\ttraining's auc: 0.851415\ttraining's binary_logloss: 0.295667\tvalid_1's auc: 0.839387\tvalid_1's binary_logloss: 0.303801\n",
      "[150]\ttraining's auc: 0.859268\ttraining's binary_logloss: 0.28995\tvalid_1's auc: 0.840946\tvalid_1's binary_logloss: 0.302193\n",
      "[200]\ttraining's auc: 0.865611\ttraining's binary_logloss: 0.285351\tvalid_1's auc: 0.841736\tvalid_1's binary_logloss: 0.301455\n",
      "Early stopping, best iteration is:\n",
      "[190]\ttraining's auc: 0.864403\ttraining's binary_logloss: 0.286203\tvalid_1's auc: 0.84179\tvalid_1's binary_logloss: 0.301504\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838847\ttraining's binary_logloss: 0.305336\tvalid_1's auc: 0.831869\tvalid_1's binary_logloss: 0.309329\n",
      "[100]\ttraining's auc: 0.851368\ttraining's binary_logloss: 0.29595\tvalid_1's auc: 0.837799\tvalid_1's binary_logloss: 0.304332\n",
      "[150]\ttraining's auc: 0.859329\ttraining's binary_logloss: 0.290077\tvalid_1's auc: 0.83984\tvalid_1's binary_logloss: 0.302641\n",
      "Early stopping, best iteration is:\n",
      "[171]\ttraining's auc: 0.862199\ttraining's binary_logloss: 0.287985\tvalid_1's auc: 0.840292\tvalid_1's binary_logloss: 0.302237\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.83989\ttraining's binary_logloss: 0.304821\tvalid_1's auc: 0.82551\tvalid_1's binary_logloss: 0.310834\n",
      "[100]\ttraining's auc: 0.852868\ttraining's binary_logloss: 0.295095\tvalid_1's auc: 0.831492\tvalid_1's binary_logloss: 0.306403\n",
      "[150]\ttraining's auc: 0.860985\ttraining's binary_logloss: 0.289121\tvalid_1's auc: 0.833654\tvalid_1's binary_logloss: 0.304907\n",
      "[200]\ttraining's auc: 0.867272\ttraining's binary_logloss: 0.284428\tvalid_1's auc: 0.834437\tvalid_1's binary_logloss: 0.304293\n",
      "Early stopping, best iteration is:\n",
      "[204]\ttraining's auc: 0.867677\ttraining's binary_logloss: 0.28406\tvalid_1's auc: 0.834561\tvalid_1's binary_logloss: 0.304172\n",
      "*********************\n",
      "roc auc estimado:  0.8395185424641647\n",
      "roc auc varianza:  0.0006217131686264802\n"
     ]
    }
   ],
   "source": [
    "test_probs, fi = cross_validation_lightgbm(train, y_train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "english-scheme",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de variables : 1131\n",
      "variables con importancia acumulada al 99% : 1063\n",
      "variables con zero importancia : 1\n"
     ]
    }
   ],
   "source": [
    "keep_columns_, zero_importance_ = get_feature_selection(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ultimate-conflict",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838634\ttraining's binary_logloss: 0.304902\tvalid_1's auc: 0.830899\tvalid_1's binary_logloss: 0.31089\n",
      "[100]\ttraining's auc: 0.851427\ttraining's binary_logloss: 0.29545\tvalid_1's auc: 0.837155\tvalid_1's binary_logloss: 0.305996\n",
      "[150]\ttraining's auc: 0.859332\ttraining's binary_logloss: 0.289615\tvalid_1's auc: 0.839297\tvalid_1's binary_logloss: 0.304372\n",
      "[200]\ttraining's auc: 0.865726\ttraining's binary_logloss: 0.28498\tvalid_1's auc: 0.840176\tvalid_1's binary_logloss: 0.303716\n",
      "[250]\ttraining's auc: 0.871426\ttraining's binary_logloss: 0.280663\tvalid_1's auc: 0.840845\tvalid_1's binary_logloss: 0.303241\n",
      "Early stopping, best iteration is:\n",
      "[276]\ttraining's auc: 0.874015\ttraining's binary_logloss: 0.27869\tvalid_1's auc: 0.841115\tvalid_1's binary_logloss: 0.303115\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838459\ttraining's binary_logloss: 0.304879\tvalid_1's auc: 0.829254\tvalid_1's binary_logloss: 0.311286\n",
      "[100]\ttraining's auc: 0.851248\ttraining's binary_logloss: 0.295376\tvalid_1's auc: 0.836476\tvalid_1's binary_logloss: 0.306023\n",
      "[150]\ttraining's auc: 0.859085\ttraining's binary_logloss: 0.28959\tvalid_1's auc: 0.838662\tvalid_1's binary_logloss: 0.304344\n",
      "[200]\ttraining's auc: 0.865351\ttraining's binary_logloss: 0.284978\tvalid_1's auc: 0.839479\tvalid_1's binary_logloss: 0.303653\n",
      "[250]\ttraining's auc: 0.870922\ttraining's binary_logloss: 0.280742\tvalid_1's auc: 0.840024\tvalid_1's binary_logloss: 0.303217\n",
      "Early stopping, best iteration is:\n",
      "[253]\ttraining's auc: 0.871229\ttraining's binary_logloss: 0.280519\tvalid_1's auc: 0.840105\tvalid_1's binary_logloss: 0.303182\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838517\ttraining's binary_logloss: 0.305277\tvalid_1's auc: 0.833366\tvalid_1's binary_logloss: 0.308868\n",
      "[100]\ttraining's auc: 0.851133\ttraining's binary_logloss: 0.295884\tvalid_1's auc: 0.839149\tvalid_1's binary_logloss: 0.303974\n",
      "[150]\ttraining's auc: 0.859109\ttraining's binary_logloss: 0.290115\tvalid_1's auc: 0.841061\tvalid_1's binary_logloss: 0.302252\n",
      "Early stopping, best iteration is:\n",
      "[178]\ttraining's auc: 0.862712\ttraining's binary_logloss: 0.287447\tvalid_1's auc: 0.841485\tvalid_1's binary_logloss: 0.301889\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.838757\ttraining's binary_logloss: 0.305326\tvalid_1's auc: 0.831431\tvalid_1's binary_logloss: 0.309425\n",
      "[100]\ttraining's auc: 0.85139\ttraining's binary_logloss: 0.295895\tvalid_1's auc: 0.837964\tvalid_1's binary_logloss: 0.304308\n",
      "[150]\ttraining's auc: 0.859155\ttraining's binary_logloss: 0.290124\tvalid_1's auc: 0.839789\tvalid_1's binary_logloss: 0.302676\n",
      "[200]\ttraining's auc: 0.86548\ttraining's binary_logloss: 0.285428\tvalid_1's auc: 0.840684\tvalid_1's binary_logloss: 0.301884\n",
      "[250]\ttraining's auc: 0.871139\ttraining's binary_logloss: 0.281239\tvalid_1's auc: 0.841225\tvalid_1's binary_logloss: 0.301453\n",
      "Early stopping, best iteration is:\n",
      "[262]\ttraining's auc: 0.87235\ttraining's binary_logloss: 0.280333\tvalid_1's auc: 0.841333\tvalid_1's binary_logloss: 0.301324\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.839899\ttraining's binary_logloss: 0.304817\tvalid_1's auc: 0.824986\tvalid_1's binary_logloss: 0.311244\n",
      "[100]\ttraining's auc: 0.852826\ttraining's binary_logloss: 0.295101\tvalid_1's auc: 0.831502\tvalid_1's binary_logloss: 0.306583\n",
      "[150]\ttraining's auc: 0.860818\ttraining's binary_logloss: 0.289158\tvalid_1's auc: 0.833136\tvalid_1's binary_logloss: 0.305172\n",
      "Early stopping, best iteration is:\n",
      "[186]\ttraining's auc: 0.865591\ttraining's binary_logloss: 0.285609\tvalid_1's auc: 0.833904\tvalid_1's binary_logloss: 0.30462\n",
      "*********************\n",
      "roc auc estimado:  0.839609653403813\n",
      "roc auc varianza:  0.0007023405473105417\n"
     ]
    }
   ],
   "source": [
    "test_probs_, fi_ = cross_validation_lightgbm(train[keep_columns_], y_train, test[keep_columns_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fallen-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de variables : 1063\n",
      "variables con importancia acumulada al 99% : 1011\n",
      "variables con zero importancia : 0\n"
     ]
    }
   ],
   "source": [
    "keep_columns__, zero_importance__ = get_feature_selection(fi_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "changed-extraction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1063), (396666, 1063))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[keep_columns_]\n",
    "test = test[keep_columns_]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "forced-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('train_1063features.pkl')\n",
    "# test.to_pickle('test_1063features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-ranking",
   "metadata": {},
   "source": [
    "## Procesar SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sacred-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_1063features.pkl')\n",
    "test = pd.read_pickle('test_1063features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "charitable-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "se_train = pd.read_csv(f'{path}/se_train.csv', index_col = 'key_value')\n",
    "se_test = pd.read_csv(f'{path}/se_test.csv', index_col = 'key_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "radio-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'sexo':'int32',\n",
    "         'est_cvl':'int32',\n",
    "         'sit_lab':'int32',\n",
    "         'cod_ocu':'int32',\n",
    "         'ctd_hijos':'int32',\n",
    "         'flg_sin_email':'int32',\n",
    "         'ctd_veh':'int32',\n",
    "         'lgr_vot':'int32',\n",
    "         'prv':'int32',\n",
    "         'dto':'int32',\n",
    "         'rgn':'int32',\n",
    "         'tip_lvledu':'int32'}\n",
    "se_train = se_train.astype(dict_)\n",
    "se_test = se_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "coupled-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(df, feature):\n",
    "    one_hot = pd.get_dummies(df[feature])\n",
    "    one_hot.columns = [feature+'_'+str(i) for i in one_hot.columns]\n",
    "    return one_hot\n",
    "one_hot_sexo_train = get_one_hot(se_train, 'sexo')\n",
    "one_hot_sexo_test = get_one_hot(se_test, 'sexo')\n",
    "one_hot_est_cvl_train = get_one_hot(se_train, 'est_cvl')\n",
    "one_hot_est_cvl_test = get_one_hot(se_test, 'est_cvl')\n",
    "one_hot_rgn_train = get_one_hot(se_train, 'rgn')\n",
    "one_hot_rgn_test = get_one_hot(se_test, 'rgn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fatal-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.join(one_hot_sexo_train).join(one_hot_est_cvl_train).join(one_hot_rgn_train).join(se_train[['edad','ctd_veh']])\n",
    "test = test.join(one_hot_sexo_test).join(one_hot_est_cvl_test).join(one_hot_rgn_test).join(se_test[['edad','ctd_veh']])\n",
    "del se_train, se_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adult-authorization",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.841931\ttraining's binary_logloss: 0.303466\tvalid_1's auc: 0.833694\tvalid_1's binary_logloss: 0.309563\n",
      "[100]\ttraining's auc: 0.855324\ttraining's binary_logloss: 0.293082\tvalid_1's auc: 0.840938\tvalid_1's binary_logloss: 0.303873\n",
      "[150]\ttraining's auc: 0.863482\ttraining's binary_logloss: 0.286824\tvalid_1's auc: 0.843654\tvalid_1's binary_logloss: 0.301808\n",
      "[200]\ttraining's auc: 0.869764\ttraining's binary_logloss: 0.282013\tvalid_1's auc: 0.844838\tvalid_1's binary_logloss: 0.300875\n",
      "[250]\ttraining's auc: 0.875494\ttraining's binary_logloss: 0.277675\tvalid_1's auc: 0.845456\tvalid_1's binary_logloss: 0.300431\n",
      "[300]\ttraining's auc: 0.880259\ttraining's binary_logloss: 0.273846\tvalid_1's auc: 0.845815\tvalid_1's binary_logloss: 0.300167\n",
      "Early stopping, best iteration is:\n",
      "[315]\ttraining's auc: 0.881755\ttraining's binary_logloss: 0.272682\tvalid_1's auc: 0.845894\tvalid_1's binary_logloss: 0.300122\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.842042\ttraining's binary_logloss: 0.303497\tvalid_1's auc: 0.833239\tvalid_1's binary_logloss: 0.309862\n",
      "[100]\ttraining's auc: 0.855479\ttraining's binary_logloss: 0.292997\tvalid_1's auc: 0.840787\tvalid_1's binary_logloss: 0.304015\n",
      "[150]\ttraining's auc: 0.863539\ttraining's binary_logloss: 0.286738\tvalid_1's auc: 0.843115\tvalid_1's binary_logloss: 0.302162\n",
      "[200]\ttraining's auc: 0.870118\ttraining's binary_logloss: 0.281798\tvalid_1's auc: 0.843936\tvalid_1's binary_logloss: 0.301425\n",
      "Early stopping, best iteration is:\n",
      "[237]\ttraining's auc: 0.874285\ttraining's binary_logloss: 0.278579\tvalid_1's auc: 0.844381\tvalid_1's binary_logloss: 0.301042\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.84212\ttraining's binary_logloss: 0.303727\tvalid_1's auc: 0.836298\tvalid_1's binary_logloss: 0.307614\n",
      "[100]\ttraining's auc: 0.85561\ttraining's binary_logloss: 0.293334\tvalid_1's auc: 0.843024\tvalid_1's binary_logloss: 0.301923\n",
      "[150]\ttraining's auc: 0.863665\ttraining's binary_logloss: 0.28712\tvalid_1's auc: 0.845088\tvalid_1's binary_logloss: 0.299989\n",
      "[200]\ttraining's auc: 0.870243\ttraining's binary_logloss: 0.282176\tvalid_1's auc: 0.845674\tvalid_1's binary_logloss: 0.29926\n",
      "Early stopping, best iteration is:\n",
      "[197]\ttraining's auc: 0.869914\ttraining's binary_logloss: 0.282447\tvalid_1's auc: 0.845734\tvalid_1's binary_logloss: 0.299238\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.841885\ttraining's binary_logloss: 0.303874\tvalid_1's auc: 0.8354\tvalid_1's binary_logloss: 0.307712\n",
      "[100]\ttraining's auc: 0.855536\ttraining's binary_logloss: 0.293546\tvalid_1's auc: 0.842351\tvalid_1's binary_logloss: 0.301903\n",
      "[150]\ttraining's auc: 0.863653\ttraining's binary_logloss: 0.287253\tvalid_1's auc: 0.844793\tvalid_1's binary_logloss: 0.299743\n",
      "[200]\ttraining's auc: 0.869961\ttraining's binary_logloss: 0.28233\tvalid_1's auc: 0.845661\tvalid_1's binary_logloss: 0.298794\n",
      "[250]\ttraining's auc: 0.875556\ttraining's binary_logloss: 0.278085\tvalid_1's auc: 0.84622\tvalid_1's binary_logloss: 0.29835\n",
      "Early stopping, best iteration is:\n",
      "[248]\ttraining's auc: 0.875345\ttraining's binary_logloss: 0.278226\tvalid_1's auc: 0.846237\tvalid_1's binary_logloss: 0.298355\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.843667\ttraining's binary_logloss: 0.303117\tvalid_1's auc: 0.829166\tvalid_1's binary_logloss: 0.30956\n",
      "[100]\ttraining's auc: 0.857353\ttraining's binary_logloss: 0.292393\tvalid_1's auc: 0.835682\tvalid_1's binary_logloss: 0.30442\n",
      "[150]\ttraining's auc: 0.86565\ttraining's binary_logloss: 0.28605\tvalid_1's auc: 0.838079\tvalid_1's binary_logloss: 0.302574\n",
      "[200]\ttraining's auc: 0.871975\ttraining's binary_logloss: 0.281096\tvalid_1's auc: 0.838816\tvalid_1's binary_logloss: 0.301932\n",
      "Early stopping, best iteration is:\n",
      "[224]\ttraining's auc: 0.874671\ttraining's binary_logloss: 0.278951\tvalid_1's auc: 0.839137\tvalid_1's binary_logloss: 0.301702\n",
      "*********************\n",
      "roc auc estimado:  0.8442881761857495\n",
      "roc auc varianza:  0.0006465481542519408\n"
     ]
    }
   ],
   "source": [
    "[test_probs,fi] = cross_validation_lightgbm(train, y_train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-expense",
   "metadata": {},
   "source": [
    "## Procesar SUNAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acting-times",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunat_train = pd.read_csv(f'{path}/sunat_train.csv')\n",
    "sunat_test = pd.read_csv(f'{path}/sunat_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "impressive-moisture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((292479, 18), (318821, 18))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### eliminar registros duplicados\n",
    "sunat_train.drop_duplicates(inplace=True)\n",
    "sunat_test.drop_duplicates(inplace=True)\n",
    "sunat_train.shape, sunat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "together-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'tipcontribuyente': 'int32',\n",
    "         'tippersona': 'int32',\n",
    "         'ciiu': 'int32',\n",
    "         'ubigeo': 'int32',\n",
    "         'condiciondomicilio': 'int32',\n",
    "         'estadocontribuyente': 'int32',\n",
    "         'codvia': 'int32',\n",
    "         'codzona': 'int32',\n",
    "         'contabilidad': 'int32',\n",
    "         'facturacion': 'int32',\n",
    "         'domiciliado': 'int32',\n",
    "         'comercioexterior': 'int32',\n",
    "         'cargorele': 'int32',\n",
    "         'codentidadtributo': 'int32',\n",
    "         'estadotributo': 'int32'}\n",
    "sunat_train = sunat_train.astype(dict_)\n",
    "sunat_test = sunat_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "optional-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunat_train['diff_fech'] = sunat_train['fecbaja'] - sunat_train['fecalta']\n",
    "sunat_test['diff_fech'] = sunat_test['fecbaja'] - sunat_test['fecalta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "alpha-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "moda=lambda x: calculate_mode(x)\n",
    "moda.__name__='mode'\n",
    "agg_sunat = {\n",
    "            'tipcontribuyente':['nunique',moda],\n",
    "            'tippersona':['nunique',moda],\n",
    "            'ciiu':['nunique', moda],\n",
    "            'ubigeo':['nunique',moda],\n",
    "           'condiciondomicilio':['nunique',moda],  ####\n",
    "#            'estadocontribuyente':['nunique',moda],\n",
    "           'codvia':['nunique',moda],\n",
    "           'codzona':['nunique',moda],\n",
    "           'contabilidad':['nunique',moda],\n",
    "           'facturacion':['nunique',moda],\n",
    "           'domiciliado':['nunique',moda],\n",
    "           'comercioexterior':['nunique',moda],\n",
    "           'cargorele':['nunique',moda],\n",
    "           'codentidadtributo':['nunique',moda], ####\n",
    "           'estadotributo':['nunique',moda], ####\n",
    "           'fecalta':['mean','max', 'nunique','min','std'],\n",
    "           'fecbaja':['mean','max', 'nunique','min'], \n",
    "           'diff_fech':['mean','max', 'nunique','min'], \n",
    "            }\n",
    "sunat_train_agg = sunat_train.groupby('key_value').agg(agg_sunat)\n",
    "sunat_train_agg.columns = [i+'_'+j for i,j in sunat_train_agg.columns]\n",
    "sunat_test_agg = sunat_test.groupby('key_value').agg(agg_sunat)\n",
    "sunat_test_agg.columns = [i+'_'+j for i,j in sunat_test_agg.columns]\n",
    "# del sunat_train, sunat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "secondary-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_estadocontribuyente_train = get_crosstab(sunat_train, 'key_value', 'estadocontribuyente')\n",
    "crosstab_estadocontribuyente_test = get_crosstab(sunat_test, 'key_value', 'estadocontribuyente')\n",
    "\n",
    "# crosstab_estadotributo_train = get_crosstab(sunat_train, 'key_value', 'estadotributo')\n",
    "# crosstab_estadotributo_test = get_crosstab(sunat_test, 'key_value', 'estadotributo')\n",
    "\n",
    "# crosstab_codentidadtributo_train = get_crosstab(sunat_train, 'key_value', 'codentidadtributo')\n",
    "# crosstab_codentidadtributo_test = get_crosstab(sunat_test, 'key_value', 'codentidadtributo')\n",
    "\n",
    "# crosstab_condiciondomicilio_train = get_crosstab(sunat_train, 'key_value', 'condiciondomicilio')\n",
    "# crosstab_condiciondomicilio_test = get_crosstab(sunat_test, 'key_value', 'condiciondomicilio')\n",
    "\n",
    "# crosstab_train = pd.concat([crosstab_estadocontribuyente_train, crosstab_estadotributo_train,\n",
    "#                             crosstab_codentidadtributo_train, crosstab_condiciondomicilio_train], axis=1)\n",
    "\n",
    "# crosstab_test = pd.concat([crosstab_estadocontribuyente_test, crosstab_estadotributo_test,\n",
    "#                             crosstab_codentidadtributo_test, crosstab_condiciondomicilio_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "failing-advantage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358487, 1136) (396666, 1135)\n",
      "(358487, 1135) (396666, 1135)\n"
     ]
    }
   ],
   "source": [
    "train = train.join(crosstab_estadocontribuyente_train).join(sunat_train_agg)\n",
    "test = test.join(crosstab_estadocontribuyente_test).join(sunat_test_agg)\n",
    "train, test = get_keep_columns(train, test)\n",
    "del sunat_train_agg, sunat_test_agg, sunat_train, sunat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "annoying-single",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.842815\ttraining's binary_logloss: 0.303136\tvalid_1's auc: 0.834673\tvalid_1's binary_logloss: 0.309239\n",
      "[100]\ttraining's auc: 0.856988\ttraining's binary_logloss: 0.292076\tvalid_1's auc: 0.843205\tvalid_1's binary_logloss: 0.302669\n",
      "[150]\ttraining's auc: 0.865499\ttraining's binary_logloss: 0.285566\tvalid_1's auc: 0.8461\tvalid_1's binary_logloss: 0.300411\n",
      "[200]\ttraining's auc: 0.872021\ttraining's binary_logloss: 0.280468\tvalid_1's auc: 0.847614\tvalid_1's binary_logloss: 0.299262\n",
      "[250]\ttraining's auc: 0.87767\ttraining's binary_logloss: 0.276098\tvalid_1's auc: 0.848302\tvalid_1's binary_logloss: 0.298766\n",
      "Early stopping, best iteration is:\n",
      "[285]\ttraining's auc: 0.881251\ttraining's binary_logloss: 0.273243\tvalid_1's auc: 0.848566\tvalid_1's binary_logloss: 0.298533\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.842587\ttraining's binary_logloss: 0.303296\tvalid_1's auc: 0.833107\tvalid_1's binary_logloss: 0.30964\n",
      "[100]\ttraining's auc: 0.857005\ttraining's binary_logloss: 0.292273\tvalid_1's auc: 0.84203\tvalid_1's binary_logloss: 0.302981\n",
      "[150]\ttraining's auc: 0.865485\ttraining's binary_logloss: 0.28564\tvalid_1's auc: 0.844776\tvalid_1's binary_logloss: 0.300825\n",
      "[200]\ttraining's auc: 0.871854\ttraining's binary_logloss: 0.280564\tvalid_1's auc: 0.845758\tvalid_1's binary_logloss: 0.299993\n",
      "[250]\ttraining's auc: 0.877683\ttraining's binary_logloss: 0.276052\tvalid_1's auc: 0.846619\tvalid_1's binary_logloss: 0.299378\n",
      "Early stopping, best iteration is:\n",
      "[283]\ttraining's auc: 0.881245\ttraining's binary_logloss: 0.273318\tvalid_1's auc: 0.846965\tvalid_1's binary_logloss: 0.299146\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.842732\ttraining's binary_logloss: 0.303608\tvalid_1's auc: 0.837411\tvalid_1's binary_logloss: 0.307235\n",
      "[100]\ttraining's auc: 0.856765\ttraining's binary_logloss: 0.292705\tvalid_1's auc: 0.844827\tvalid_1's binary_logloss: 0.300966\n",
      "[150]\ttraining's auc: 0.865231\ttraining's binary_logloss: 0.286197\tvalid_1's auc: 0.847205\tvalid_1's binary_logloss: 0.298807\n",
      "[200]\ttraining's auc: 0.871953\ttraining's binary_logloss: 0.281064\tvalid_1's auc: 0.848134\tvalid_1's binary_logloss: 0.297881\n",
      "Early stopping, best iteration is:\n",
      "[229]\ttraining's auc: 0.875067\ttraining's binary_logloss: 0.278511\tvalid_1's auc: 0.848588\tvalid_1's binary_logloss: 0.297502\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.843051\ttraining's binary_logloss: 0.303443\tvalid_1's auc: 0.835683\tvalid_1's binary_logloss: 0.307638\n",
      "[100]\ttraining's auc: 0.857258\ttraining's binary_logloss: 0.292518\tvalid_1's auc: 0.843615\tvalid_1's binary_logloss: 0.301271\n",
      "[150]\ttraining's auc: 0.865644\ttraining's binary_logloss: 0.286026\tvalid_1's auc: 0.846375\tvalid_1's binary_logloss: 0.29908\n",
      "[200]\ttraining's auc: 0.872155\ttraining's binary_logloss: 0.280945\tvalid_1's auc: 0.847284\tvalid_1's binary_logloss: 0.29812\n",
      "[250]\ttraining's auc: 0.877827\ttraining's binary_logloss: 0.27648\tvalid_1's auc: 0.847807\tvalid_1's binary_logloss: 0.297637\n",
      "[300]\ttraining's auc: 0.882966\ttraining's binary_logloss: 0.272402\tvalid_1's auc: 0.848186\tvalid_1's binary_logloss: 0.297231\n",
      "Early stopping, best iteration is:\n",
      "[311]\ttraining's auc: 0.884087\ttraining's binary_logloss: 0.271536\tvalid_1's auc: 0.848246\tvalid_1's binary_logloss: 0.297175\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.844161\ttraining's binary_logloss: 0.303061\tvalid_1's auc: 0.82968\tvalid_1's binary_logloss: 0.309531\n",
      "[100]\ttraining's auc: 0.858526\ttraining's binary_logloss: 0.291779\tvalid_1's auc: 0.837016\tvalid_1's binary_logloss: 0.303724\n",
      "[150]\ttraining's auc: 0.867164\ttraining's binary_logloss: 0.285166\tvalid_1's auc: 0.839494\tvalid_1's binary_logloss: 0.301827\n",
      "[200]\ttraining's auc: 0.873904\ttraining's binary_logloss: 0.279902\tvalid_1's auc: 0.840792\tvalid_1's binary_logloss: 0.300867\n",
      "Early stopping, best iteration is:\n",
      "[213]\ttraining's auc: 0.875536\ttraining's binary_logloss: 0.278656\tvalid_1's auc: 0.841009\tvalid_1's binary_logloss: 0.300665\n",
      "*********************\n",
      "roc auc estimado:  0.8466946478555364\n",
      "roc auc varianza:  0.0007110244161166008\n"
     ]
    }
   ],
   "source": [
    "[test_probs, fi] = cross_validation_lightgbm(train, y_train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "minute-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('train_1135features.pkl')\n",
    "# test.to_pickle('test_1135features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-parks",
   "metadata": {},
   "source": [
    "## Aggregate Unstack de cod_instit_financiera and PRODUCTO de los 12 ultimos meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "resistant-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_1135features.pkl')\n",
    "test = pd.read_pickle('test_1135features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "proprietary-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc_train = pd.read_csv(f'{path}/rcc_train.csv')\n",
    "rcc_test = pd.read_csv(f'{path}/rcc_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "alive-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### inputar datos faltantes en la base de rcc_test\n",
    "rcc_test['cod_instit_financiera'].fillna(rcc_test['cod_instit_financiera'].value_counts().index[0], inplace=True)\n",
    "rcc_test['PRODUCTO'].fillna(rcc_test['PRODUCTO'].value_counts().index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "spare-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'codmes': 'int32',\n",
    " 'key_value': 'int32',\n",
    " 'condicion': 'int32',\n",
    " 'tipo_credito': 'int32',\n",
    " 'cod_instit_financiera': 'int32',\n",
    " 'PRODUCTO': 'int32',\n",
    " 'RIESGO_DIRECTO': 'int32',\n",
    " 'COD_CLASIFICACION_DEUDOR': 'int32'}\n",
    "rcc_train = rcc_train.astype(dict_)\n",
    "rcc_test = rcc_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dominant-vaccine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de cod_instit_financiera desde 201703 con la funcion de agregacion Sum\n",
      "haciendo unstack de cod_instit_financiera desde 201703 con la funcion de agregacion Unique\n",
      "haciendo unstack de cod_instit_financiera desde 201703 con la funcion de agregacion Min\n",
      "haciendo unstack de cod_instit_financiera desde 201703 con la funcion de agregacion Max\n",
      "haciendo unstack de cod_instit_financiera desde 201703 con la funcion de agregacion Std\n",
      "haciendo unstack de cod_instit_financiera desde 201703 con la funcion de agregacion Mean\n",
      "haciendo unstack de cod_instit_financiera desde 201703 con la funcion de agregacion Median\n",
      "haciendo unstack de PRODUCTO desde 201703 con la funcion de agregacion Sum\n",
      "haciendo unstack de PRODUCTO desde 201703 con la funcion de agregacion Unique\n",
      "haciendo unstack de PRODUCTO desde 201703 con la funcion de agregacion Min\n",
      "haciendo unstack de PRODUCTO desde 201703 con la funcion de agregacion Max\n",
      "haciendo unstack de PRODUCTO desde 201703 con la funcion de agregacion Std\n",
      "haciendo unstack de PRODUCTO desde 201703 con la funcion de agregacion Mean\n",
      "haciendo unstack de PRODUCTO desde 201703 con la funcion de agregacion Median\n"
     ]
    }
   ],
   "source": [
    "rcc_train_list=[]\n",
    "for feature in ['cod_instit_financiera','PRODUCTO']:\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201703, 11, 'Sum'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201703, 11, 'Unique'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201703, 11, 'Min'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201703, 11, 'Max'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201703, 11, 'Std'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201703, 11, 'Mean'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201703, 11, 'Median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "otherwise-occurrence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358487, 1057)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcc_train = pd.concat(rcc_train_list, axis=1)\n",
    "del rcc_train_list\n",
    "rcc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "basic-ministry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de cod_instit_financiera desde 201803 con la funcion de agregacion Sum\n",
      "haciendo unstack de cod_instit_financiera desde 201803 con la funcion de agregacion Unique\n",
      "haciendo unstack de cod_instit_financiera desde 201803 con la funcion de agregacion Min\n",
      "haciendo unstack de cod_instit_financiera desde 201803 con la funcion de agregacion Max\n",
      "haciendo unstack de cod_instit_financiera desde 201803 con la funcion de agregacion Std\n",
      "haciendo unstack de cod_instit_financiera desde 201803 con la funcion de agregacion Mean\n",
      "haciendo unstack de cod_instit_financiera desde 201803 con la funcion de agregacion Median\n",
      "haciendo unstack de PRODUCTO desde 201803 con la funcion de agregacion Sum\n",
      "haciendo unstack de PRODUCTO desde 201803 con la funcion de agregacion Unique\n",
      "haciendo unstack de PRODUCTO desde 201803 con la funcion de agregacion Min\n",
      "haciendo unstack de PRODUCTO desde 201803 con la funcion de agregacion Max\n",
      "haciendo unstack de PRODUCTO desde 201803 con la funcion de agregacion Std\n",
      "haciendo unstack de PRODUCTO desde 201803 con la funcion de agregacion Mean\n",
      "haciendo unstack de PRODUCTO desde 201803 con la funcion de agregacion Median\n"
     ]
    }
   ],
   "source": [
    "rcc_test_list=[]\n",
    "for feature in ['cod_instit_financiera','PRODUCTO']:\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201803, 11, 'Sum'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201803, 11, 'Unique'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201803, 11, 'Min'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201803, 11, 'Max'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201803, 11, 'Std'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201803, 11, 'Mean'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201803, 11, 'Median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "opening-lindsay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396666, 959)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcc_test = pd.concat(rcc_test_list, axis=1)\n",
    "del rcc_test_list\n",
    "rcc_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "promotional-pillow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358487, 1057) (396666, 959)\n",
      "(358487, 959) (396666, 959)\n"
     ]
    }
   ],
   "source": [
    "rcc_train , rcc_test = get_keep_columns(rcc_train, rcc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "utility-history",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 2094), (396666, 2094))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.join(rcc_train)\n",
    "test = test.join(rcc_test)\n",
    "del rcc_train, rcc_test\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "together-market",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.849474\ttraining's binary_logloss: 0.298747\tvalid_1's auc: 0.841043\tvalid_1's binary_logloss: 0.305141\n",
      "[100]\ttraining's auc: 0.862844\ttraining's binary_logloss: 0.287447\tvalid_1's auc: 0.848488\tvalid_1's binary_logloss: 0.298783\n",
      "[150]\ttraining's auc: 0.871171\ttraining's binary_logloss: 0.280715\tvalid_1's auc: 0.851142\tvalid_1's binary_logloss: 0.296634\n",
      "[200]\ttraining's auc: 0.877743\ttraining's binary_logloss: 0.27542\tvalid_1's auc: 0.852547\tvalid_1's binary_logloss: 0.29554\n",
      "[250]\ttraining's auc: 0.883317\ttraining's binary_logloss: 0.270831\tvalid_1's auc: 0.852967\tvalid_1's binary_logloss: 0.295191\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttraining's auc: 0.88478\ttraining's binary_logloss: 0.269647\tvalid_1's auc: 0.853141\tvalid_1's binary_logloss: 0.295091\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.849287\ttraining's binary_logloss: 0.298722\tvalid_1's auc: 0.84057\tvalid_1's binary_logloss: 0.305054\n",
      "[100]\ttraining's auc: 0.862958\ttraining's binary_logloss: 0.287283\tvalid_1's auc: 0.848269\tvalid_1's binary_logloss: 0.298615\n",
      "[150]\ttraining's auc: 0.871258\ttraining's binary_logloss: 0.280528\tvalid_1's auc: 0.850739\tvalid_1's binary_logloss: 0.296519\n",
      "[200]\ttraining's auc: 0.877743\ttraining's binary_logloss: 0.275247\tvalid_1's auc: 0.851843\tvalid_1's binary_logloss: 0.295586\n",
      "[250]\ttraining's auc: 0.883348\ttraining's binary_logloss: 0.270656\tvalid_1's auc: 0.85246\tvalid_1's binary_logloss: 0.295023\n",
      "Early stopping, best iteration is:\n",
      "[268]\ttraining's auc: 0.885264\ttraining's binary_logloss: 0.269133\tvalid_1's auc: 0.852767\tvalid_1's binary_logloss: 0.29481\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.84937\ttraining's binary_logloss: 0.299207\tvalid_1's auc: 0.8435\tvalid_1's binary_logloss: 0.302799\n",
      "[100]\ttraining's auc: 0.863\ttraining's binary_logloss: 0.287695\tvalid_1's auc: 0.85042\tvalid_1's binary_logloss: 0.296275\n",
      "[150]\ttraining's auc: 0.871246\ttraining's binary_logloss: 0.281025\tvalid_1's auc: 0.852617\tvalid_1's binary_logloss: 0.294348\n",
      "[200]\ttraining's auc: 0.877691\ttraining's binary_logloss: 0.275761\tvalid_1's auc: 0.853503\tvalid_1's binary_logloss: 0.293484\n",
      "[250]\ttraining's auc: 0.883091\ttraining's binary_logloss: 0.271204\tvalid_1's auc: 0.854029\tvalid_1's binary_logloss: 0.292998\n",
      "Early stopping, best iteration is:\n",
      "[256]\ttraining's auc: 0.883675\ttraining's binary_logloss: 0.270725\tvalid_1's auc: 0.854072\tvalid_1's binary_logloss: 0.292964\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.849059\ttraining's binary_logloss: 0.299288\tvalid_1's auc: 0.842256\tvalid_1's binary_logloss: 0.303467\n",
      "[100]\ttraining's auc: 0.862885\ttraining's binary_logloss: 0.287854\tvalid_1's auc: 0.849819\tvalid_1's binary_logloss: 0.296813\n",
      "[150]\ttraining's auc: 0.871213\ttraining's binary_logloss: 0.281114\tvalid_1's auc: 0.851921\tvalid_1's binary_logloss: 0.294792\n",
      "[200]\ttraining's auc: 0.877693\ttraining's binary_logloss: 0.275904\tvalid_1's auc: 0.852785\tvalid_1's binary_logloss: 0.293911\n",
      "Early stopping, best iteration is:\n",
      "[219]\ttraining's auc: 0.880067\ttraining's binary_logloss: 0.274084\tvalid_1's auc: 0.853047\tvalid_1's binary_logloss: 0.29367\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.851242\ttraining's binary_logloss: 0.298298\tvalid_1's auc: 0.835152\tvalid_1's binary_logloss: 0.306056\n",
      "[100]\ttraining's auc: 0.865081\ttraining's binary_logloss: 0.286646\tvalid_1's auc: 0.841456\tvalid_1's binary_logloss: 0.300442\n",
      "[150]\ttraining's auc: 0.873265\ttraining's binary_logloss: 0.279764\tvalid_1's auc: 0.843496\tvalid_1's binary_logloss: 0.298611\n",
      "[200]\ttraining's auc: 0.879748\ttraining's binary_logloss: 0.274498\tvalid_1's auc: 0.844466\tvalid_1's binary_logloss: 0.297789\n",
      "Early stopping, best iteration is:\n",
      "[238]\ttraining's auc: 0.884151\ttraining's binary_logloss: 0.270888\tvalid_1's auc: 0.844879\tvalid_1's binary_logloss: 0.297418\n",
      "*********************\n",
      "roc auc estimado:  0.8516168658960651\n",
      "roc auc varianza:  0.0008218480457957318\n",
      "total de variables : 2094\n",
      "variables con importancia acumulada al 99% : 1338\n",
      "variables con zero importancia : 614\n",
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.848948\ttraining's binary_logloss: 0.298813\tvalid_1's auc: 0.840172\tvalid_1's binary_logloss: 0.305429\n",
      "[100]\ttraining's auc: 0.862551\ttraining's binary_logloss: 0.287456\tvalid_1's auc: 0.848313\tvalid_1's binary_logloss: 0.298915\n",
      "[150]\ttraining's auc: 0.870939\ttraining's binary_logloss: 0.280661\tvalid_1's auc: 0.851059\tvalid_1's binary_logloss: 0.296598\n",
      "[200]\ttraining's auc: 0.877389\ttraining's binary_logloss: 0.275494\tvalid_1's auc: 0.852339\tvalid_1's binary_logloss: 0.295596\n",
      "[250]\ttraining's auc: 0.883251\ttraining's binary_logloss: 0.270851\tvalid_1's auc: 0.852771\tvalid_1's binary_logloss: 0.295117\n",
      "Early stopping, best iteration is:\n",
      "[260]\ttraining's auc: 0.88431\ttraining's binary_logloss: 0.269988\tvalid_1's auc: 0.852886\tvalid_1's binary_logloss: 0.295055\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.849333\ttraining's binary_logloss: 0.298749\tvalid_1's auc: 0.840344\tvalid_1's binary_logloss: 0.305246\n",
      "[100]\ttraining's auc: 0.863174\ttraining's binary_logloss: 0.287294\tvalid_1's auc: 0.848187\tvalid_1's binary_logloss: 0.298721\n",
      "[150]\ttraining's auc: 0.871255\ttraining's binary_logloss: 0.280586\tvalid_1's auc: 0.850813\tvalid_1's binary_logloss: 0.29655\n",
      "[200]\ttraining's auc: 0.877744\ttraining's binary_logloss: 0.275283\tvalid_1's auc: 0.851935\tvalid_1's binary_logloss: 0.295612\n",
      "[250]\ttraining's auc: 0.883284\ttraining's binary_logloss: 0.2707\tvalid_1's auc: 0.852503\tvalid_1's binary_logloss: 0.295196\n",
      "Early stopping, best iteration is:\n",
      "[253]\ttraining's auc: 0.883666\ttraining's binary_logloss: 0.270419\tvalid_1's auc: 0.852524\tvalid_1's binary_logloss: 0.295172\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.849213\ttraining's binary_logloss: 0.299335\tvalid_1's auc: 0.843629\tvalid_1's binary_logloss: 0.302694\n",
      "[100]\ttraining's auc: 0.862908\ttraining's binary_logloss: 0.2878\tvalid_1's auc: 0.850907\tvalid_1's binary_logloss: 0.296005\n",
      "[150]\ttraining's auc: 0.871055\ttraining's binary_logloss: 0.281095\tvalid_1's auc: 0.852786\tvalid_1's binary_logloss: 0.294063\n",
      "[200]\ttraining's auc: 0.87772\ttraining's binary_logloss: 0.275808\tvalid_1's auc: 0.853632\tvalid_1's binary_logloss: 0.293204\n",
      "[250]\ttraining's auc: 0.883299\ttraining's binary_logloss: 0.271253\tvalid_1's auc: 0.854018\tvalid_1's binary_logloss: 0.292761\n",
      "Early stopping, best iteration is:\n",
      "[244]\ttraining's auc: 0.882589\ttraining's binary_logloss: 0.271799\tvalid_1's auc: 0.854039\tvalid_1's binary_logloss: 0.292768\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.849034\ttraining's binary_logloss: 0.299285\tvalid_1's auc: 0.842558\tvalid_1's binary_logloss: 0.303438\n",
      "[100]\ttraining's auc: 0.862927\ttraining's binary_logloss: 0.287881\tvalid_1's auc: 0.849988\tvalid_1's binary_logloss: 0.296689\n",
      "[150]\ttraining's auc: 0.871023\ttraining's binary_logloss: 0.281198\tvalid_1's auc: 0.85218\tvalid_1's binary_logloss: 0.294515\n",
      "[200]\ttraining's auc: 0.877616\ttraining's binary_logloss: 0.275898\tvalid_1's auc: 0.853153\tvalid_1's binary_logloss: 0.293552\n",
      "Early stopping, best iteration is:\n",
      "[207]\ttraining's auc: 0.878461\ttraining's binary_logloss: 0.275218\tvalid_1's auc: 0.853253\tvalid_1's binary_logloss: 0.293473\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.850747\ttraining's binary_logloss: 0.298645\tvalid_1's auc: 0.834333\tvalid_1's binary_logloss: 0.306411\n",
      "[100]\ttraining's auc: 0.864705\ttraining's binary_logloss: 0.286771\tvalid_1's auc: 0.841341\tvalid_1's binary_logloss: 0.300388\n",
      "[150]\ttraining's auc: 0.873107\ttraining's binary_logloss: 0.279919\tvalid_1's auc: 0.843219\tvalid_1's binary_logloss: 0.298699\n",
      "[200]\ttraining's auc: 0.879609\ttraining's binary_logloss: 0.274608\tvalid_1's auc: 0.844082\tvalid_1's binary_logloss: 0.297933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[196]\ttraining's auc: 0.879122\ttraining's binary_logloss: 0.274986\tvalid_1's auc: 0.844117\tvalid_1's binary_logloss: 0.29794\n",
      "*********************\n",
      "roc auc estimado:  0.8513995738147447\n",
      "roc auc varianza:  0.0008891522476088254\n",
      "total de variables : 1338\n",
      "variables con importancia acumulada al 99% : 1242\n",
      "variables con zero importancia : 0\n"
     ]
    }
   ],
   "source": [
    "# eliminar variables con zero_importance\n",
    "while True:\n",
    "    test_probs, fi = cross_validation_lightgbm(train, y_train, test)\n",
    "    keep_columns, zero_importance = get_feature_selection(fi)\n",
    "    train = train[keep_columns]\n",
    "    test = test[keep_columns]\n",
    "    if len(zero_importance)==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "worldwide-syndicate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.849467\ttraining's binary_logloss: 0.298653\tvalid_1's auc: 0.841334\tvalid_1's binary_logloss: 0.305017\n",
      "[100]\ttraining's auc: 0.862752\ttraining's binary_logloss: 0.287409\tvalid_1's auc: 0.849029\tvalid_1's binary_logloss: 0.298518\n",
      "[150]\ttraining's auc: 0.870825\ttraining's binary_logloss: 0.280815\tvalid_1's auc: 0.851775\tvalid_1's binary_logloss: 0.296306\n",
      "[200]\ttraining's auc: 0.877364\ttraining's binary_logloss: 0.275638\tvalid_1's auc: 0.852525\tvalid_1's binary_logloss: 0.295595\n",
      "[250]\ttraining's auc: 0.882944\ttraining's binary_logloss: 0.271075\tvalid_1's auc: 0.853165\tvalid_1's binary_logloss: 0.295049\n",
      "[300]\ttraining's auc: 0.887974\ttraining's binary_logloss: 0.266895\tvalid_1's auc: 0.853475\tvalid_1's binary_logloss: 0.294771\n",
      "Early stopping, best iteration is:\n",
      "[326]\ttraining's auc: 0.89053\ttraining's binary_logloss: 0.264784\tvalid_1's auc: 0.853623\tvalid_1's binary_logloss: 0.294682\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.849218\ttraining's binary_logloss: 0.298665\tvalid_1's auc: 0.840849\tvalid_1's binary_logloss: 0.304953\n",
      "[100]\ttraining's auc: 0.862825\ttraining's binary_logloss: 0.287358\tvalid_1's auc: 0.848693\tvalid_1's binary_logloss: 0.298508\n",
      "[150]\ttraining's auc: 0.871164\ttraining's binary_logloss: 0.280624\tvalid_1's auc: 0.851421\tvalid_1's binary_logloss: 0.296351\n",
      "[200]\ttraining's auc: 0.877612\ttraining's binary_logloss: 0.275366\tvalid_1's auc: 0.852361\tvalid_1's binary_logloss: 0.295483\n",
      "[250]\ttraining's auc: 0.883315\ttraining's binary_logloss: 0.270737\tvalid_1's auc: 0.853138\tvalid_1's binary_logloss: 0.294899\n",
      "Early stopping, best iteration is:\n",
      "[254]\ttraining's auc: 0.883802\ttraining's binary_logloss: 0.270376\tvalid_1's auc: 0.85316\tvalid_1's binary_logloss: 0.294869\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.849258\ttraining's binary_logloss: 0.299149\tvalid_1's auc: 0.843229\tvalid_1's binary_logloss: 0.302911\n",
      "[100]\ttraining's auc: 0.863031\ttraining's binary_logloss: 0.287744\tvalid_1's auc: 0.850181\tvalid_1's binary_logloss: 0.29656\n",
      "[150]\ttraining's auc: 0.871306\ttraining's binary_logloss: 0.281036\tvalid_1's auc: 0.852202\tvalid_1's binary_logloss: 0.294558\n",
      "[200]\ttraining's auc: 0.877668\ttraining's binary_logloss: 0.27583\tvalid_1's auc: 0.853065\tvalid_1's binary_logloss: 0.2937\n",
      "[250]\ttraining's auc: 0.883398\ttraining's binary_logloss: 0.271197\tvalid_1's auc: 0.853362\tvalid_1's binary_logloss: 0.293345\n",
      "Early stopping, best iteration is:\n",
      "[243]\ttraining's auc: 0.882597\ttraining's binary_logloss: 0.271826\tvalid_1's auc: 0.853406\tvalid_1's binary_logloss: 0.293368\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.849109\ttraining's binary_logloss: 0.299273\tvalid_1's auc: 0.84222\tvalid_1's binary_logloss: 0.303613\n",
      "[100]\ttraining's auc: 0.862847\ttraining's binary_logloss: 0.287814\tvalid_1's auc: 0.849842\tvalid_1's binary_logloss: 0.296792\n",
      "[150]\ttraining's auc: 0.871088\ttraining's binary_logloss: 0.281172\tvalid_1's auc: 0.851944\tvalid_1's binary_logloss: 0.294788\n",
      "[200]\ttraining's auc: 0.877705\ttraining's binary_logloss: 0.275844\tvalid_1's auc: 0.852906\tvalid_1's binary_logloss: 0.293844\n",
      "[250]\ttraining's auc: 0.883353\ttraining's binary_logloss: 0.27119\tvalid_1's auc: 0.853279\tvalid_1's binary_logloss: 0.29342\n",
      "Early stopping, best iteration is:\n",
      "[255]\ttraining's auc: 0.883925\ttraining's binary_logloss: 0.270738\tvalid_1's auc: 0.853328\tvalid_1's binary_logloss: 0.293392\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.850893\ttraining's binary_logloss: 0.298415\tvalid_1's auc: 0.834817\tvalid_1's binary_logloss: 0.306138\n",
      "[100]\ttraining's auc: 0.864699\ttraining's binary_logloss: 0.286837\tvalid_1's auc: 0.841457\tvalid_1's binary_logloss: 0.300531\n",
      "[150]\ttraining's auc: 0.872902\ttraining's binary_logloss: 0.280025\tvalid_1's auc: 0.843452\tvalid_1's binary_logloss: 0.298768\n",
      "[200]\ttraining's auc: 0.879275\ttraining's binary_logloss: 0.274777\tvalid_1's auc: 0.844078\tvalid_1's binary_logloss: 0.298109\n",
      "Early stopping, best iteration is:\n",
      "[191]\ttraining's auc: 0.878092\ttraining's binary_logloss: 0.275676\tvalid_1's auc: 0.844167\tvalid_1's binary_logloss: 0.298114\n",
      "*********************\n",
      "roc auc estimado:  0.8515723446808912\n",
      "roc auc varianza:  0.0008964163364301804\n"
     ]
    }
   ],
   "source": [
    "test_probs_, fi_ = cross_validation_lightgbm(train[keep_columns], y_train, test[keep_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "unusual-looking",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RIESGO_DIRECTO_1_saldoUnique_ult1meses               0.009520\n",
       "edad                                                 0.009003\n",
       "PRODUCTO_1_saldoStd_ult12meses                       0.008067\n",
       "ubigeo_mode                                          0.007729\n",
       "PRODUCTO_1_saldoMax_ult12meses                       0.007622\n",
       "PRODUCTO_3_saldoUnique_ult12meses                    0.006343\n",
       "RIESGO_DIRECTO_1_saldoStd_ult1meses                  0.006243\n",
       "PRODUCTO_1_saldoMean_ult12meses                      0.005783\n",
       "RIESGO_DIRECTO_-1_saldoMedian_ult1meses              0.005665\n",
       "sexo_0                                               0.005398\n",
       "RIESGO_DIRECTO_-1_saldoMedian_ult2meses              0.005121\n",
       "cod_instit_financiera_10_saldoStd_ult12meses         0.004941\n",
       "PRODUCTO_8_saldoSum_ult12meses                       0.004894\n",
       "PRODUCTO_0_saldoMin_ult12meses                       0.004785\n",
       "RIESGO_DIRECTO_1_saldoMedian_ult2meses               0.004692\n",
       "RIESGO_DIRECTO_1_saldoMin_ult1meses                  0.004596\n",
       "COD_CLASIFICACION_DEUDOR_0_saldoUnique_ult12meses    0.004583\n",
       "PRODUCTO_6_saldoMean_ult12meses                      0.004564\n",
       "RIESGO_DIRECTO_1_saldoMedian_ult1meses               0.004547\n",
       "PRODUCTO_3_saldoMedian_ult12meses                    0.004499\n",
       "estadocontribuyente_0                                0.004489\n",
       "PRODUCTO_8_saldoUnique_ult12meses                    0.004438\n",
       "PRODUCTO_3_saldoMean_ult12meses                      0.004436\n",
       "RIESGO_DIRECTO_1_saldoUnique_ult12meses              0.004304\n",
       "RIESGO_DIRECTO_1_saldoSum_ult1meses                  0.004295\n",
       "ciiu_mode                                            0.004022\n",
       "PRODUCTO_3_saldoMin_ult12meses                       0.004005\n",
       "fecalta_min                                          0.003923\n",
       "PRODUCTO_6_saldoSum_ult12meses                       0.003883\n",
       "cod_instit_financiera_32_saldoStd_ult12meses         0.003780\n",
       "cod_instit_financiera_max_ult1mes                    0.003758\n",
       "COD_CLASIFICACION_DEUDOR_5_saldoMax_ult1meses        0.003698\n",
       "PRODUCTO_12_saldoUnique_ult12meses                   0.003671\n",
       "PRODUCTO_0_saldoStd_ult12meses                       0.003654\n",
       "PRODUCTO_8_saldoMin_ult12meses                       0.003619\n",
       "PRODUCTO_1_saldoMedian_ult12meses                    0.003596\n",
       "PRODUCTO_11_saldoStd_ult12meses                      0.003523\n",
       "PRODUCTO_4_saldoStd_ult12meses                       0.003520\n",
       "PRODUCTO_11_saldoMin_ult12meses                      0.003520\n",
       "RIESGO_DIRECTO_1_saldoMin_ult2meses                  0.003498\n",
       "RIESGO_DIRECTO_-1_saldoSum_ult1meses                 0.003361\n",
       "PRODUCTO_8_saldoMedian_ult12meses                    0.003360\n",
       "RIESGO_DIRECTO_1_saldoMedian_ult12meses              0.003314\n",
       "RIESGO_DIRECTO_1_saldoMedian_ult3meses               0.003168\n",
       "cod_instit_financiera_34_saldoUnique_ult12meses      0.003142\n",
       "PRODUCTO_2_saldoUnique_ult12meses                    0.003119\n",
       "tipo_credito_12_saldoMin_ult1meses                   0.003105\n",
       "PRODUCTO_6_saldoMin_ult12meses                       0.003072\n",
       "RIESGO_DIRECTO_1_saldoStd_ult2meses                  0.003060\n",
       "condicion_0_saldoUnique_ult12meses                   0.003053\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi_.sort_values(ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "governmental-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs_.name='target'\n",
    "test_probs_.to_csv('../results/test_1242features_0.85157.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "amended-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[keep_columns].to_pickle('train_1242features.pkl')\n",
    "# test[keep_columns].to_pickle('test_1242features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-circle",
   "metadata": {},
   "source": [
    "## Aggregate Unstack de cod_instit_financiera and PRODUCTO del ultimo mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cooperative-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('train_1242features.pkl')\n",
    "test =pd.read_pickle('test_1242features.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expanded-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc_train = pd.read_csv(f'{path}/rcc_train.csv')\n",
    "rcc_test = pd.read_csv(f'{path}/rcc_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fancy-store",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### inputar datos faltantes en la base de rcc_test\n",
    "rcc_test['cod_instit_financiera'].fillna(rcc_test['cod_instit_financiera'].value_counts().index[0], inplace=True)\n",
    "rcc_test['PRODUCTO'].fillna(rcc_test['PRODUCTO'].value_counts().index[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "compliant-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'codmes': 'int32',\n",
    " 'key_value': 'int32',\n",
    " 'condicion': 'int32',\n",
    " 'tipo_credito': 'int32',\n",
    " 'cod_instit_financiera': 'int32',\n",
    " 'PRODUCTO': 'int32',\n",
    " 'RIESGO_DIRECTO': 'int32',\n",
    " 'COD_CLASIFICACION_DEUDOR': 'int32'}\n",
    "rcc_train = rcc_train.astype(dict_)\n",
    "rcc_test = rcc_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "spiritual-compatibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de cod_instit_financiera desde 201802 con la funcion de agregacion Sum\n",
      "haciendo unstack de cod_instit_financiera desde 201802 con la funcion de agregacion Unique\n",
      "haciendo unstack de cod_instit_financiera desde 201802 con la funcion de agregacion Min\n",
      "haciendo unstack de cod_instit_financiera desde 201802 con la funcion de agregacion Max\n",
      "haciendo unstack de cod_instit_financiera desde 201802 con la funcion de agregacion Std\n",
      "haciendo unstack de cod_instit_financiera desde 201802 con la funcion de agregacion Mean\n",
      "haciendo unstack de cod_instit_financiera desde 201802 con la funcion de agregacion Median\n",
      "haciendo unstack de PRODUCTO desde 201802 con la funcion de agregacion Sum\n",
      "haciendo unstack de PRODUCTO desde 201802 con la funcion de agregacion Unique\n",
      "haciendo unstack de PRODUCTO desde 201802 con la funcion de agregacion Min\n",
      "haciendo unstack de PRODUCTO desde 201802 con la funcion de agregacion Max\n",
      "haciendo unstack de PRODUCTO desde 201802 con la funcion de agregacion Std\n",
      "haciendo unstack de PRODUCTO desde 201802 con la funcion de agregacion Mean\n",
      "haciendo unstack de PRODUCTO desde 201802 con la funcion de agregacion Median\n"
     ]
    }
   ],
   "source": [
    "rcc_train_list=[]\n",
    "for feature in ['cod_instit_financiera','PRODUCTO']:\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201802, 0, 'Sum'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201802, 0, 'Unique'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201802, 0, 'Min'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201802, 0, 'Max'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201802, 0, 'Std'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201802, 0, 'Mean'))\n",
    "    rcc_train_list.append(get_unstack(rcc_train, feature, 201802, 0, 'Median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "communist-spiritual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358487, 987)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcc_train = pd.concat(rcc_train_list, axis=1)\n",
    "del rcc_train_list\n",
    "rcc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "chinese-messenger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de cod_instit_financiera desde 201902 con la funcion de agregacion Sum\n",
      "haciendo unstack de cod_instit_financiera desde 201902 con la funcion de agregacion Unique\n",
      "haciendo unstack de cod_instit_financiera desde 201902 con la funcion de agregacion Min\n",
      "haciendo unstack de cod_instit_financiera desde 201902 con la funcion de agregacion Max\n",
      "haciendo unstack de cod_instit_financiera desde 201902 con la funcion de agregacion Std\n",
      "haciendo unstack de cod_instit_financiera desde 201902 con la funcion de agregacion Mean\n",
      "haciendo unstack de cod_instit_financiera desde 201902 con la funcion de agregacion Median\n",
      "haciendo unstack de PRODUCTO desde 201902 con la funcion de agregacion Sum\n",
      "haciendo unstack de PRODUCTO desde 201902 con la funcion de agregacion Unique\n",
      "haciendo unstack de PRODUCTO desde 201902 con la funcion de agregacion Min\n",
      "haciendo unstack de PRODUCTO desde 201902 con la funcion de agregacion Max\n",
      "haciendo unstack de PRODUCTO desde 201902 con la funcion de agregacion Std\n",
      "haciendo unstack de PRODUCTO desde 201902 con la funcion de agregacion Mean\n",
      "haciendo unstack de PRODUCTO desde 201902 con la funcion de agregacion Median\n"
     ]
    }
   ],
   "source": [
    "rcc_test_list=[]\n",
    "for feature in ['cod_instit_financiera','PRODUCTO']:\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201902, 0, 'Sum'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201902, 0, 'Unique'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201902, 0, 'Min'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201902, 0, 'Max'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201902, 0, 'Std'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201902, 0, 'Mean'))\n",
    "    rcc_test_list.append(get_unstack(rcc_test, feature, 201902, 0, 'Median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "stone-dividend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396666, 896)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcc_test = pd.concat(rcc_test_list, axis=1)\n",
    "del rcc_test_list\n",
    "rcc_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "material-medline",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358487, 987) (396666, 896)\n",
      "(358487, 889) (396666, 889)\n"
     ]
    }
   ],
   "source": [
    "rcc_train , rcc_test = get_keep_columns(rcc_train, rcc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ultimate-gabriel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 2131), (396666, 2131))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.join(rcc_train)\n",
    "test = test.join(rcc_test)\n",
    "del rcc_train, rcc_test\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "nuclear-deputy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852444\ttraining's binary_logloss: 0.296276\tvalid_1's auc: 0.843457\tvalid_1's binary_logloss: 0.302985\n",
      "[100]\ttraining's auc: 0.865245\ttraining's binary_logloss: 0.285196\tvalid_1's auc: 0.850852\tvalid_1's binary_logloss: 0.296781\n",
      "[150]\ttraining's auc: 0.873169\ttraining's binary_logloss: 0.278708\tvalid_1's auc: 0.852872\tvalid_1's binary_logloss: 0.295104\n",
      "[200]\ttraining's auc: 0.879514\ttraining's binary_logloss: 0.273502\tvalid_1's auc: 0.85393\tvalid_1's binary_logloss: 0.2943\n",
      "[250]\ttraining's auc: 0.885342\ttraining's binary_logloss: 0.268774\tvalid_1's auc: 0.854694\tvalid_1's binary_logloss: 0.293784\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttraining's auc: 0.887504\ttraining's binary_logloss: 0.266968\tvalid_1's auc: 0.854936\tvalid_1's binary_logloss: 0.293641\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852376\ttraining's binary_logloss: 0.296363\tvalid_1's auc: 0.843686\tvalid_1's binary_logloss: 0.30299\n",
      "[100]\ttraining's auc: 0.865225\ttraining's binary_logloss: 0.285158\tvalid_1's auc: 0.850513\tvalid_1's binary_logloss: 0.296838\n",
      "[150]\ttraining's auc: 0.873153\ttraining's binary_logloss: 0.278604\tvalid_1's auc: 0.852825\tvalid_1's binary_logloss: 0.294872\n",
      "[200]\ttraining's auc: 0.879496\ttraining's binary_logloss: 0.273445\tvalid_1's auc: 0.853677\tvalid_1's binary_logloss: 0.294211\n",
      "[250]\ttraining's auc: 0.885116\ttraining's binary_logloss: 0.2688\tvalid_1's auc: 0.854241\tvalid_1's binary_logloss: 0.293763\n",
      "Early stopping, best iteration is:\n",
      "[254]\ttraining's auc: 0.88552\ttraining's binary_logloss: 0.268472\tvalid_1's auc: 0.854302\tvalid_1's binary_logloss: 0.29372\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852414\ttraining's binary_logloss: 0.296753\tvalid_1's auc: 0.846452\tvalid_1's binary_logloss: 0.300292\n",
      "[100]\ttraining's auc: 0.865597\ttraining's binary_logloss: 0.285435\tvalid_1's auc: 0.853486\tvalid_1's binary_logloss: 0.294043\n",
      "[150]\ttraining's auc: 0.873369\ttraining's binary_logloss: 0.278953\tvalid_1's auc: 0.855284\tvalid_1's binary_logloss: 0.292236\n",
      "[200]\ttraining's auc: 0.879786\ttraining's binary_logloss: 0.273728\tvalid_1's auc: 0.855843\tvalid_1's binary_logloss: 0.2917\n",
      "Early stopping, best iteration is:\n",
      "[222]\ttraining's auc: 0.882348\ttraining's binary_logloss: 0.271601\tvalid_1's auc: 0.856042\tvalid_1's binary_logloss: 0.291465\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852207\ttraining's binary_logloss: 0.296661\tvalid_1's auc: 0.84484\tvalid_1's binary_logloss: 0.30132\n",
      "[100]\ttraining's auc: 0.865309\ttraining's binary_logloss: 0.285506\tvalid_1's auc: 0.851538\tvalid_1's binary_logloss: 0.295016\n",
      "[150]\ttraining's auc: 0.873396\ttraining's binary_logloss: 0.278838\tvalid_1's auc: 0.853491\tvalid_1's binary_logloss: 0.29323\n",
      "[200]\ttraining's auc: 0.879746\ttraining's binary_logloss: 0.273655\tvalid_1's auc: 0.854239\tvalid_1's binary_logloss: 0.292535\n",
      "[250]\ttraining's auc: 0.885445\ttraining's binary_logloss: 0.268936\tvalid_1's auc: 0.854768\tvalid_1's binary_logloss: 0.292046\n",
      "Early stopping, best iteration is:\n",
      "[279]\ttraining's auc: 0.888639\ttraining's binary_logloss: 0.266372\tvalid_1's auc: 0.854999\tvalid_1's binary_logloss: 0.29184\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.854264\ttraining's binary_logloss: 0.295823\tvalid_1's auc: 0.836686\tvalid_1's binary_logloss: 0.30429\n",
      "[100]\ttraining's auc: 0.867176\ttraining's binary_logloss: 0.284367\tvalid_1's auc: 0.842876\tvalid_1's binary_logloss: 0.298973\n",
      "[150]\ttraining's auc: 0.875334\ttraining's binary_logloss: 0.277685\tvalid_1's auc: 0.84483\tvalid_1's binary_logloss: 0.297498\n",
      "[200]\ttraining's auc: 0.88163\ttraining's binary_logloss: 0.272425\tvalid_1's auc: 0.845526\tvalid_1's binary_logloss: 0.296877\n",
      "Early stopping, best iteration is:\n",
      "[223]\ttraining's auc: 0.884485\ttraining's binary_logloss: 0.27018\tvalid_1's auc: 0.845697\tvalid_1's binary_logloss: 0.296711\n",
      "*********************\n",
      "roc auc estimado:  0.8532420247322343\n",
      "roc auc varianza:  0.0009214141644526705\n",
      "total de variables : 2131\n",
      "variables con importancia acumulada al 99% : 1415\n",
      "variables con zero importancia : 577\n",
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852396\ttraining's binary_logloss: 0.296255\tvalid_1's auc: 0.8436\tvalid_1's binary_logloss: 0.302924\n",
      "[100]\ttraining's auc: 0.865277\ttraining's binary_logloss: 0.285066\tvalid_1's auc: 0.851088\tvalid_1's binary_logloss: 0.296708\n",
      "[150]\ttraining's auc: 0.873221\ttraining's binary_logloss: 0.278504\tvalid_1's auc: 0.853091\tvalid_1's binary_logloss: 0.295015\n",
      "[200]\ttraining's auc: 0.879538\ttraining's binary_logloss: 0.273374\tvalid_1's auc: 0.854186\tvalid_1's binary_logloss: 0.29415\n",
      "Early stopping, best iteration is:\n",
      "[219]\ttraining's auc: 0.881817\ttraining's binary_logloss: 0.271573\tvalid_1's auc: 0.854389\tvalid_1's binary_logloss: 0.294006\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852507\ttraining's binary_logloss: 0.296183\tvalid_1's auc: 0.843991\tvalid_1's binary_logloss: 0.302689\n",
      "[100]\ttraining's auc: 0.865513\ttraining's binary_logloss: 0.284988\tvalid_1's auc: 0.85089\tvalid_1's binary_logloss: 0.296508\n",
      "[150]\ttraining's auc: 0.873605\ttraining's binary_logloss: 0.278296\tvalid_1's auc: 0.853378\tvalid_1's binary_logloss: 0.294479\n",
      "[200]\ttraining's auc: 0.879838\ttraining's binary_logloss: 0.273166\tvalid_1's auc: 0.854278\tvalid_1's binary_logloss: 0.293714\n",
      "Early stopping, best iteration is:\n",
      "[236]\ttraining's auc: 0.8839\ttraining's binary_logloss: 0.269836\tvalid_1's auc: 0.854598\tvalid_1's binary_logloss: 0.293462\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852257\ttraining's binary_logloss: 0.296851\tvalid_1's auc: 0.846965\tvalid_1's binary_logloss: 0.300025\n",
      "[100]\ttraining's auc: 0.865324\ttraining's binary_logloss: 0.285461\tvalid_1's auc: 0.853435\tvalid_1's binary_logloss: 0.293862\n",
      "[150]\ttraining's auc: 0.873401\ttraining's binary_logloss: 0.27888\tvalid_1's auc: 0.855374\tvalid_1's binary_logloss: 0.292082\n",
      "Early stopping, best iteration is:\n",
      "[182]\ttraining's auc: 0.877816\ttraining's binary_logloss: 0.2754\tvalid_1's auc: 0.855831\tvalid_1's binary_logloss: 0.291676\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852426\ttraining's binary_logloss: 0.296625\tvalid_1's auc: 0.844912\tvalid_1's binary_logloss: 0.30132\n",
      "[100]\ttraining's auc: 0.865493\ttraining's binary_logloss: 0.285398\tvalid_1's auc: 0.851482\tvalid_1's binary_logloss: 0.295057\n",
      "[150]\ttraining's auc: 0.873494\ttraining's binary_logloss: 0.278842\tvalid_1's auc: 0.853561\tvalid_1's binary_logloss: 0.29313\n",
      "Early stopping, best iteration is:\n",
      "[183]\ttraining's auc: 0.87795\ttraining's binary_logloss: 0.275278\tvalid_1's auc: 0.854237\tvalid_1's binary_logloss: 0.292526\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.8542\ttraining's binary_logloss: 0.295851\tvalid_1's auc: 0.837458\tvalid_1's binary_logloss: 0.304095\n",
      "[100]\ttraining's auc: 0.867405\ttraining's binary_logloss: 0.284336\tvalid_1's auc: 0.843432\tvalid_1's binary_logloss: 0.298714\n",
      "[150]\ttraining's auc: 0.875358\ttraining's binary_logloss: 0.27766\tvalid_1's auc: 0.845016\tvalid_1's binary_logloss: 0.297204\n",
      "[200]\ttraining's auc: 0.881803\ttraining's binary_logloss: 0.272383\tvalid_1's auc: 0.845758\tvalid_1's binary_logloss: 0.296688\n",
      "[250]\ttraining's auc: 0.88745\ttraining's binary_logloss: 0.267725\tvalid_1's auc: 0.846256\tvalid_1's binary_logloss: 0.296385\n",
      "Early stopping, best iteration is:\n",
      "[279]\ttraining's auc: 0.890326\ttraining's binary_logloss: 0.265194\tvalid_1's auc: 0.846441\tvalid_1's binary_logloss: 0.296267\n",
      "*********************\n",
      "roc auc estimado:  0.8531346713199813\n",
      "roc auc varianza:  0.0008230005776538714\n",
      "total de variables : 1415\n",
      "variables con importancia acumulada al 99% : 1320\n",
      "variables con zero importancia : 0\n"
     ]
    }
   ],
   "source": [
    "# eliminar variables con zero_importance\n",
    "while True:\n",
    "    test_probs, fi = cross_validation_lightgbm(train, y_train, test)\n",
    "    keep_columns, zero_importance = get_feature_selection(fi)\n",
    "    train = train[keep_columns]\n",
    "    test = test[keep_columns]\n",
    "    if len(zero_importance)==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "primary-ghana",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852465\ttraining's binary_logloss: 0.296169\tvalid_1's auc: 0.843404\tvalid_1's binary_logloss: 0.302823\n",
      "[100]\ttraining's auc: 0.865319\ttraining's binary_logloss: 0.285035\tvalid_1's auc: 0.85101\tvalid_1's binary_logloss: 0.296508\n",
      "[150]\ttraining's auc: 0.873099\ttraining's binary_logloss: 0.278533\tvalid_1's auc: 0.853268\tvalid_1's binary_logloss: 0.294718\n",
      "[200]\ttraining's auc: 0.879707\ttraining's binary_logloss: 0.273318\tvalid_1's auc: 0.854022\tvalid_1's binary_logloss: 0.294056\n",
      "[250]\ttraining's auc: 0.885238\ttraining's binary_logloss: 0.268786\tvalid_1's auc: 0.854511\tvalid_1's binary_logloss: 0.293702\n",
      "Early stopping, best iteration is:\n",
      "[284]\ttraining's auc: 0.88885\ttraining's binary_logloss: 0.265847\tvalid_1's auc: 0.854905\tvalid_1's binary_logloss: 0.293443\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.85207\ttraining's binary_logloss: 0.296295\tvalid_1's auc: 0.843727\tvalid_1's binary_logloss: 0.302667\n",
      "[100]\ttraining's auc: 0.864997\ttraining's binary_logloss: 0.28516\tvalid_1's auc: 0.850819\tvalid_1's binary_logloss: 0.296441\n",
      "[150]\ttraining's auc: 0.872894\ttraining's binary_logloss: 0.27869\tvalid_1's auc: 0.852882\tvalid_1's binary_logloss: 0.294686\n",
      "[200]\ttraining's auc: 0.879319\ttraining's binary_logloss: 0.273464\tvalid_1's auc: 0.85398\tvalid_1's binary_logloss: 0.293806\n",
      "[250]\ttraining's auc: 0.885034\ttraining's binary_logloss: 0.268844\tvalid_1's auc: 0.854512\tvalid_1's binary_logloss: 0.29334\n",
      "Early stopping, best iteration is:\n",
      "[281]\ttraining's auc: 0.888196\ttraining's binary_logloss: 0.266213\tvalid_1's auc: 0.854742\tvalid_1's binary_logloss: 0.29319\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852565\ttraining's binary_logloss: 0.296623\tvalid_1's auc: 0.846616\tvalid_1's binary_logloss: 0.300183\n",
      "[100]\ttraining's auc: 0.865476\ttraining's binary_logloss: 0.285391\tvalid_1's auc: 0.853014\tvalid_1's binary_logloss: 0.294127\n",
      "[150]\ttraining's auc: 0.87339\ttraining's binary_logloss: 0.278849\tvalid_1's auc: 0.855061\tvalid_1's binary_logloss: 0.292292\n",
      "[200]\ttraining's auc: 0.879691\ttraining's binary_logloss: 0.273728\tvalid_1's auc: 0.855846\tvalid_1's binary_logloss: 0.291543\n",
      "Early stopping, best iteration is:\n",
      "[217]\ttraining's auc: 0.881718\ttraining's binary_logloss: 0.272108\tvalid_1's auc: 0.856051\tvalid_1's binary_logloss: 0.291387\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852448\ttraining's binary_logloss: 0.296547\tvalid_1's auc: 0.845263\tvalid_1's binary_logloss: 0.301101\n",
      "[100]\ttraining's auc: 0.865436\ttraining's binary_logloss: 0.285419\tvalid_1's auc: 0.851865\tvalid_1's binary_logloss: 0.294844\n",
      "[150]\ttraining's auc: 0.873421\ttraining's binary_logloss: 0.278811\tvalid_1's auc: 0.853639\tvalid_1's binary_logloss: 0.293075\n",
      "[200]\ttraining's auc: 0.87977\ttraining's binary_logloss: 0.273612\tvalid_1's auc: 0.854416\tvalid_1's binary_logloss: 0.292223\n",
      "Early stopping, best iteration is:\n",
      "[231]\ttraining's auc: 0.88345\ttraining's binary_logloss: 0.270631\tvalid_1's auc: 0.854755\tvalid_1's binary_logloss: 0.291907\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.85419\ttraining's binary_logloss: 0.295888\tvalid_1's auc: 0.837247\tvalid_1's binary_logloss: 0.304169\n",
      "[100]\ttraining's auc: 0.867305\ttraining's binary_logloss: 0.284381\tvalid_1's auc: 0.843265\tvalid_1's binary_logloss: 0.298749\n",
      "[150]\ttraining's auc: 0.875551\ttraining's binary_logloss: 0.277634\tvalid_1's auc: 0.844884\tvalid_1's binary_logloss: 0.29734\n",
      "[200]\ttraining's auc: 0.881898\ttraining's binary_logloss: 0.27236\tvalid_1's auc: 0.84569\tvalid_1's binary_logloss: 0.296586\n",
      "Early stopping, best iteration is:\n",
      "[235]\ttraining's auc: 0.885987\ttraining's binary_logloss: 0.269003\tvalid_1's auc: 0.846008\tvalid_1's binary_logloss: 0.296347\n",
      "*********************\n",
      "roc auc estimado:  0.853330422301886\n",
      "roc auc varianza:  0.000894862383652377\n"
     ]
    }
   ],
   "source": [
    "test_probs, fi = cross_validation_lightgbm(train, y_train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "established-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name='target'\n",
    "test_probs.to_csv('../results/lighgbm_with_1320features_0.85333.csv')   ####### score de 0.85577 en la tabla publica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "frozen-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.to_pickle('../data/train_features1320_score_0.85577.pkl')\n",
    "# test.to_pickle('../data/test_features1320_score_0.85577.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-thermal",
   "metadata": {},
   "source": [
    "## Aggregate Unstack de cod_instit_financiera and PRODUCTO del los ultimos 2 meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "textile-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../data/train_features1320_score_0.85577.pkl')\n",
    "test =pd.read_pickle('../data/test_features1320_score_0.85577.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "leading-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcc():\n",
    "    rcc_train = pd.read_csv(f'{path}/rcc_train.csv')\n",
    "    rcc_test = pd.read_csv(f'{path}/rcc_test.csv')\n",
    "    ##### inputar datos faltantes en la base de rcc_test\n",
    "    rcc_test['cod_instit_financiera'].fillna(rcc_test['cod_instit_financiera'].value_counts().index[0], inplace=True)\n",
    "    rcc_test['PRODUCTO'].fillna(rcc_test['PRODUCTO'].value_counts().index[0], inplace=True)\n",
    "\n",
    "    dict_ = {'codmes': 'int32',\n",
    "     'key_value': 'int32',\n",
    "     'condicion': 'int32',\n",
    "     'tipo_credito': 'int32',\n",
    "     'cod_instit_financiera': 'int32',\n",
    "     'PRODUCTO': 'int32',\n",
    "     'RIESGO_DIRECTO': 'int32',\n",
    "     'COD_CLASIFICACION_DEUDOR': 'int32'}\n",
    "    rcc_train = rcc_train.astype(dict_)\n",
    "    rcc_test = rcc_test.astype(dict_)\n",
    "    return rcc_train, rcc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "infinite-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rcc_list(df, desde, n):\n",
    "    rcc_list=[]\n",
    "    for feature in ['cod_instit_financiera','PRODUCTO']:\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Sum'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Unique'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Min'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Max'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Std'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Mean'))\n",
    "        rcc_list.append(get_unstack(df, feature, desde, n, 'Median'))\n",
    "    return rcc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ideal-salvation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de cod_instit_financiera desde 201801 con la funcion de agregacion Sum\n",
      "haciendo unstack de cod_instit_financiera desde 201801 con la funcion de agregacion Unique\n",
      "haciendo unstack de cod_instit_financiera desde 201801 con la funcion de agregacion Min\n",
      "haciendo unstack de cod_instit_financiera desde 201801 con la funcion de agregacion Max\n",
      "haciendo unstack de cod_instit_financiera desde 201801 con la funcion de agregacion Std\n",
      "haciendo unstack de cod_instit_financiera desde 201801 con la funcion de agregacion Mean\n",
      "haciendo unstack de cod_instit_financiera desde 201801 con la funcion de agregacion Median\n",
      "haciendo unstack de PRODUCTO desde 201801 con la funcion de agregacion Sum\n",
      "haciendo unstack de PRODUCTO desde 201801 con la funcion de agregacion Unique\n",
      "haciendo unstack de PRODUCTO desde 201801 con la funcion de agregacion Min\n",
      "haciendo unstack de PRODUCTO desde 201801 con la funcion de agregacion Max\n",
      "haciendo unstack de PRODUCTO desde 201801 con la funcion de agregacion Std\n",
      "haciendo unstack de PRODUCTO desde 201801 con la funcion de agregacion Mean\n",
      "haciendo unstack de PRODUCTO desde 201801 con la funcion de agregacion Median\n",
      "haciendo unstack de cod_instit_financiera desde 201901 con la funcion de agregacion Sum\n",
      "haciendo unstack de cod_instit_financiera desde 201901 con la funcion de agregacion Unique\n",
      "haciendo unstack de cod_instit_financiera desde 201901 con la funcion de agregacion Min\n",
      "haciendo unstack de cod_instit_financiera desde 201901 con la funcion de agregacion Max\n",
      "haciendo unstack de cod_instit_financiera desde 201901 con la funcion de agregacion Std\n",
      "haciendo unstack de cod_instit_financiera desde 201901 con la funcion de agregacion Mean\n",
      "haciendo unstack de cod_instit_financiera desde 201901 con la funcion de agregacion Median\n",
      "haciendo unstack de PRODUCTO desde 201901 con la funcion de agregacion Sum\n",
      "haciendo unstack de PRODUCTO desde 201901 con la funcion de agregacion Unique\n",
      "haciendo unstack de PRODUCTO desde 201901 con la funcion de agregacion Min\n",
      "haciendo unstack de PRODUCTO desde 201901 con la funcion de agregacion Max\n",
      "haciendo unstack de PRODUCTO desde 201901 con la funcion de agregacion Std\n",
      "haciendo unstack de PRODUCTO desde 201901 con la funcion de agregacion Mean\n",
      "haciendo unstack de PRODUCTO desde 201901 con la funcion de agregacion Median\n",
      "(358487, 994) (396666, 910)\n",
      "(358487, 903) (396666, 903)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((358487, 2223), (396666, 2223))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcc_train, rcc_test = get_rcc()\n",
    "\n",
    "rcc_train_list = get_rcc_list(rcc_train,201801, 1)\n",
    "rcc_test_list = get_rcc_list(rcc_test,201901, 1)\n",
    "\n",
    "rcc_train = pd.concat(rcc_train_list, axis=1)\n",
    "rcc_test = pd.concat(rcc_test_list, axis=1)\n",
    "del rcc_train_list, rcc_test_list\n",
    "\n",
    "rcc_train , rcc_test = get_keep_columns(rcc_train, rcc_test)\n",
    "train = train.join(rcc_train)\n",
    "test = test.join(rcc_test)\n",
    "del rcc_train, rcc_test\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hawaiian-restriction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853303\ttraining's binary_logloss: 0.295741\tvalid_1's auc: 0.844058\tvalid_1's binary_logloss: 0.302665\n",
      "[100]\ttraining's auc: 0.865962\ttraining's binary_logloss: 0.284604\tvalid_1's auc: 0.851174\tvalid_1's binary_logloss: 0.296581\n",
      "[150]\ttraining's auc: 0.873915\ttraining's binary_logloss: 0.277986\tvalid_1's auc: 0.853349\tvalid_1's binary_logloss: 0.294757\n",
      "[200]\ttraining's auc: 0.88046\ttraining's binary_logloss: 0.272653\tvalid_1's auc: 0.854363\tvalid_1's binary_logloss: 0.293898\n",
      "[250]\ttraining's auc: 0.88595\ttraining's binary_logloss: 0.268115\tvalid_1's auc: 0.854619\tvalid_1's binary_logloss: 0.293697\n",
      "Early stopping, best iteration is:\n",
      "[264]\ttraining's auc: 0.887542\ttraining's binary_logloss: 0.266838\tvalid_1's auc: 0.854755\tvalid_1's binary_logloss: 0.293611\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852472\ttraining's binary_logloss: 0.296088\tvalid_1's auc: 0.844083\tvalid_1's binary_logloss: 0.302475\n",
      "[100]\ttraining's auc: 0.865501\ttraining's binary_logloss: 0.284824\tvalid_1's auc: 0.851338\tvalid_1's binary_logloss: 0.296097\n",
      "[150]\ttraining's auc: 0.873508\ttraining's binary_logloss: 0.278224\tvalid_1's auc: 0.853453\tvalid_1's binary_logloss: 0.294325\n",
      "[200]\ttraining's auc: 0.879831\ttraining's binary_logloss: 0.272977\tvalid_1's auc: 0.854392\tvalid_1's binary_logloss: 0.293536\n",
      "Early stopping, best iteration is:\n",
      "[233]\ttraining's auc: 0.883627\ttraining's binary_logloss: 0.269886\tvalid_1's auc: 0.854747\tvalid_1's binary_logloss: 0.293311\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852697\ttraining's binary_logloss: 0.296577\tvalid_1's auc: 0.846935\tvalid_1's binary_logloss: 0.299871\n",
      "[100]\ttraining's auc: 0.865822\ttraining's binary_logloss: 0.285148\tvalid_1's auc: 0.853487\tvalid_1's binary_logloss: 0.293659\n",
      "[150]\ttraining's auc: 0.873784\ttraining's binary_logloss: 0.278489\tvalid_1's auc: 0.855312\tvalid_1's binary_logloss: 0.291919\n",
      "[200]\ttraining's auc: 0.880137\ttraining's binary_logloss: 0.273289\tvalid_1's auc: 0.856085\tvalid_1's binary_logloss: 0.291271\n",
      "Early stopping, best iteration is:\n",
      "[209]\ttraining's auc: 0.881202\ttraining's binary_logloss: 0.272416\tvalid_1's auc: 0.856218\tvalid_1's binary_logloss: 0.291167\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.85266\ttraining's binary_logloss: 0.296495\tvalid_1's auc: 0.844784\tvalid_1's binary_logloss: 0.301548\n",
      "[100]\ttraining's auc: 0.865904\ttraining's binary_logloss: 0.285108\tvalid_1's auc: 0.851538\tvalid_1's binary_logloss: 0.295089\n",
      "[150]\ttraining's auc: 0.873839\ttraining's binary_logloss: 0.278459\tvalid_1's auc: 0.853589\tvalid_1's binary_logloss: 0.293197\n",
      "[200]\ttraining's auc: 0.88033\ttraining's binary_logloss: 0.273139\tvalid_1's auc: 0.854317\tvalid_1's binary_logloss: 0.292463\n",
      "[250]\ttraining's auc: 0.88626\ttraining's binary_logloss: 0.268396\tvalid_1's auc: 0.854673\tvalid_1's binary_logloss: 0.292082\n",
      "Early stopping, best iteration is:\n",
      "[240]\ttraining's auc: 0.885039\ttraining's binary_logloss: 0.269336\tvalid_1's auc: 0.854721\tvalid_1's binary_logloss: 0.292088\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.854696\ttraining's binary_logloss: 0.2955\tvalid_1's auc: 0.837801\tvalid_1's binary_logloss: 0.303775\n",
      "[100]\ttraining's auc: 0.867782\ttraining's binary_logloss: 0.283968\tvalid_1's auc: 0.84387\tvalid_1's binary_logloss: 0.298348\n",
      "[150]\ttraining's auc: 0.875973\ttraining's binary_logloss: 0.277188\tvalid_1's auc: 0.845679\tvalid_1's binary_logloss: 0.296825\n",
      "Early stopping, best iteration is:\n",
      "[175]\ttraining's auc: 0.879381\ttraining's binary_logloss: 0.274399\tvalid_1's auc: 0.846143\tvalid_1's binary_logloss: 0.296452\n",
      "*********************\n",
      "roc auc estimado:  0.8533545078722546\n",
      "roc auc varianza:  0.0008871410255861866\n"
     ]
    }
   ],
   "source": [
    "test_probs, fi = cross_validation_lightgbm(train, y_train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "applied-track",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de variables : 2223\n",
      "variables con importancia acumulada al 99% : 1482\n",
      "variables con zero importancia : 590\n"
     ]
    }
   ],
   "source": [
    "keep_columns, zero_importance = get_feature_selection(fi)\n",
    "# total de variables : 1490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "federal-thomas",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853168\ttraining's binary_logloss: 0.295812\tvalid_1's auc: 0.843919\tvalid_1's binary_logloss: 0.302772\n",
      "[100]\ttraining's auc: 0.865882\ttraining's binary_logloss: 0.284621\tvalid_1's auc: 0.851262\tvalid_1's binary_logloss: 0.296494\n",
      "[150]\ttraining's auc: 0.873988\ttraining's binary_logloss: 0.278015\tvalid_1's auc: 0.853524\tvalid_1's binary_logloss: 0.294641\n",
      "[200]\ttraining's auc: 0.880473\ttraining's binary_logloss: 0.272722\tvalid_1's auc: 0.854603\tvalid_1's binary_logloss: 0.293776\n",
      "[250]\ttraining's auc: 0.886051\ttraining's binary_logloss: 0.268107\tvalid_1's auc: 0.855315\tvalid_1's binary_logloss: 0.293383\n",
      "Early stopping, best iteration is:\n",
      "[273]\ttraining's auc: 0.888636\ttraining's binary_logloss: 0.266039\tvalid_1's auc: 0.855515\tvalid_1's binary_logloss: 0.293312\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852808\ttraining's binary_logloss: 0.295968\tvalid_1's auc: 0.844255\tvalid_1's binary_logloss: 0.302509\n",
      "[100]\ttraining's auc: 0.86579\ttraining's binary_logloss: 0.284746\tvalid_1's auc: 0.851331\tvalid_1's binary_logloss: 0.296383\n",
      "[150]\ttraining's auc: 0.873733\ttraining's binary_logloss: 0.278052\tvalid_1's auc: 0.853614\tvalid_1's binary_logloss: 0.294487\n",
      "[200]\ttraining's auc: 0.880339\ttraining's binary_logloss: 0.272729\tvalid_1's auc: 0.854577\tvalid_1's binary_logloss: 0.293692\n",
      "[250]\ttraining's auc: 0.885883\ttraining's binary_logloss: 0.268119\tvalid_1's auc: 0.854921\tvalid_1's binary_logloss: 0.293409\n",
      "Early stopping, best iteration is:\n",
      "[243]\ttraining's auc: 0.885122\ttraining's binary_logloss: 0.268773\tvalid_1's auc: 0.854955\tvalid_1's binary_logloss: 0.293401\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852739\ttraining's binary_logloss: 0.296405\tvalid_1's auc: 0.846804\tvalid_1's binary_logloss: 0.300051\n",
      "[100]\ttraining's auc: 0.865783\ttraining's binary_logloss: 0.285128\tvalid_1's auc: 0.853144\tvalid_1's binary_logloss: 0.293928\n",
      "[150]\ttraining's auc: 0.87375\ttraining's binary_logloss: 0.278445\tvalid_1's auc: 0.855208\tvalid_1's binary_logloss: 0.292084\n",
      "Early stopping, best iteration is:\n",
      "[181]\ttraining's auc: 0.877848\ttraining's binary_logloss: 0.275159\tvalid_1's auc: 0.855784\tvalid_1's binary_logloss: 0.29153\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852696\ttraining's binary_logloss: 0.296492\tvalid_1's auc: 0.845452\tvalid_1's binary_logloss: 0.301181\n",
      "[100]\ttraining's auc: 0.865987\ttraining's binary_logloss: 0.285004\tvalid_1's auc: 0.852292\tvalid_1's binary_logloss: 0.294663\n",
      "[150]\ttraining's auc: 0.874053\ttraining's binary_logloss: 0.278372\tvalid_1's auc: 0.854348\tvalid_1's binary_logloss: 0.292796\n",
      "[200]\ttraining's auc: 0.880612\ttraining's binary_logloss: 0.272988\tvalid_1's auc: 0.855115\tvalid_1's binary_logloss: 0.292066\n",
      "Early stopping, best iteration is:\n",
      "[195]\ttraining's auc: 0.880023\ttraining's binary_logloss: 0.273463\tvalid_1's auc: 0.855174\tvalid_1's binary_logloss: 0.292073\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.854524\ttraining's binary_logloss: 0.295581\tvalid_1's auc: 0.837653\tvalid_1's binary_logloss: 0.303747\n",
      "[100]\ttraining's auc: 0.867696\ttraining's binary_logloss: 0.284102\tvalid_1's auc: 0.84386\tvalid_1's binary_logloss: 0.298361\n",
      "[150]\ttraining's auc: 0.876021\ttraining's binary_logloss: 0.277259\tvalid_1's auc: 0.845717\tvalid_1's binary_logloss: 0.296788\n",
      "[200]\ttraining's auc: 0.882562\ttraining's binary_logloss: 0.271849\tvalid_1's auc: 0.846273\tvalid_1's binary_logloss: 0.296252\n",
      "Early stopping, best iteration is:\n",
      "[214]\ttraining's auc: 0.884117\ttraining's binary_logloss: 0.270475\tvalid_1's auc: 0.84645\tvalid_1's binary_logloss: 0.296116\n",
      "*********************\n",
      "roc auc estimado:  0.8536114393699352\n",
      "roc auc varianza:  0.0008677661568596232\n"
     ]
    }
   ],
   "source": [
    "test_probs, fi = cross_validation_lightgbm(train[keep_columns], y_train, test[keep_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "palestinian-uniform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1482), (396666, 1482))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[keep_columns]\n",
    "test = test[keep_columns]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cordless-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name = 'target'\n",
    "test_probs.to_csv('../results/lightgbm_with_1482features_score_0.85361.csv') #### score de 0.85605 en la tabla publica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "major-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('aux_1.npy', train.columns)\n",
    "train.to_pickle('../data/train_features1482_score_0.85605.pkl')\n",
    "test.to_pickle('../data/test_features1482_score_0.85605.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-construction",
   "metadata": {},
   "source": [
    "## Aggregate Unstack de cod_instit_financiera and PRODUCTO del los ultimos 3 meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "structural-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../data/train_features1482_score_0.85605.pkl')\n",
    "test = pd.read_pickle('../data/test_features1482_score_0.85605.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "racial-housing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de cod_instit_financiera desde 201712 con la funcion de agregacion Sum\n",
      "haciendo unstack de cod_instit_financiera desde 201712 con la funcion de agregacion Unique\n",
      "haciendo unstack de cod_instit_financiera desde 201712 con la funcion de agregacion Min\n",
      "haciendo unstack de cod_instit_financiera desde 201712 con la funcion de agregacion Max\n",
      "haciendo unstack de cod_instit_financiera desde 201712 con la funcion de agregacion Std\n",
      "haciendo unstack de cod_instit_financiera desde 201712 con la funcion de agregacion Mean\n",
      "haciendo unstack de cod_instit_financiera desde 201712 con la funcion de agregacion Median\n",
      "haciendo unstack de PRODUCTO desde 201712 con la funcion de agregacion Sum\n",
      "haciendo unstack de PRODUCTO desde 201712 con la funcion de agregacion Unique\n",
      "haciendo unstack de PRODUCTO desde 201712 con la funcion de agregacion Min\n",
      "haciendo unstack de PRODUCTO desde 201712 con la funcion de agregacion Max\n",
      "haciendo unstack de PRODUCTO desde 201712 con la funcion de agregacion Std\n",
      "haciendo unstack de PRODUCTO desde 201712 con la funcion de agregacion Mean\n",
      "haciendo unstack de PRODUCTO desde 201712 con la funcion de agregacion Median\n",
      "haciendo unstack de cod_instit_financiera desde 201812 con la funcion de agregacion Sum\n",
      "haciendo unstack de cod_instit_financiera desde 201812 con la funcion de agregacion Unique\n",
      "haciendo unstack de cod_instit_financiera desde 201812 con la funcion de agregacion Min\n",
      "haciendo unstack de cod_instit_financiera desde 201812 con la funcion de agregacion Max\n",
      "haciendo unstack de cod_instit_financiera desde 201812 con la funcion de agregacion Std\n",
      "haciendo unstack de cod_instit_financiera desde 201812 con la funcion de agregacion Mean\n",
      "haciendo unstack de cod_instit_financiera desde 201812 con la funcion de agregacion Median\n",
      "haciendo unstack de PRODUCTO desde 201812 con la funcion de agregacion Sum\n",
      "haciendo unstack de PRODUCTO desde 201812 con la funcion de agregacion Unique\n",
      "haciendo unstack de PRODUCTO desde 201812 con la funcion de agregacion Min\n",
      "haciendo unstack de PRODUCTO desde 201812 con la funcion de agregacion Max\n",
      "haciendo unstack de PRODUCTO desde 201812 con la funcion de agregacion Std\n",
      "haciendo unstack de PRODUCTO desde 201812 con la funcion de agregacion Mean\n",
      "haciendo unstack de PRODUCTO desde 201812 con la funcion de agregacion Median\n",
      "(358487, 994) (396666, 917)\n",
      "(358487, 910) (396666, 910)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((358487, 2392), (396666, 2392))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcc_train, rcc_test = get_rcc()\n",
    "\n",
    "rcc_train_list = get_rcc_list(rcc_train,201712, 2)\n",
    "rcc_test_list = get_rcc_list(rcc_test,201812, 2)\n",
    "\n",
    "rcc_train = pd.concat(rcc_train_list, axis=1)\n",
    "rcc_test = pd.concat(rcc_test_list, axis=1)\n",
    "del rcc_train_list, rcc_test_list\n",
    "\n",
    "rcc_train , rcc_test = get_keep_columns(rcc_train, rcc_test)\n",
    "train = train.join(rcc_train)\n",
    "test = test.join(rcc_test)\n",
    "del rcc_train, rcc_test\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "effective-chocolate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852847\ttraining's binary_logloss: 0.295968\tvalid_1's auc: 0.843726\tvalid_1's binary_logloss: 0.302931\n",
      "[100]\ttraining's auc: 0.865787\ttraining's binary_logloss: 0.284642\tvalid_1's auc: 0.850971\tvalid_1's binary_logloss: 0.296822\n",
      "[150]\ttraining's auc: 0.873914\ttraining's binary_logloss: 0.27793\tvalid_1's auc: 0.853294\tvalid_1's binary_logloss: 0.294852\n",
      "[200]\ttraining's auc: 0.880503\ttraining's binary_logloss: 0.272635\tvalid_1's auc: 0.854071\tvalid_1's binary_logloss: 0.294145\n",
      "Early stopping, best iteration is:\n",
      "[232]\ttraining's auc: 0.88422\ttraining's binary_logloss: 0.269592\tvalid_1's auc: 0.854465\tvalid_1's binary_logloss: 0.293846\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853061\ttraining's binary_logloss: 0.295878\tvalid_1's auc: 0.844201\tvalid_1's binary_logloss: 0.302636\n",
      "[100]\ttraining's auc: 0.866015\ttraining's binary_logloss: 0.284511\tvalid_1's auc: 0.851131\tvalid_1's binary_logloss: 0.296389\n",
      "[150]\ttraining's auc: 0.874165\ttraining's binary_logloss: 0.277785\tvalid_1's auc: 0.853386\tvalid_1's binary_logloss: 0.294482\n",
      "[200]\ttraining's auc: 0.880667\ttraining's binary_logloss: 0.272409\tvalid_1's auc: 0.854202\tvalid_1's binary_logloss: 0.293718\n",
      "[250]\ttraining's auc: 0.886282\ttraining's binary_logloss: 0.267758\tvalid_1's auc: 0.854622\tvalid_1's binary_logloss: 0.293391\n",
      "Early stopping, best iteration is:\n",
      "[267]\ttraining's auc: 0.888156\ttraining's binary_logloss: 0.266257\tvalid_1's auc: 0.854857\tvalid_1's binary_logloss: 0.29321\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852771\ttraining's binary_logloss: 0.296483\tvalid_1's auc: 0.846778\tvalid_1's binary_logloss: 0.299849\n",
      "[100]\ttraining's auc: 0.865793\ttraining's binary_logloss: 0.28516\tvalid_1's auc: 0.853115\tvalid_1's binary_logloss: 0.293873\n",
      "[150]\ttraining's auc: 0.873839\ttraining's binary_logloss: 0.278459\tvalid_1's auc: 0.85491\tvalid_1's binary_logloss: 0.292081\n",
      "[200]\ttraining's auc: 0.880462\ttraining's binary_logloss: 0.273136\tvalid_1's auc: 0.855545\tvalid_1's binary_logloss: 0.291495\n",
      "Early stopping, best iteration is:\n",
      "[230]\ttraining's auc: 0.8841\ttraining's binary_logloss: 0.270193\tvalid_1's auc: 0.855952\tvalid_1's binary_logloss: 0.291188\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852862\ttraining's binary_logloss: 0.296413\tvalid_1's auc: 0.844994\tvalid_1's binary_logloss: 0.30134\n",
      "[100]\ttraining's auc: 0.865865\ttraining's binary_logloss: 0.284998\tvalid_1's auc: 0.851734\tvalid_1's binary_logloss: 0.294832\n",
      "[150]\ttraining's auc: 0.87409\ttraining's binary_logloss: 0.278303\tvalid_1's auc: 0.85377\tvalid_1's binary_logloss: 0.293006\n",
      "[200]\ttraining's auc: 0.8806\ttraining's binary_logloss: 0.272981\tvalid_1's auc: 0.85484\tvalid_1's binary_logloss: 0.292034\n",
      "Early stopping, best iteration is:\n",
      "[229]\ttraining's auc: 0.883902\ttraining's binary_logloss: 0.270213\tvalid_1's auc: 0.85505\tvalid_1's binary_logloss: 0.291795\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.854744\ttraining's binary_logloss: 0.29557\tvalid_1's auc: 0.83796\tvalid_1's binary_logloss: 0.30368\n",
      "[100]\ttraining's auc: 0.867904\ttraining's binary_logloss: 0.284\tvalid_1's auc: 0.843956\tvalid_1's binary_logloss: 0.298325\n",
      "[150]\ttraining's auc: 0.876191\ttraining's binary_logloss: 0.277045\tvalid_1's auc: 0.845759\tvalid_1's binary_logloss: 0.296739\n",
      "[200]\ttraining's auc: 0.882828\ttraining's binary_logloss: 0.271573\tvalid_1's auc: 0.846151\tvalid_1's binary_logloss: 0.29627\n",
      "[250]\ttraining's auc: 0.888506\ttraining's binary_logloss: 0.266809\tvalid_1's auc: 0.846632\tvalid_1's binary_logloss: 0.295926\n",
      "Early stopping, best iteration is:\n",
      "[261]\ttraining's auc: 0.889648\ttraining's binary_logloss: 0.26584\tvalid_1's auc: 0.846719\tvalid_1's binary_logloss: 0.295851\n",
      "*********************\n",
      "roc auc estimado:  0.8534387483517538\n",
      "roc auc varianza:  0.0008254484325110733\n"
     ]
    }
   ],
   "source": [
    "test_probs, fi = cross_validation_lightgbm(train, y_train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fatal-convert",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de variables : 2392\n",
      "variables con importancia acumulada al 99% : 1628\n",
      "variables con zero importancia : 588\n"
     ]
    }
   ],
   "source": [
    "keep_columns, zero_importance = get_feature_selection(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "infectious-consumer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853188\ttraining's binary_logloss: 0.295697\tvalid_1's auc: 0.844304\tvalid_1's binary_logloss: 0.302495\n",
      "[100]\ttraining's auc: 0.865934\ttraining's binary_logloss: 0.284444\tvalid_1's auc: 0.851608\tvalid_1's binary_logloss: 0.296298\n",
      "[150]\ttraining's auc: 0.874047\ttraining's binary_logloss: 0.277757\tvalid_1's auc: 0.853819\tvalid_1's binary_logloss: 0.294482\n",
      "Early stopping, best iteration is:\n",
      "[182]\ttraining's auc: 0.878261\ttraining's binary_logloss: 0.27438\tvalid_1's auc: 0.854401\tvalid_1's binary_logloss: 0.294017\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852669\ttraining's binary_logloss: 0.296081\tvalid_1's auc: 0.844036\tvalid_1's binary_logloss: 0.302764\n",
      "[100]\ttraining's auc: 0.865794\ttraining's binary_logloss: 0.284727\tvalid_1's auc: 0.851538\tvalid_1's binary_logloss: 0.296237\n",
      "[150]\ttraining's auc: 0.873876\ttraining's binary_logloss: 0.277999\tvalid_1's auc: 0.853817\tvalid_1's binary_logloss: 0.294233\n",
      "[200]\ttraining's auc: 0.880394\ttraining's binary_logloss: 0.27267\tvalid_1's auc: 0.854701\tvalid_1's binary_logloss: 0.293414\n",
      "[250]\ttraining's auc: 0.886162\ttraining's binary_logloss: 0.268\tvalid_1's auc: 0.855197\tvalid_1's binary_logloss: 0.293017\n",
      "Early stopping, best iteration is:\n",
      "[260]\ttraining's auc: 0.887218\ttraining's binary_logloss: 0.267118\tvalid_1's auc: 0.855368\tvalid_1's binary_logloss: 0.292908\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853046\ttraining's binary_logloss: 0.296378\tvalid_1's auc: 0.847858\tvalid_1's binary_logloss: 0.299569\n",
      "[100]\ttraining's auc: 0.865825\ttraining's binary_logloss: 0.285029\tvalid_1's auc: 0.853978\tvalid_1's binary_logloss: 0.293482\n",
      "[150]\ttraining's auc: 0.873773\ttraining's binary_logloss: 0.278366\tvalid_1's auc: 0.855995\tvalid_1's binary_logloss: 0.291639\n",
      "[200]\ttraining's auc: 0.880216\ttraining's binary_logloss: 0.273114\tvalid_1's auc: 0.856705\tvalid_1's binary_logloss: 0.290929\n",
      "Early stopping, best iteration is:\n",
      "[226]\ttraining's auc: 0.883229\ttraining's binary_logloss: 0.270687\tvalid_1's auc: 0.856928\tvalid_1's binary_logloss: 0.290722\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852945\ttraining's binary_logloss: 0.296339\tvalid_1's auc: 0.845627\tvalid_1's binary_logloss: 0.301076\n",
      "[100]\ttraining's auc: 0.866066\ttraining's binary_logloss: 0.284922\tvalid_1's auc: 0.852288\tvalid_1's binary_logloss: 0.29465\n",
      "[150]\ttraining's auc: 0.874352\ttraining's binary_logloss: 0.278147\tvalid_1's auc: 0.854286\tvalid_1's binary_logloss: 0.292651\n",
      "[200]\ttraining's auc: 0.880759\ttraining's binary_logloss: 0.272811\tvalid_1's auc: 0.855017\tvalid_1's binary_logloss: 0.291883\n",
      "[250]\ttraining's auc: 0.886473\ttraining's binary_logloss: 0.268167\tvalid_1's auc: 0.855466\tvalid_1's binary_logloss: 0.291503\n",
      "Early stopping, best iteration is:\n",
      "[274]\ttraining's auc: 0.88911\ttraining's binary_logloss: 0.266014\tvalid_1's auc: 0.855552\tvalid_1's binary_logloss: 0.291385\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.855245\ttraining's binary_logloss: 0.295292\tvalid_1's auc: 0.837577\tvalid_1's binary_logloss: 0.303904\n",
      "[100]\ttraining's auc: 0.868228\ttraining's binary_logloss: 0.283699\tvalid_1's auc: 0.843208\tvalid_1's binary_logloss: 0.298629\n",
      "[150]\ttraining's auc: 0.876454\ttraining's binary_logloss: 0.276879\tvalid_1's auc: 0.845027\tvalid_1's binary_logloss: 0.297073\n",
      "[200]\ttraining's auc: 0.882848\ttraining's binary_logloss: 0.27156\tvalid_1's auc: 0.845672\tvalid_1's binary_logloss: 0.296606\n",
      "[250]\ttraining's auc: 0.888596\ttraining's binary_logloss: 0.26686\tvalid_1's auc: 0.846101\tvalid_1's binary_logloss: 0.296215\n",
      "Early stopping, best iteration is:\n",
      "[241]\ttraining's auc: 0.887585\ttraining's binary_logloss: 0.267656\tvalid_1's auc: 0.846202\tvalid_1's binary_logloss: 0.296187\n",
      "*********************\n",
      "roc auc estimado:  0.853743491016964\n",
      "roc auc varianza:  0.0009315890674282501\n"
     ]
    }
   ],
   "source": [
    "test_probs, fi = cross_validation_lightgbm(train[keep_columns], y_train, test[keep_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "center-facility",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edad                                                 0.008045\n",
       "ubigeo_mode                                          0.007883\n",
       "sexo_0                                               0.005815\n",
       "PRODUCTO_1_saldoMax_ult1meses                        0.005545\n",
       "PRODUCTO_1_saldoMean_ult1meses                       0.005412\n",
       "PRODUCTO_3_saldoUnique_ult12meses                    0.004899\n",
       "estadocontribuyente_0                                0.004498\n",
       "COD_CLASIFICACION_DEUDOR_0_saldoUnique_ult12meses    0.004368\n",
       "PRODUCTO_1_saldoSum_ult1meses                        0.004280\n",
       "PRODUCTO_6_saldoSum_ult12meses                       0.004146\n",
       "PRODUCTO_1_saldoSum_ult2meses                        0.003999\n",
       "RIESGO_DIRECTO_1_saldoSum_ult1meses                  0.003904\n",
       "PRODUCTO_8_saldoUnique_ult12meses                    0.003786\n",
       "cod_instit_financiera_34_saldoUnique_ult12meses      0.003680\n",
       "RIESGO_DIRECTO_1_saldoStd_ult1meses                  0.003663\n",
       "RIESGO_DIRECTO_1_saldoUnique_ult12meses              0.003588\n",
       "PRODUCTO_8_saldoSum_ult12meses                       0.003495\n",
       "PRODUCTO_1_saldoMedian_ult1meses                     0.003406\n",
       "PRODUCTO_4_saldoStd_ult12meses                       0.003328\n",
       "ciiu_mode                                            0.003298\n",
       "PRODUCTO_0_saldoStd_ult12meses                       0.003245\n",
       "condicion_0_saldoUnique_ult12meses                   0.003086\n",
       "RIESGO_DIRECTO_-1_saldoMedian_ult1meses              0.003069\n",
       "fecalta_min                                          0.003050\n",
       "RIESGO_DIRECTO_1_saldoMin_ult1meses                  0.003050\n",
       "PRODUCTO_1_saldoSum_ult12meses                       0.003042\n",
       "PRODUCTO_1_saldoMax_ult3meses                        0.003025\n",
       "RIESGO_DIRECTO_1_saldoMax_ult12meses                 0.003016\n",
       "PRODUCTO_0_saldoMin_ult12meses                       0.002986\n",
       "PRODUCTO_0_saldoStd_ult1meses                        0.002976\n",
       "PRODUCTO_1_saldoMedian_ult2meses                     0.002932\n",
       "PRODUCTO_1_saldoMedian_ult3meses                     0.002926\n",
       "fecalta_mean                                         0.002913\n",
       "ctd_veh                                              0.002875\n",
       "PRODUCTO_3_saldoMedian_ult1meses                     0.002867\n",
       "cod_instit_financiera_10_saldoStd_ult1meses          0.002849\n",
       "PRODUCTO_12_saldoMin_ult1meses                       0.002844\n",
       "PRODUCTO_0_saldoMax_ult1meses                        0.002783\n",
       "cod_instit_financiera_max_ult1mes                    0.002751\n",
       "PRODUCTO_3_saldoMean_ult1meses                       0.002724\n",
       "PRODUCTO_7_saldoMax_ult12meses                       0.002625\n",
       "PRODUCTO_1_saldoMax_ult2meses                        0.002613\n",
       "PRODUCTO_1_saldoMean_ult2meses                       0.002590\n",
       "PRODUCTO_1_saldoMean_ult3meses                       0.002581\n",
       "PRODUCTO_1_saldoStd_ult2meses                        0.002571\n",
       "PRODUCTO_2_saldoUnique_ult12meses                    0.002566\n",
       "RIESGO_DIRECTO_1_saldoStd_ult12meses                 0.002551\n",
       "PRODUCTO_6_saldoMin_ult12meses                       0.002522\n",
       "rgn_4                                                0.002455\n",
       "PRODUCTO_8_saldoMin_ult12meses                       0.002389\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi.sort_values(ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "chicken-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name = 'target'\n",
    "test_probs.to_csv('../results/lightgbm_with_1628features_score_0.85374.csv') #### score de 0.85642 en la tabla publica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "corrected-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[keep_columns]\n",
    "test = test[keep_columns]\n",
    "train.shape, test.shape\n",
    "np.save('aux_2.npy', train.columns)\n",
    "train.to_pickle('../data/train_features1628_score_0.85642.pkl')\n",
    "test.to_pickle('../data/test_features1628_score_0.85642.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "recorded-confusion",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PRODUCTO_1     28\n",
       "PRODUCTO_0     28\n",
       "PRODUCTO_11    28\n",
       "PRODUCTO_4     27\n",
       "PRODUCTO_6     26\n",
       "PRODUCTO_3     26\n",
       "PRODUCTO_8     26\n",
       "PRODUCTO_2     25\n",
       "PRODUCTO_13    24\n",
       "PRODUCTO_15    23\n",
       "PRODUCTO_7     22\n",
       "PRODUCTO_5     20\n",
       "PRODUCTO_9     18\n",
       "PRODUCTO_12    11\n",
       "PRODUCTO_21     6\n",
       "PRODUCTO_29     6\n",
       "PRODUCTO_10     4\n",
       "PRODUCTO_16     4\n",
       "PRODUCTO_20     3\n",
       "PRODUCTO_14     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regex_producto = re.compile(r'PRODUCTO.*meses')\n",
    "regex_cod_instit = re.compile(r'cod_instit_financiera.*meses')\n",
    "features_1628 = list(fi.index)\n",
    "pd.Series([\"_\".join(f.split('_')[:-2]) for f in features_1628 if regex_producto.match(f)]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "relevant-diagram",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cod_instit_financiera_34    27\n",
       "cod_instit_financiera_10    27\n",
       "cod_instit_financiera_37    25\n",
       "cod_instit_financiera_1     25\n",
       "cod_instit_financiera_33    23\n",
       "cod_instit_financiera_55    23\n",
       "cod_instit_financiera_11    23\n",
       "cod_instit_financiera_28    23\n",
       "cod_instit_financiera_32    21\n",
       "cod_instit_financiera_0     20\n",
       "cod_instit_financiera_61    19\n",
       "cod_instit_financiera_13    18\n",
       "cod_instit_financiera_20    15\n",
       "cod_instit_financiera_2     14\n",
       "cod_instit_financiera_8     13\n",
       "cod_instit_financiera_57    13\n",
       "cod_instit_financiera_46    13\n",
       "cod_instit_financiera_36    13\n",
       "cod_instit_financiera_18    12\n",
       "cod_instit_financiera_3     11\n",
       "cod_instit_financiera_19    10\n",
       "cod_instit_financiera_15    10\n",
       "cod_instit_financiera_17     7\n",
       "cod_instit_financiera_38     6\n",
       "cod_instit_financiera_41     6\n",
       "cod_instit_financiera_25     5\n",
       "cod_instit_financiera_21     5\n",
       "cod_instit_financiera_65     5\n",
       "cod_instit_financiera_63     5\n",
       "cod_instit_financiera_62     4\n",
       "cod_instit_financiera_7      4\n",
       "cod_instit_financiera_45     4\n",
       "cod_instit_financiera_54     4\n",
       "cod_instit_financiera_29     2\n",
       "cod_instit_financiera_22     2\n",
       "cod_instit_financiera_16     2\n",
       "cod_instit_financiera_56     2\n",
       "cod_instit_financiera_27     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([\"_\".join(f.split('_')[:-2]) for f in features_1628 if regex_cod_instit.match(f)]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-baseball",
   "metadata": {},
   "source": [
    "## Aggregate Unstack de cod_instit_financiera and PRODUCTO del los ultimos 4 meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "senior-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../data/train_features1628_score_0.85642.pkl')\n",
    "test = pd.read_pickle('../data/test_features1628_score_0.85642.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ignored-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = np.load('keep_columns.npy')\n",
    "rcc_train, rcc_test = get_rcc()\n",
    "\n",
    "rcc_train_list = get_rcc_list(rcc_train,201711, 3)\n",
    "rcc_test_list = get_rcc_list(rcc_test,201811, 3)\n",
    "\n",
    "rcc_train = pd.concat(rcc_train_list, axis=1)\n",
    "rcc_test = pd.concat(rcc_test_list, axis=1)\n",
    "del rcc_train_list, rcc_test_list\n",
    "\n",
    "# rcc_train , rcc_test = get_keep_columns(rcc_train, rcc_test)\n",
    "rcc_train , rcc_test = rcc_train[keep_columns], rcc_test[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "alert-weight",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1929), (396666, 1929))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.join(rcc_train)\n",
    "test = test.join(rcc_test)\n",
    "del rcc_train, rcc_test\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "rolled-headquarters",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853107\ttraining's binary_logloss: 0.295922\tvalid_1's auc: 0.844309\tvalid_1's binary_logloss: 0.302684\n",
      "[100]\ttraining's auc: 0.865715\ttraining's binary_logloss: 0.284686\tvalid_1's auc: 0.85133\tvalid_1's binary_logloss: 0.296582\n",
      "[150]\ttraining's auc: 0.873971\ttraining's binary_logloss: 0.277986\tvalid_1's auc: 0.8534\tvalid_1's binary_logloss: 0.294867\n",
      "[200]\ttraining's auc: 0.880519\ttraining's binary_logloss: 0.272641\tvalid_1's auc: 0.85427\tvalid_1's binary_logloss: 0.294139\n",
      "[250]\ttraining's auc: 0.886264\ttraining's binary_logloss: 0.267944\tvalid_1's auc: 0.854943\tvalid_1's binary_logloss: 0.293675\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttraining's auc: 0.888581\ttraining's binary_logloss: 0.266064\tvalid_1's auc: 0.855114\tvalid_1's binary_logloss: 0.293593\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852838\ttraining's binary_logloss: 0.295917\tvalid_1's auc: 0.843864\tvalid_1's binary_logloss: 0.30263\n",
      "[100]\ttraining's auc: 0.865965\ttraining's binary_logloss: 0.284475\tvalid_1's auc: 0.851101\tvalid_1's binary_logloss: 0.296327\n",
      "[150]\ttraining's auc: 0.874044\ttraining's binary_logloss: 0.277778\tvalid_1's auc: 0.853358\tvalid_1's binary_logloss: 0.294353\n",
      "[200]\ttraining's auc: 0.880607\ttraining's binary_logloss: 0.27238\tvalid_1's auc: 0.854161\tvalid_1's binary_logloss: 0.293615\n",
      "Early stopping, best iteration is:\n",
      "[221]\ttraining's auc: 0.883113\ttraining's binary_logloss: 0.270354\tvalid_1's auc: 0.854474\tvalid_1's binary_logloss: 0.293372\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852712\ttraining's binary_logloss: 0.296502\tvalid_1's auc: 0.846708\tvalid_1's binary_logloss: 0.299987\n",
      "[100]\ttraining's auc: 0.865943\ttraining's binary_logloss: 0.285043\tvalid_1's auc: 0.853089\tvalid_1's binary_logloss: 0.293973\n",
      "[150]\ttraining's auc: 0.874124\ttraining's binary_logloss: 0.278207\tvalid_1's auc: 0.85521\tvalid_1's binary_logloss: 0.291985\n",
      "[200]\ttraining's auc: 0.880639\ttraining's binary_logloss: 0.272897\tvalid_1's auc: 0.855835\tvalid_1's binary_logloss: 0.291535\n",
      "Early stopping, best iteration is:\n",
      "[196]\ttraining's auc: 0.880169\ttraining's binary_logloss: 0.273286\tvalid_1's auc: 0.855934\tvalid_1's binary_logloss: 0.291483\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853155\ttraining's binary_logloss: 0.296172\tvalid_1's auc: 0.845299\tvalid_1's binary_logloss: 0.301335\n",
      "[100]\ttraining's auc: 0.866387\ttraining's binary_logloss: 0.28471\tvalid_1's auc: 0.85162\tvalid_1's binary_logloss: 0.295129\n",
      "[150]\ttraining's auc: 0.87459\ttraining's binary_logloss: 0.277884\tvalid_1's auc: 0.853761\tvalid_1's binary_logloss: 0.293221\n",
      "[200]\ttraining's auc: 0.88114\ttraining's binary_logloss: 0.272567\tvalid_1's auc: 0.854474\tvalid_1's binary_logloss: 0.292476\n",
      "[250]\ttraining's auc: 0.886715\ttraining's binary_logloss: 0.26791\tvalid_1's auc: 0.855042\tvalid_1's binary_logloss: 0.29196\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttraining's auc: 0.888996\ttraining's binary_logloss: 0.266014\tvalid_1's auc: 0.855215\tvalid_1's binary_logloss: 0.291786\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.854707\ttraining's binary_logloss: 0.295531\tvalid_1's auc: 0.837202\tvalid_1's binary_logloss: 0.304106\n",
      "[100]\ttraining's auc: 0.868102\ttraining's binary_logloss: 0.283874\tvalid_1's auc: 0.843208\tvalid_1's binary_logloss: 0.298811\n",
      "[150]\ttraining's auc: 0.876277\ttraining's binary_logloss: 0.276989\tvalid_1's auc: 0.845162\tvalid_1's binary_logloss: 0.297223\n",
      "[200]\ttraining's auc: 0.882834\ttraining's binary_logloss: 0.271566\tvalid_1's auc: 0.845858\tvalid_1's binary_logloss: 0.29659\n",
      "Early stopping, best iteration is:\n",
      "[235]\ttraining's auc: 0.8869\ttraining's binary_logloss: 0.26816\tvalid_1's auc: 0.8461\tvalid_1's binary_logloss: 0.296376\n",
      "*********************\n",
      "roc auc estimado:  0.8534127971496073\n",
      "roc auc varianza:  0.000889817909501271\n"
     ]
    }
   ],
   "source": [
    "test_probs, fi = cross_validation_lightgbm(train, y_train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "careful-zealand",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de variables : 1929\n",
      "variables con importancia acumulada al 99% : 1725\n",
      "variables con zero importancia : 39\n"
     ]
    }
   ],
   "source": [
    "keep_columns, zero_importance = get_feature_selection(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "laden-savings",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853146\ttraining's binary_logloss: 0.295711\tvalid_1's auc: 0.843841\tvalid_1's binary_logloss: 0.302653\n",
      "[100]\ttraining's auc: 0.866212\ttraining's binary_logloss: 0.284357\tvalid_1's auc: 0.85119\tvalid_1's binary_logloss: 0.296587\n",
      "[150]\ttraining's auc: 0.874432\ttraining's binary_logloss: 0.277649\tvalid_1's auc: 0.853665\tvalid_1's binary_logloss: 0.294621\n",
      "[200]\ttraining's auc: 0.880724\ttraining's binary_logloss: 0.272433\tvalid_1's auc: 0.854373\tvalid_1's binary_logloss: 0.293939\n",
      "[250]\ttraining's auc: 0.886617\ttraining's binary_logloss: 0.267693\tvalid_1's auc: 0.85481\tvalid_1's binary_logloss: 0.293592\n",
      "Early stopping, best iteration is:\n",
      "[244]\ttraining's auc: 0.885931\ttraining's binary_logloss: 0.268246\tvalid_1's auc: 0.854883\tvalid_1's binary_logloss: 0.293561\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852938\ttraining's binary_logloss: 0.295916\tvalid_1's auc: 0.844236\tvalid_1's binary_logloss: 0.302665\n",
      "[100]\ttraining's auc: 0.866042\ttraining's binary_logloss: 0.28456\tvalid_1's auc: 0.851457\tvalid_1's binary_logloss: 0.296331\n",
      "[150]\ttraining's auc: 0.87423\ttraining's binary_logloss: 0.27778\tvalid_1's auc: 0.853574\tvalid_1's binary_logloss: 0.294363\n",
      "[200]\ttraining's auc: 0.880767\ttraining's binary_logloss: 0.272474\tvalid_1's auc: 0.854448\tvalid_1's binary_logloss: 0.293615\n",
      "Early stopping, best iteration is:\n",
      "[231]\ttraining's auc: 0.88451\ttraining's binary_logloss: 0.269444\tvalid_1's auc: 0.854822\tvalid_1's binary_logloss: 0.293328\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853353\ttraining's binary_logloss: 0.296225\tvalid_1's auc: 0.84695\tvalid_1's binary_logloss: 0.299893\n",
      "[100]\ttraining's auc: 0.866286\ttraining's binary_logloss: 0.2848\tvalid_1's auc: 0.853415\tvalid_1's binary_logloss: 0.29374\n",
      "[150]\ttraining's auc: 0.874172\ttraining's binary_logloss: 0.27817\tvalid_1's auc: 0.85543\tvalid_1's binary_logloss: 0.291838\n",
      "[200]\ttraining's auc: 0.880906\ttraining's binary_logloss: 0.272751\tvalid_1's auc: 0.856315\tvalid_1's binary_logloss: 0.291131\n",
      "Early stopping, best iteration is:\n",
      "[211]\ttraining's auc: 0.882205\ttraining's binary_logloss: 0.271664\tvalid_1's auc: 0.856403\tvalid_1's binary_logloss: 0.291018\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853191\ttraining's binary_logloss: 0.296244\tvalid_1's auc: 0.84529\tvalid_1's binary_logloss: 0.301153\n",
      "[100]\ttraining's auc: 0.866228\ttraining's binary_logloss: 0.284844\tvalid_1's auc: 0.851872\tvalid_1's binary_logloss: 0.294678\n",
      "[150]\ttraining's auc: 0.874469\ttraining's binary_logloss: 0.278025\tvalid_1's auc: 0.853629\tvalid_1's binary_logloss: 0.292869\n",
      "[200]\ttraining's auc: 0.881299\ttraining's binary_logloss: 0.272577\tvalid_1's auc: 0.85448\tvalid_1's binary_logloss: 0.292113\n",
      "[250]\ttraining's auc: 0.886892\ttraining's binary_logloss: 0.26787\tvalid_1's auc: 0.854854\tvalid_1's binary_logloss: 0.291782\n",
      "Early stopping, best iteration is:\n",
      "[242]\ttraining's auc: 0.886034\ttraining's binary_logloss: 0.268616\tvalid_1's auc: 0.854872\tvalid_1's binary_logloss: 0.291811\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.854803\ttraining's binary_logloss: 0.295463\tvalid_1's auc: 0.837398\tvalid_1's binary_logloss: 0.303876\n",
      "[100]\ttraining's auc: 0.86821\ttraining's binary_logloss: 0.283709\tvalid_1's auc: 0.843398\tvalid_1's binary_logloss: 0.298586\n",
      "[150]\ttraining's auc: 0.876469\ttraining's binary_logloss: 0.276793\tvalid_1's auc: 0.845424\tvalid_1's binary_logloss: 0.296904\n",
      "Early stopping, best iteration is:\n",
      "[148]\ttraining's auc: 0.876201\ttraining's binary_logloss: 0.277007\tvalid_1's auc: 0.845432\tvalid_1's binary_logloss: 0.2969\n",
      "*********************\n",
      "roc auc estimado:  0.8533313112934255\n",
      "roc auc varianza:  0.000968768012956236\n"
     ]
    }
   ],
   "source": [
    "test_probs, fi = cross_validation_lightgbm(train[keep_columns], y_train, test[keep_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bulgarian-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name = 'target'\n",
    "test_probs.to_csv('../results/lightgbm_with_1725features_score_0.85333.csv') #### score de 0.85612 en la tabla publica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-religious",
   "metadata": {},
   "source": [
    "## Aggregate Unstack de cod_instit_financiera and PRODUCTO del los ultimos 6 meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "threatened-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../data/train_features1628_score_0.85642.pkl')\n",
    "test = pd.read_pickle('../data/test_features1628_score_0.85642.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "arabic-copying",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haciendo unstack de cod_instit_financiera desde 201709 con la funcion de agregacion Sum\n",
      "haciendo unstack de cod_instit_financiera desde 201709 con la funcion de agregacion Unique\n",
      "haciendo unstack de cod_instit_financiera desde 201709 con la funcion de agregacion Min\n",
      "haciendo unstack de cod_instit_financiera desde 201709 con la funcion de agregacion Max\n",
      "haciendo unstack de cod_instit_financiera desde 201709 con la funcion de agregacion Std\n",
      "haciendo unstack de cod_instit_financiera desde 201709 con la funcion de agregacion Mean\n",
      "haciendo unstack de cod_instit_financiera desde 201709 con la funcion de agregacion Median\n",
      "haciendo unstack de PRODUCTO desde 201709 con la funcion de agregacion Sum\n",
      "haciendo unstack de PRODUCTO desde 201709 con la funcion de agregacion Unique\n",
      "haciendo unstack de PRODUCTO desde 201709 con la funcion de agregacion Min\n",
      "haciendo unstack de PRODUCTO desde 201709 con la funcion de agregacion Max\n",
      "haciendo unstack de PRODUCTO desde 201709 con la funcion de agregacion Std\n",
      "haciendo unstack de PRODUCTO desde 201709 con la funcion de agregacion Mean\n",
      "haciendo unstack de PRODUCTO desde 201709 con la funcion de agregacion Median\n",
      "haciendo unstack de cod_instit_financiera desde 201809 con la funcion de agregacion Sum\n",
      "haciendo unstack de cod_instit_financiera desde 201809 con la funcion de agregacion Unique\n",
      "haciendo unstack de cod_instit_financiera desde 201809 con la funcion de agregacion Min\n",
      "haciendo unstack de cod_instit_financiera desde 201809 con la funcion de agregacion Max\n",
      "haciendo unstack de cod_instit_financiera desde 201809 con la funcion de agregacion Std\n",
      "haciendo unstack de cod_instit_financiera desde 201809 con la funcion de agregacion Mean\n",
      "haciendo unstack de cod_instit_financiera desde 201809 con la funcion de agregacion Median\n",
      "haciendo unstack de PRODUCTO desde 201809 con la funcion de agregacion Sum\n",
      "haciendo unstack de PRODUCTO desde 201809 con la funcion de agregacion Unique\n",
      "haciendo unstack de PRODUCTO desde 201809 con la funcion de agregacion Min\n",
      "haciendo unstack de PRODUCTO desde 201809 con la funcion de agregacion Max\n",
      "haciendo unstack de PRODUCTO desde 201809 con la funcion de agregacion Std\n",
      "haciendo unstack de PRODUCTO desde 201809 con la funcion de agregacion Mean\n",
      "haciendo unstack de PRODUCTO desde 201809 con la funcion de agregacion Median\n",
      "(358487, 1029) (396666, 931)\n",
      "(358487, 924) (396666, 924)\n"
     ]
    }
   ],
   "source": [
    "# keep_columns = np.load('keep_columns.npy')\n",
    "rcc_train, rcc_test = get_rcc()\n",
    "\n",
    "rcc_train_list = get_rcc_list(rcc_train,201709, 5)\n",
    "rcc_test_list = get_rcc_list(rcc_test,201809, 5)\n",
    "\n",
    "rcc_train = pd.concat(rcc_train_list, axis=1)\n",
    "rcc_test = pd.concat(rcc_test_list, axis=1)\n",
    "del rcc_train_list, rcc_test_list\n",
    "\n",
    "rcc_train , rcc_test = get_keep_columns(rcc_train, rcc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "excellent-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para obtener variables con alta correlación\n",
    "def get_correlated(data, cut=0.85):\n",
    "    correlated_features = set()\n",
    "    correlation_matrix = data.corr()\n",
    "    \n",
    "    for i in range(len(correlation_matrix .columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > cut:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                correlated_features.add(colname)\n",
    "            \n",
    "    return correlated_features \n",
    "drop_columns = get_correlated(rcc_train, cut=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "individual-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcc_train.drop(drop_columns, axis=1, inplace=True)\n",
    "rcc_test.drop(drop_columns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "naked-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((358487, 1713), (396666, 1713))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.join(rcc_train)\n",
    "test = test.join(rcc_test)\n",
    "del rcc_train, rcc_test\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pointed-deviation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853077\ttraining's binary_logloss: 0.295831\tvalid_1's auc: 0.843997\tvalid_1's binary_logloss: 0.30259\n",
      "[100]\ttraining's auc: 0.865844\ttraining's binary_logloss: 0.284651\tvalid_1's auc: 0.851549\tvalid_1's binary_logloss: 0.296369\n",
      "[150]\ttraining's auc: 0.873877\ttraining's binary_logloss: 0.278039\tvalid_1's auc: 0.853463\tvalid_1's binary_logloss: 0.294666\n",
      "[200]\ttraining's auc: 0.880458\ttraining's binary_logloss: 0.272721\tvalid_1's auc: 0.854078\tvalid_1's binary_logloss: 0.294175\n",
      "[250]\ttraining's auc: 0.886184\ttraining's binary_logloss: 0.267988\tvalid_1's auc: 0.854783\tvalid_1's binary_logloss: 0.293661\n",
      "Early stopping, best iteration is:\n",
      "[248]\ttraining's auc: 0.885973\ttraining's binary_logloss: 0.268166\tvalid_1's auc: 0.854805\tvalid_1's binary_logloss: 0.293642\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853086\ttraining's binary_logloss: 0.295825\tvalid_1's auc: 0.843825\tvalid_1's binary_logloss: 0.302784\n",
      "[100]\ttraining's auc: 0.866008\ttraining's binary_logloss: 0.284529\tvalid_1's auc: 0.85107\tvalid_1's binary_logloss: 0.296532\n",
      "[150]\ttraining's auc: 0.874041\ttraining's binary_logloss: 0.277772\tvalid_1's auc: 0.853518\tvalid_1's binary_logloss: 0.294398\n",
      "[200]\ttraining's auc: 0.880745\ttraining's binary_logloss: 0.272465\tvalid_1's auc: 0.854282\tvalid_1's binary_logloss: 0.293714\n",
      "[250]\ttraining's auc: 0.886359\ttraining's binary_logloss: 0.267801\tvalid_1's auc: 0.854521\tvalid_1's binary_logloss: 0.293498\n",
      "[300]\ttraining's auc: 0.891511\ttraining's binary_logloss: 0.263492\tvalid_1's auc: 0.854889\tvalid_1's binary_logloss: 0.293204\n",
      "Early stopping, best iteration is:\n",
      "[308]\ttraining's auc: 0.892265\ttraining's binary_logloss: 0.262815\tvalid_1's auc: 0.854989\tvalid_1's binary_logloss: 0.293143\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853168\ttraining's binary_logloss: 0.296267\tvalid_1's auc: 0.846553\tvalid_1's binary_logloss: 0.300163\n",
      "[100]\ttraining's auc: 0.866077\ttraining's binary_logloss: 0.284896\tvalid_1's auc: 0.853149\tvalid_1's binary_logloss: 0.294002\n",
      "[150]\ttraining's auc: 0.874002\ttraining's binary_logloss: 0.278268\tvalid_1's auc: 0.854867\tvalid_1's binary_logloss: 0.292332\n",
      "[200]\ttraining's auc: 0.880677\ttraining's binary_logloss: 0.272879\tvalid_1's auc: 0.85561\tvalid_1's binary_logloss: 0.291703\n",
      "[250]\ttraining's auc: 0.886473\ttraining's binary_logloss: 0.268234\tvalid_1's auc: 0.856265\tvalid_1's binary_logloss: 0.291276\n",
      "Early stopping, best iteration is:\n",
      "[246]\ttraining's auc: 0.885984\ttraining's binary_logloss: 0.268589\tvalid_1's auc: 0.856274\tvalid_1's binary_logloss: 0.291272\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852911\ttraining's binary_logloss: 0.296311\tvalid_1's auc: 0.845153\tvalid_1's binary_logloss: 0.301251\n",
      "[100]\ttraining's auc: 0.866131\ttraining's binary_logloss: 0.284932\tvalid_1's auc: 0.851524\tvalid_1's binary_logloss: 0.294983\n",
      "[150]\ttraining's auc: 0.87406\ttraining's binary_logloss: 0.278225\tvalid_1's auc: 0.853497\tvalid_1's binary_logloss: 0.292975\n",
      "Early stopping, best iteration is:\n",
      "[188]\ttraining's auc: 0.879179\ttraining's binary_logloss: 0.274102\tvalid_1's auc: 0.854256\tvalid_1's binary_logloss: 0.29229\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.8549\ttraining's binary_logloss: 0.295607\tvalid_1's auc: 0.837756\tvalid_1's binary_logloss: 0.303901\n",
      "[100]\ttraining's auc: 0.868021\ttraining's binary_logloss: 0.283906\tvalid_1's auc: 0.843862\tvalid_1's binary_logloss: 0.298351\n",
      "[150]\ttraining's auc: 0.876185\ttraining's binary_logloss: 0.277069\tvalid_1's auc: 0.845513\tvalid_1's binary_logloss: 0.29691\n",
      "[200]\ttraining's auc: 0.882688\ttraining's binary_logloss: 0.271689\tvalid_1's auc: 0.846348\tvalid_1's binary_logloss: 0.296219\n",
      "[250]\ttraining's auc: 0.888355\ttraining's binary_logloss: 0.266943\tvalid_1's auc: 0.846675\tvalid_1's binary_logloss: 0.295982\n",
      "Early stopping, best iteration is:\n",
      "[258]\ttraining's auc: 0.889265\ttraining's binary_logloss: 0.266215\tvalid_1's auc: 0.846788\tvalid_1's binary_logloss: 0.295873\n",
      "*********************\n",
      "roc auc estimado:  0.8534554182578482\n",
      "roc auc varianza:  0.0008254893558351615\n"
     ]
    }
   ],
   "source": [
    "test_probs, fi = cross_validation_lightgbm(train, y_train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sensitive-peter",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de variables : 1713\n",
      "variables con importancia acumulada al 99% : 1537\n",
      "variables con zero importancia : 71\n"
     ]
    }
   ],
   "source": [
    "keep_columns, zero_importance = get_feature_selection(fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-government",
   "metadata": {},
   "source": [
    "## Aggregate crosstab en Sunat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "infinite-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('../data/train_features1628_score_0.85642.pkl')\n",
    "test = pd.read_pickle('../data/test_features1628_score_0.85642.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "consecutive-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunat_train = pd.read_csv(f'{path}/sunat_train.csv')\n",
    "sunat_test = pd.read_csv(f'{path}/sunat_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adequate-conversion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((292479, 18), (318821, 18))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### eliminar registros duplicados\n",
    "sunat_train.drop_duplicates(inplace=True)\n",
    "sunat_test.drop_duplicates(inplace=True)\n",
    "sunat_train.shape, sunat_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hungry-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {'tipcontribuyente': 'int32',\n",
    "         'tippersona': 'int32',\n",
    "         'ciiu': 'int32',\n",
    "         'ubigeo': 'int32',\n",
    "         'condiciondomicilio': 'int32',\n",
    "         'estadocontribuyente': 'int32',\n",
    "         'codvia': 'int32',\n",
    "         'codzona': 'int32',\n",
    "         'contabilidad': 'int32',\n",
    "         'facturacion': 'int32',\n",
    "         'domiciliado': 'int32',\n",
    "         'comercioexterior': 'int32',\n",
    "         'cargorele': 'int32',\n",
    "         'codentidadtributo': 'int32',\n",
    "         'estadotributo': 'int32'}\n",
    "sunat_train = sunat_train.astype(dict_)\n",
    "sunat_test = sunat_test.astype(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "western-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_estadotributo_train = get_crosstab(sunat_train, 'key_value', 'estadotributo')\n",
    "crosstab_estadotributo_test = get_crosstab(sunat_test, 'key_value', 'estadotributo')\n",
    "\n",
    "crosstab_codentidadtributo_train = get_crosstab(sunat_train, 'key_value', 'codentidadtributo')\n",
    "crosstab_codentidadtributo_test = get_crosstab(sunat_test, 'key_value', 'codentidadtributo')\n",
    "\n",
    "crosstab_condiciondomicilio_train = get_crosstab(sunat_train, 'key_value', 'condiciondomicilio')\n",
    "crosstab_condiciondomicilio_test = get_crosstab(sunat_test, 'key_value', 'condiciondomicilio')\n",
    "\n",
    "crosstab_train = pd.concat([crosstab_estadotributo_train, crosstab_codentidadtributo_train, crosstab_condiciondomicilio_train], axis=1)\n",
    "\n",
    "crosstab_test = pd.concat([crosstab_estadotributo_test, crosstab_codentidadtributo_test, crosstab_condiciondomicilio_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "institutional-purchase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358487, 1628) (396666, 1628)\n",
      "(358487, 1625) (396666, 1625)\n"
     ]
    }
   ],
   "source": [
    "drop_columns = ['estadotributo_mode','codentidadtributo_mode','condiciondomicilio_mode']\n",
    "print(train.shape, test.shape)\n",
    "train = train.drop(drop_columns, axis=1)\n",
    "test = test.drop(drop_columns, axis=1)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "similar-montreal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358487, 1652) (396666, 1651)\n",
      "(358487, 1651) (396666, 1651)\n"
     ]
    }
   ],
   "source": [
    "train = train.join(crosstab_train)\n",
    "test = test.join(crosstab_test)\n",
    "train, test = get_keep_columns(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "backed-upper",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** 0 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.853084\ttraining's binary_logloss: 0.295859\tvalid_1's auc: 0.843625\tvalid_1's binary_logloss: 0.303054\n",
      "[100]\ttraining's auc: 0.866008\ttraining's binary_logloss: 0.284547\tvalid_1's auc: 0.85098\tvalid_1's binary_logloss: 0.296921\n",
      "[150]\ttraining's auc: 0.874105\ttraining's binary_logloss: 0.277872\tvalid_1's auc: 0.852837\tvalid_1's binary_logloss: 0.295255\n",
      "[200]\ttraining's auc: 0.880459\ttraining's binary_logloss: 0.272669\tvalid_1's auc: 0.853855\tvalid_1's binary_logloss: 0.294469\n",
      "[250]\ttraining's auc: 0.8862\ttraining's binary_logloss: 0.267976\tvalid_1's auc: 0.854504\tvalid_1's binary_logloss: 0.294006\n",
      "Early stopping, best iteration is:\n",
      "[278]\ttraining's auc: 0.889084\ttraining's binary_logloss: 0.265529\tvalid_1's auc: 0.854875\tvalid_1's binary_logloss: 0.293802\n",
      "********** 1 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852876\ttraining's binary_logloss: 0.295857\tvalid_1's auc: 0.844108\tvalid_1's binary_logloss: 0.302607\n",
      "[100]\ttraining's auc: 0.866058\ttraining's binary_logloss: 0.28442\tvalid_1's auc: 0.851363\tvalid_1's binary_logloss: 0.296271\n",
      "[150]\ttraining's auc: 0.874133\ttraining's binary_logloss: 0.277761\tvalid_1's auc: 0.853545\tvalid_1's binary_logloss: 0.294519\n",
      "[200]\ttraining's auc: 0.880672\ttraining's binary_logloss: 0.272437\tvalid_1's auc: 0.854103\tvalid_1's binary_logloss: 0.293838\n",
      "[250]\ttraining's auc: 0.886227\ttraining's binary_logloss: 0.267808\tvalid_1's auc: 0.85467\tvalid_1's binary_logloss: 0.293307\n",
      "Early stopping, best iteration is:\n",
      "[267]\ttraining's auc: 0.888097\ttraining's binary_logloss: 0.26627\tvalid_1's auc: 0.854828\tvalid_1's binary_logloss: 0.293188\n",
      "********** 2 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852861\ttraining's binary_logloss: 0.296475\tvalid_1's auc: 0.847003\tvalid_1's binary_logloss: 0.299956\n",
      "[100]\ttraining's auc: 0.865851\ttraining's binary_logloss: 0.28509\tvalid_1's auc: 0.853473\tvalid_1's binary_logloss: 0.293792\n",
      "[150]\ttraining's auc: 0.874022\ttraining's binary_logloss: 0.278373\tvalid_1's auc: 0.855575\tvalid_1's binary_logloss: 0.291978\n",
      "Early stopping, best iteration is:\n",
      "[183]\ttraining's auc: 0.87835\ttraining's binary_logloss: 0.274752\tvalid_1's auc: 0.856133\tvalid_1's binary_logloss: 0.291422\n",
      "********** 3 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.852818\ttraining's binary_logloss: 0.296291\tvalid_1's auc: 0.845163\tvalid_1's binary_logloss: 0.301035\n",
      "[100]\ttraining's auc: 0.865886\ttraining's binary_logloss: 0.285037\tvalid_1's auc: 0.85166\tvalid_1's binary_logloss: 0.294898\n",
      "[150]\ttraining's auc: 0.874066\ttraining's binary_logloss: 0.278264\tvalid_1's auc: 0.853789\tvalid_1's binary_logloss: 0.292875\n",
      "Early stopping, best iteration is:\n",
      "[183]\ttraining's auc: 0.87849\ttraining's binary_logloss: 0.274718\tvalid_1's auc: 0.854714\tvalid_1's binary_logloss: 0.292108\n",
      "********** 4 **********\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[50]\ttraining's auc: 0.855024\ttraining's binary_logloss: 0.295397\tvalid_1's auc: 0.837827\tvalid_1's binary_logloss: 0.303749\n",
      "[100]\ttraining's auc: 0.868228\ttraining's binary_logloss: 0.283762\tvalid_1's auc: 0.843778\tvalid_1's binary_logloss: 0.298362\n",
      "[150]\ttraining's auc: 0.876214\ttraining's binary_logloss: 0.276944\tvalid_1's auc: 0.845734\tvalid_1's binary_logloss: 0.296652\n",
      "[200]\ttraining's auc: 0.882711\ttraining's binary_logloss: 0.271563\tvalid_1's auc: 0.846584\tvalid_1's binary_logloss: 0.296024\n",
      "Early stopping, best iteration is:\n",
      "[194]\ttraining's auc: 0.881962\ttraining's binary_logloss: 0.272172\tvalid_1's auc: 0.846628\tvalid_1's binary_logloss: 0.29601\n",
      "*********************\n",
      "roc auc estimado:  0.8534585207880689\n",
      "roc auc varianza:  0.0008386188242324427\n"
     ]
    }
   ],
   "source": [
    "test_probs, fi = cross_validation_lightgbm(train, y_train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "opposite-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs.name='target'\n",
    "test_probs.to_csv('../results/lightgbm_with_1651features_score_0.85345.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
